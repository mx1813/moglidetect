text;labels
"3.2 Entwicklung des App-Designs
Für das grundlegende Design wird ein Mockup erstellt. Dieses beinhaltet die zentralen
Komponenten, die für die App erforderlich sind. Für Features, die nicht in den Kontext der
Journaling App passen, wird eine extra Ansicht erstellt. Dadurch wird die Umsetzbarkeit
der technischen Komponente demonstriert, was der zentrale Teil dieser Arbeit ist. In der
folgenden Abbildung 3.2 ist das erste Mockup der zwei zentralen Ansichten zu sehen.
Abbildung 3.1: Mockup der Journal-Eintrag Erstellungsansicht
Es handelt sich um die Ansicht, mit der Journal-Einträge erstellt werden können. Da es
sich beim Journaling hauptsächlich um eine textuelle Praxis handelt, ist ein Großteil der
Ansicht ein Freitext Feld. Dazu kommen noch ein Button, der das Speichern des erstellten
Journal-Eintrags auslöst.
Die Listenansicht der erstellten Journal-Einträge ist die zweite zentrale Ansicht und im
folgenden Mockup abgebildet.
Hier wird als Titel der Zeitstempel der Erstellung gewählt. Der Titel kann aufgeklappt
werden. In der aufgeklappten Ansicht ist der Inhalt des Journal-Eintrags enthalten. Die
Ansicht soll den Nutzer zur Selbstreﬂexion motivieren.";0
Besonders wichtig beim Einsatz von Metriken ist ein durchdachter Umgang mit den  Messergebnissen. Die eingesetzten Kennzahlen  sowie  deren Aufbau und Bedeutung für die  Softwarequalität, müssen genau verstanden werden. Andernfalls besteht die Gefahr von  Fehlinterpretationen. Auch bei der Auswahl der Metriken gibt es einige entscheidende Punkte zu  beachten. Hier ist der Einsatz von Methoden zur Herleitung passender Metriken empfohlen, um  relevante Messungen durchzuführen, statt nur Werte zu sammeln, die leicht gem essen werden  können. Weiterhin ist es entscheidend sich nicht auf die Rohdaten der Messungen zu verlassen,  sondern Analyseergebnisse zu hinterfragen und die betroffenen Stellen des  vorliegenden Quellcode s  genauer  zu betrachten.  Besonders sehr spezielle oder zusammengesetzte Metriken sind oftmals  nicht ausreichend validiert. Aus diesem Grund muss die Genauigkeit und Validität der Daten geprüft  werden. Gleiches gilt für den Einsatz von Grenzwerten, deren Einsatz oftmals sinnvoll ist, allerdings  eher als Richt linie angesehen werden sollte.   Alle genannten Gründe zeigen auf, dass der Einsatz von Metriken als Bewertungskriterium für  Softwareprojekte fraglich ist, da die Messergebnisse  keine Allgemeingültigkeit haben . Im Gegenteil  dienen diese eher als Denkanstöße,  an welchen Stellen und in welcher Hinsicht die Codequalität  möglicherweise verbessert werden könnte. Dennoch kann eine Metrikunterstützung in  studentischen Projekten Vorteile bringen. Der Einsatz von Tools bei der Softwareentwicklung kann  hilfreiche Anhal tspunkte für Studierende hervorbringen, was die Softwarequalität in den Fokus rückt.;0
 Evaluierung eines IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung     Die Integration von Internet of Things (IoT)-Technologien in den Alltag hat signifikante Fortschritte gemacht, insbesondere im Bereich der Heimautomatisierung. Dieses Projekt zielt darauf ab, ein IoT-System zu entwickeln, das die Kontrolle einer Katzenklappe durch eine KI-gestützte Katzenerkennung ermöglicht. Die vorliegende Evaluierung begreift die kritischen Aspekte des Systems, einschließlich der Effizienz, Benutzerfreundlichkeit, Sicherheit und der allgemeinen Funktionalität.    Technische Umsetzung  Das entwickelte System besteht aus mehreren Komponenteneiner Katzenklappe mit motorisiertem Antrieb, einem Raspberry Pi als zentrale Steuereinheit, einer Webcam zur Bildaufnahme und einem KI-Modell zur Katzenerkennung. Die Katzenerkennung basiert auf einem trainierten Convolutional Neural Network (CNN), das in der Lage ist, das Tier in Echtzeit anhand von Kamerabildern zu identifizieren. Die Steuerung der Katzenklappe erfolgt über ein einfaches Web-Interface, das von den Besitzern verwendet werden kann, um Einstellungen vorzunehmen oder den Zugang zu überwachen.   Kriterien für die Evaluierung  Die Evaluierung des IoT-Systems wurde anhand folgender Kriterien durchgeführt 1. Genauigkeit der KatzenerkennungDie Leistung des KI-Modells wurde in einer Vielzahl von Testszenarien evaluiert, um die Zuverlässigkeit und Präzision der Erkennung zu bestimmen. 2. Reaktionszeit der Katze-KlappeDie Zeit von der Identifikation der Katze bis zur Öffnung der Klappe wurde gemessen, um zu gewährleisten, dass das System im Alltag praktikabel ist. 3. Benutzerfreundlichkeit des InterfacesDie Einfachheit der Nutzung des Web-Interfaces wurde durch eine Nutzerbefragung bewertet. Aspekte wie Zugänglichkeit und Verständlichkeit wurden ebenfalls berücksichtigt. 4. SicherheitsaspekteDa das System Anfälligkeiten gegenüber Cyberangriffen aufweist, wurde eine Sicherheitsanalyse durchgeführt, um potentielle Schwachstellen zu identifizieren und Gegenmaßnahmen zu prüfen. 5. Integration in bestehende Smart-Home-SystemeDie Fähigkeit, das System nahtlos in bestehende Smart-Home-Umgebungen zu integrieren, wurde getestet, um die Interoperabilität zu bewerten.   Ergebnisse der Evaluierung  1. Genauigkeit der KatzenerkennungDie Evaluierung ergab eine Erkennungsgenauigkeit von über 95 % bei durchschnittlichen Lichtverhältnissen. Unter schlechten Lichtbedingungen sank die Genauigkeit jedoch auf etwa 80 %, was auf einen bestehenden Optimierungsbedarf hinweist.  2. Reaktionszeit der KatzenklappeDie durchschnittliche Reaktionszeit lag bei 1,2 Sekunden, was als akzeptabel für eine spontane Nutzung betrachtet wird. In Szenarien mit mehreren Ansätzen während eines kurzen Zeitraums konnte es jedoch zu Verzögerungen kommen.  3. Benutzerfreundlichkeit des InterfacesDie Nutzerbefragung ergab, dass 85 % der Benutzer das Interface als intuitiv und einfach bedienbar einstufen. Einige Vorschläge zur Verbesserung beinhalteten eine detailliertere Hilfefunktion sowie eine mobile App zur bequemeren Steuerung.  4. SicherheitsaspekteDie Sicherheitsanalyse zeigte mehrere Schwachstellen, insbesondere in der Datenübertragung zwischen der Webcam und dem Raspberry Pi. Die Implementierung von End-to-End-Verschlüsselung wurde als notwendig erachtet, um die Datenintegrität zu gewährleisten.  5. Integration in bestehende Smart-Home-SystemeDas System konnte erfolgreich in ein Beispiel-Smart-Home-Setup integriert werden, was die Interoperabilität unter Beweis stellte. Anpassungen in der API waren notwendig, um die Kompatibilität zu gewährleisten.   Fazit  Die Evaluierung des IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung hat sowohl Stärken als auch Schwächen aufgezeigt. Während die Katzenerkennung und die Reaktionszeit der Klappe zufriedenstellend sind, bestehen Optimierungspotenziale in der Lichtempfindlichkeit der Erkennung, der Datensicherheit und der Benutzerinterface-Gestaltung. Weitere Forschung könnte sich darauf konzentrieren, die Robustheit des KI-Modells unter variierenden Umweltbedingungen zu verbessern und Lösungen für die identifizierten Sicherheitsrisiken zu entwickeln. Die Integration in Smart-Home-Technologien bietet eine vielversprechende Perspektive für zukünftige Anwendungen, die über die Katzenklappe hinausgehen.;1
"Ausblick: Möglichkeiten und Gefahren der digitalen Überwachung im Zeitalter von Zero  Die digitale Überwachung ist zu einem zentralen Element unserer modernen Gesellschaft geworden, das sowohl Chancen als auch Herausforderungen birgt. In einer Zeit, in der Daten in nie zuvor gekanntem Ausmaß gesammelt und analysiert werden, ist es unerlässlich, die vielschichtigen Implikationen dieser Praktiken zu beleuchten. Der Begriff ""Zero"" steht symbolisch für die umfassende, oft unsichtbare Natur der Überwachung, die nahezu alle Aspekte unseres Lebens durchdringt – von den alltäglichen Entscheidungen bis hin zu den grundlegendsten Fragen der Privatsphäre und Freiheit.  Auf der einen Seite eröffnet die digitale Überwachung innovative Möglichkeiten zur Verbesserung der Sicherheit, Effizienz und des Lebensstandards. Technologien wie Künstliche Intelligenz und Big Data ermöglichen es, Muster zu erkennen, Risiken frühzeitig zu identifizieren und personalisierte Dienstleistungen anzubieten. Im Bereich der öffentlichen Sicherheit können beispielsweise Überwachungssysteme dazu beitragen, Verbrechen zu verhindern und die Reaktionszeiten der Einsatzkräfte zu verkürzen. Auch im Gesundheitswesen lassen sich durch die Analyse von Datenströmen wertvolle Erkenntnisse gewinnen, die zur Verbesserung der Patientenversorgung beitragen.  Auf der anderen Seite sind die Gefahren, die mit dieser Form der Überwachung einhergehen, nicht zu unterschätzen. Die ständige Beobachtung kann zu einem Verlust der Privatsphäre führen, der nicht nur das individuelle Wohlbefinden beeinträchtigt, sondern auch das Vertrauen in staatliche Institutionen und Unternehmen untergräbt. Fragen der Datenhoheit und -sicherheit gewinnen an Bedeutung, da immer mehr persönliche Informationen in die Hände Dritter gelangen. Zudem besteht die Gefahr, dass durch algorithmische Entscheidungen Diskriminierung und Ungerechtigkeit perpetuiert werden, wenn beispielsweise Überwachungsmaßnahmen gezielt gegen bestimmte Bevölkerungsgruppen gerichtet sind.  In Anbetracht dieser dualen Natur der digitalen Überwachung ist es entscheidend, einen ausgewogenen Diskurs zu führen, der sowohl die Potenziale als auch die Risiken berücksichtigt. Zukünftige Forschungen sollten sich darauf konzentrieren, ethische Rahmenbedingungen zu entwickeln, die sicherstellen, dass technologische Fortschritte im Bereich der Überwachung im Einklang mit den Werten der Gesellschaft stehen. Der Dialog zwischen Politik, Wissenschaft und Zivilgesellschaft ist unerlässlich, um einen verantwortungsvollen Umgang mit den Technologien zu fördern und gleichzeitig die Rechte und Freiheiten der Individuen zu schützen.  In diesem Kontext wird die Auseinandersetzung mit dem Thema ""Zero - Möglichkeiten und Gefahren der digitalen Überwachung"" nicht nur für die akademische Welt, sondern auch für die breite Öffentlichkeit von großer Relevanz sein. Die Erkenntnisse aus dieser Arbeit sollen dazu beitragen, ein Bewusstsein für die weitreichenden Konsequenzen digitaler Überwachung zu schaffen und eine informierte Debatte über die Zukunft der Privatsphäre in einer zunehmend vernetzten Welt zu fördern. Der Weg zu einer verantwortungsvollen digitalen Gesellschaft erfordert eine kritische Reflexion und die Bereitschaft, die Herausforderungen anzunehmen, die mit den Möglichkeiten der Überwachung einhergehen.";1
  Die digitale Transformation hat in den letzten Jahren zu einem exponentiellen Anstieg der Anzahl und Vielfalt von Content-Management-Systemen (CMS) geführt. Unternehmen und Organisationen stehen vor der Herausforderung, das geeignete CMS auszuwählen, um ihre spezifischen Bedürfnisse in Bezug auf Content-Erstellung, -Verwaltung und -Verbreitung zu erfüllen. In diesem Kontext ist die Entscheidung für die  nicht nur eine technische, sondern auch eine strategische Überlegung, die sowohl Vor- als auch Nachteile mit sich bringt.  1. Definition und Typen von Content-Management-Systemen  Content-Management-Systeme sind Softwarelösungen, die es Nutzern ermöglichen, digitale Inhalte zu erstellen, zu bearbeiten und zu verwalten, ohne tiefgehende technische Kenntnisse zu benötigen. Die gängigsten Typen von CMS sind Open-Source-Systeme wie WordPress, Drupal und Joomla sowie proprietäre Lösungen wie Adobe Experience Manager oder Sitecore. Während Open-Source-Systeme eine hohe Flexibilität und Anpassungsfähigkeit bieten, zeichnen sich proprietäre Systeme durch umfassenden Support und integrierte Funktionalitäten aus.  2. Vor- und Nachteile bestehender CMS-Lösungen  Die Auswahl eines bestehenden CMS bietet den Vorteil, dass diese Systeme in der Regel bereits ausgereift sind und eine breite Nutzerbasis aufweisen. Dies führt zu einer Vielzahl von Plugins, Themes und Community-Support. Allerdings können Standardlösungen auch Einschränkungen hinsichtlich der Anpassbarkeit und der spezifischen Funktionalitäten aufweisen, die für bestimmte Branchen oder Unternehmen notwendig sind. Darüber hinaus können Lizenzkosten und Abhängigkeiten von Drittanbietern zu einem nicht unerheblichen finanziellen und organisatorischen Aufwand führen.  3. Implementierung einer eigenen CMS-Lösung  Die Implementierung einer eigenen CMS-Lösung kann eine attraktive Option sein, insbesondere für Unternehmen mit spezifischen Anforderungen, die von bestehenden Systemen nicht erfüllt werden. Eine maßgeschneiderte Lösung ermöglicht es, Funktionen zu integrieren, die genau auf die Bedürfnisse des Unternehmens zugeschnitten sind. Dies kann beispielsweise die Integration von speziellen Workflows, Datenbanken oder Schnittstellen zu anderen Unternehmenssystemen umfassen.  4. Technische und organisatorische Herausforderungen  Die Entwicklung eines eigenen CMS erfordert jedoch umfassende technische Expertise und Ressourcen. Die Programmierung, das Design und die laufende Wartung sind zeitaufwändig und kostenintensiv. Zudem müssen Unternehmen sicherstellen, dass sie über das notwendige Fachwissen verfügen, um Sicherheitslücken zu schließen und das System regelmäßig zu aktualisieren. Organisatorisch kann die  zu Widerständen innerhalb des Unternehmens führen, insbesondere wenn Mitarbeiter an bestehende Systeme gewöhnt sind.  5. Fallstudien und Best Practices  Die Analyse von Fallstudien zeigt, dass einige Unternehmen erfolgreich eigene CMS-Lösungen implementiert haben, indem sie agile Methoden und iterative Entwicklungsansätze nutzten. Dies ermöglicht eine kontinuierliche Anpassung an sich ändernde Anforderungen und Technologien. Best Practices umfassen die frühzeitige Einbindung von Endnutzern in den Entwicklungsprozess sowie die Durchführung von umfassenden Tests, um die Benutzerfreundlichkeit und Funktionalität sicherzustellen.  6. Fazit und Ausblick  Die Entscheidung, ob ein bestehendes CMS oder eine eigene Lösung implementiert werden;1
"Ausblick für die wissenschaftliche Arbeit: ""State of the Art beim Testen von MQTT basierten Lösungen""  In der vorliegenden Arbeit wurde der aktuelle Stand der Wissenschaft und Technik im Bereich des Testens von MQTT (Message Queuing Telemetry Transport) basierten Lösungen umfassend untersucht. Die Analyse der verfügbaren Testmethoden, -werkzeuge und -strategien hat gezeigt, dass MQTT aufgrund seiner Leichtgewichtigkeit und Effizienz eine bedeutende Rolle in zahlreichen IoT-Anwendungen (Internet of Things) spielt. Angesichts der zunehmend komplexen Anwendungsszenarien, in denen MQTT implementiert wird, ist ein fundiertes Verständnis der Testverfahren von entscheidender Bedeutung.  Im Ausblick dieser Arbeit sollen mehrere Perspektiven betrachtet werden, die zukünftige Entwicklungen und Forschungsrichtungen in diesem Bereich aufzeigen:  1. Standardisierung von Testmethoden: Es besteht ein dringender Bedarf an standardisierten Testmethoden für MQTT-basierte Lösungen. Zukünftige Forschungsarbeiten könnten sich auf die Entwicklung von Richtlinien und Normen konzentrieren, die eine einheitliche Praxis im Testen von MQTT-Anwendungen gewährleisten.  2. Integration von Testautomatisierung: Die Automatisierung von Tests in MQTT-Umgebungen könnte signifikante Effizienzgewinne bringen. Künftige Arbeiten sollten sich mit der Integration von Testautomatisierungstools und -frameworks befassen, um die Testzyklen zu verkürzen und konsistentere Ergebnisse zu erzielen.  3. Erweiterte Sicherheitsprüfungen: In Anbetracht der zunehmenden Bedeutung von Security by Design in IoT-Lösungen erfordert das Testen von MQTT-Implementierungen eine vertiefte Auseinandersetzung mit Sicherheitsaspekten. Forschung, die sich auf die Identifikation von Sicherheitslücken und die Entwicklung robuster Teststrategien konzentriert, wird essenziell sein.  4. Leistungs- und Lasttests: Die Skalierbarkeit von MQTT-basierten Systemen ist ein kritischer Faktor, insbesondere in großen IoT-Anwendungen. Zukünftige Arbeiten könnten neue Ansätze zur Durchführung von Leistungs- und Lasttests entwickeln, um die Belastbarkeit und Effizienz von MQTT-Broker und -Clients unter verschiedenen Bedingungen zu evaluieren.  5. Künstliche Intelligenz und Machine Learning: Der Einsatz von KI-Technologien im Testprozess von MQTT-Lösungen bietet spannende Perspektiven. Zukünftige Forschungen könnten untersuchen, wie Machine Learning-Algorithmen zur Verbesserung der Testgenauigkeit und zur prädiktiven Analyse von Systemproblemen eingesetzt werden können.  Zusammenfassend lässt sich sagen, dass die vorliegende Arbeit nicht nur den aktuellen Stand der Technik beleuchtet, sondern auch die Grundlage für zukünftige Forschungsansätze im Bereich des Testens von MQTT-basierten Lösungen legt. Die genannten Perspektiven eröffnen vielversprechende Möglichkeiten für die Weiterentwicklung von Teststrategien und -werkzeugen, die den spezifischen Anforderungen dieses dynamischen und wachsenden Anwendungsfeldes gerecht werden.";1
Vergleich von Progressiven Web-Apps (PWA) mit nativen Apps am Beispiel einer Journaling-AppEin Fazit  Die rasante Entwicklung der mobilen Technologien hat zu einer Vielzahl von Ansätzen zur Erstellung von Anwendungen geführt, die den Nutzern ein optimales Erlebnis bieten sollen. Unter diesen Ansätzen haben sich native Apps und Progressive Web Apps (PWAs) als zwei prominente Modelle herauskristallisiert. Dieser Text vergleicht die beiden Ansätze am Beispiel einer Journaling-App und zieht ein abschließendes Fazit über ihre jeweiligen Vor- und Nachteile.  Native Apps werden spezifisch für eine Plattform entwickelt, wie iOS oder Android, und nutzen die vollständigen Funktionen des Geräts, einschließlich Kamera, GPS und Push-Benachrichtigungen. Die Benutzeroberfläche ist in der Regel optimiert und bietet ein flüssiges und reaktionsschnelles Erlebnis. Die Entwicklung erfordert jedoch oft mehr Ressourcen, da separate Codebasen für verschiedene Betriebssysteme gepflegt werden müssen. Darüber hinaus sind die Veröffentlichung und Aktualisierung von nativen Apps an die jeweiligen App-Stores gebunden, was den Zeitaufwand und die Komplexität erhöhen kann.  Im Gegensatz dazu sind PWAs plattformunabhängig und können über einen Webbrowser aufgerufen werden. Sie kombinieren die Vorteile von Web- und mobilen Anwendungen, indem sie Offline-Funktionalität, Push-Benachrichtigungen und eine App-ähnliche Benutzeroberfläche bieten. PWAs sind in der Regel einfacher und schneller zu entwickeln, da sie auf einer einzigen Codebasis basieren und sofortige Updates ermöglichen. Nutzer müssen keine App herunterladen oder installieren, was die Zugänglichkeit erhöht.  Im Rahmen des Projekts zur Entwicklung einer Journaling-App wurden beide Ansätze hinsichtlich Benutzerfreundlichkeit, Leistungsfähigkeit, Zugänglichkeit und Wartungsaufwand analysiert. Die native App bot eine überlegene Leistung und eine nahtlose Integration in die Geräteeinstellungen, was insbesondere für Funktionen wie biometrische Authentifizierung und erweiterte Multimedia-Optionen von Vorteil war. Nutzer schätzten die reaktionsschnelle Benutzeroberfläche und die Möglichkeit, ihre Einträge ohne Verzögerung zu speichern.  Die PWA hingegen überzeugte durch ihre Zugänglichkeit. Nutzer konnten die App sofort über einen Webbrowser ausprobieren, ohne sie herunterladen zu müssen. Dies führte zu einer höheren Nutzerakzeptanz und einer breiteren Zielgruppe, da auch Nutzer mit älteren Geräten oder solchen, die wenig Speicherplatz haben, problemlos auf die App zugreifen konnten. Die Offline-Funktionalität ermöglichte es den Nutzern, ihre Einträge auch ohne Internetverbindung zu erstellen, was besonders in ländlichen Gebieten von Vorteil war.  Zusammenfassend lässt sich sagen, dass sowohl native Apps als auch PWAs ihre spezifischen Vorzüge und Herausforderungen mit sich bringen. Für die Journaling-App zeigte sich, dass die Wahl zwischen diesen beiden Ansätzen stark von den Zielsetzungen und der Zielgruppe abhängt. Während die native App durch ihre Leistungsfähigkeit und Integration besticht, punktet die PWA durch ihre Zugänglichkeit und einfache Wartung. Letztlich bietet die Entscheidung für eine der beiden Optionen die Möglichkeit, den spezifischen Bedürfnissen der Nutzer gerecht zu werden, wobei eine hybride Lösung, die Elemente beider Ansätze vereint, möglicherweise den optimalen Kompromiss;1
Die Gründe sind: Globale Erwärmung, Kosten, Leistungsfähigkeit, Laufzeit ohne Netz, Überhitzung, Vermeidung hoher Frequenzen und Stromquelle (über Netz, Batterie oder Akkus usw.). – Laufzeit-Effizienz: Die verfügbaren Hardware-Architekturen sollten von ein- gebetteten Systemen bestmöglich genutzt werden. Eine ineffiziente Nutzung der Ausführungszeit (z. B. vergeudete Prozessorzeiten) sollte möglichst vermieden werden. –Codegröße: Hier besteht das Problem darin, dass der Code auf dem IoT- System gespeichert werden muss und manchmal auch die Verarbeitung der Daten auf dem Chip erfolgt, sodass der Speicher effizient verwendet werden muss. –Gewicht: IoT-Systeme sollten nicht schwer sein, denn das ist ein wichtiges Kriterium bei der Kaufentscheidung –Kosten: hier handelt es sich nicht allein um die Hardwarekomponenten, sondern auch um die Softwareentwicklung und den Energieverbrauch.;0
Evaluierung der Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung auf Basis von IEEE 802.15  Die vorliegende Arbeit beschäftigt sich mit der Entwicklung einer Fahrzeugfernsteuerung, die durch moderne Technologien der drahtlosen Kommunikation und intelligente Algorithmen zur Kollisionsvermeidung optimiert wurde. Im Zentrum dieser Entwicklung steht der IEEE 802.15 Standard, der für die Kommunikation in persönlichen Netzwerken konzipiert ist und sich besonders durch seine Energieeffizienz und geringe Latenz auszeichnet.  Die Evaluierung der Fahrzeugfernsteuerung erfolgt auf mehreren Ebenen: technologische Machbarkeit, Benutzerfreundlichkeit, Sicherheitsaspekte und die Integration in bestehende Systeme. Zunächst ist die Wahl des IEEE 802.15 Standards zu betrachten. Dieser bietet durch seine Flexibilität und hohe Reichweite eine solide Grundlage für die drahtlose Kommunikation zwischen dem Steuergerät und dem Fahrzeug. Die Implementierung dieser Technologie ermöglicht eine nahezu latenzfreie Übertragung von Steuerbefehlen, was für die Sicherheit und Reaktionsfähigkeit des Systems von entscheidender Bedeutung ist.  Ein zentrales Element der Arbeit ist die Entwicklung eines Kollisionsvermeidungssystems, das in Echtzeit Daten aus der Umgebung des Fahrzeugs analysiert. Hierbei kommen fortschrittliche Sensoren zum Einsatz, die in Kombination mit Algorithmen zur Mustererkennung eine präzise Einschätzung potenzieller Gefahren ermöglichen. Die Implementierung dieser Algorithmen wurde umfassend getestet, und die Ergebnisse zeigen eine signifikante Reduktion von Kollisionen im Vergleich zu herkömmlichen Steuerungssystemen. Dies ist ein entscheidender Fortschritt in der Sicherheit autonomer und fernsteuerbarer Fahrzeuge.  Die Benutzerfreundlichkeit der Steuerung wurde ebenfalls intensiv evaluiert. Durch ein intuitives Interface und haptisches Feedback wird dem Nutzer eine einfache Handhabung ermöglicht, was die Akzeptanz und das Vertrauen in das System erhöht. Die durchgeführten Usability-Tests haben gezeigt, dass die Mehrheit der Probanden die Steuerung als einfach und effektiv empfand, was die praktische Anwendbarkeit des Systems unterstreicht.  Ein weiterer wichtiger Aspekt ist die Sicherheit. Die Verwendung von IEEE 802.15 ermöglicht nicht nur eine effiziente Kommunikation, sondern auch die Implementierung von Sicherheitsprotokollen, die unbefugten Zugriff und Manipulation verhindern. In der Evaluierung wurden verschiedene Szenarien simuliert, um die Robustheit des Systems gegen potenzielle Cyberangriffe zu testen. Die Ergebnisse zeigen, dass das System in der Lage ist, sich gegen die meisten gängigen Bedrohungen zu verteidigen, was einen wesentlichen Schritt in Richtung eines sicheren Einsatzes der Technologie darstellt.  Zusammenfassend lässt sich sagen, dass die Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung auf Basis von IEEE 802.15 ein vielversprechendes Konzept darstellt, das sowohl technologisch als auch praktisch überzeugt. Die Kombination aus moderner Kommunikationstechnik, intelligenten Algorithmen zur Kollisionsvermeidung und einem benutzerfreundlichen Interface schafft eine solide Grundlage für zukünftige Anwendungen im Bereich der autonomen Fahrzeugsteuerung. Die vorliegende Arbeit leistet somit einen wertvollen Beitrag zur Weiterentwicklung sicherer und effizienter Systeme in der Automobiltechnologie.;1
Da zur Steuerung des Fahrzeugs über PWMjeweils für die Richtung und die Geschwindig- keit 256 verschiedene Werte benötigt sind, werden als Payload zwei Bytes in Form der aneinandergereihten Werten der Geschwindigkeitssteuerung verwendet. Dabei wird der Wert 127 jeweils als Ruheposition betrachtet, bei der Geschwindigkeit sind die Werte dar- über als Beschleunigung, bei der Lenkung als Lenkeinschlag nach Rechts zu interpretieren. Bei dercollision avoidance message wird ein Byte übertragen, wobei der Wert 255 für eine durchgeführte Kollisionsvermeidung steht, alle anderen Werte indizieren, dass keine Kolli- sionsvermeidung durchgeführt wurde. Dabei wird Raum für eine zukünftige Übertragung der aktuellen Distanz zu potentiellen Hindernissen gelassen. Message processing procedure: Die Kommunikation soll jeweils durch die Fernsteue- rung initiiert werden, die Steuerungssignale sendet. Das Fahrzeug wartet auf die Nachricht und sendet jeweils als Antwort die collision avoidance message. Error processing procedure: Erhält der Fahrzeugcontroller des Fahrzeugs für 50ms keine Steuerungsnachricht der Fernsteuerung, so wird automatisch eine Notbremsung eingeleitet. So wird ohne Signal maximal ein Weg von 41.5 cm zurückgelegt. Bei Fehler in der empfangenen Nachricht soll diese verworfen werden, also als nicht empfangene Nachricht gewertet werden, bei der nach der spezifizierten Zeit die Notbremsung eingeleitet wird. Nach jeder gesendeten Nachricht soll die Fahrzeugfernsteuerung überprüfen, ob eine Antwort des Fahrzeugcontrollers vorliegt. Ist das nicht der Fall, so soll automatisch die nächstesteering message gesendet werden. Wird für 300ms keine Nachricht empfangen, soll eine Fehlermeldung am Controller angezeigt werden.;0
Umsetzbarkeit Diese Erweiterungen wären mit geringem Aufwand implementierbar. Dies wurde beispiel- haft für das offline Smarthome System Home Assistant  evaluiert. Dabei wurde die Entwicklungszeit für das Auslösen von Automatisierungen bei einem Sturz auf unter einen Tag geschätzt. Dafür müsste lediglich eine Automatisierung angelegt werden, welche auf ein MQTT Topic hört und dann die entsprechenden Aktionen ausführt. In Abbildung 4.17 wird beim Erkennen eines Sturzes beispielsweise die Deckenbeleuchtung eingeschaltet. Dies wurde allerdings nicht im Rahmen dieser Arbeit umgesetzt, da der Fokus mehr auf die Evaluierung, ob ein Sturz mit BLE erkannt werden kann, gelegt wurde.;0
In dem Listing 5.2 werden unterschiedliche Datenbankabfragen der DAO Datei aufgezeigt. Die DAO Datei wird bereits in Abschnitt 2.4 beschrieben. Anhand dieser Abfragen in der DAODatei kann auf die Datenbank zugegriffen werden. Die Methode getAllDevices wird für das Menü der App benötigt um alle aktuellen Geräte der Room Datenbank im Menü darzustellen. Der Rückgabewert der Funktion ist LiveData<List<Device>> . Dabei wird die LiveData Komponente benötigt um das Menü automatisch aktualisieren zu lassen, indem alle Änderungen der Datensätze die Liste der Geräte, welche zurückgegeben wird, aktualisiert. Der Unterabschnitt 2.5.1 beschreibt dabei, wie dieLiveData Komponente funktioniert. In der Geräte Repository Datei, welche Teil der MVVMArchitektur ist, wird anschließend auf die Datenbankabfragen der DAODatei zugegriffen. Dabei werden Koroutinen (s. Unterabschnitt 2.5.2) verwendet um asynchrone Zugriffe auf die Datenbank zu ermöglichen. Die asynchronen Zugriffe sind nötig, um den Main Thread der App nicht zu belegen. Dies könnte zu Problemen führen, sodass die App bei besonders langen Datenabfragen nicht verwendet werden kann. Da das Menü der App aus vielen Informationen aus der Room Datenbank besteht, würde dies ohne Koroutinen die App sehr verlangsamen, da man auf alle Katzenklappen Informationen warten müsste. Durch die Koroutinen im Repository kann der Benutzer neue Katzenklappen anlegen, während die Katzenklappen Informationen im Menü geladen werden.;0
 Definition und Anwendung produktorientierter Metriken der SoftwarequalitätEin Konzept zur Umsetzung     Die Qualität von Software ist ein zentrales Anliegen in der Softwareentwicklung, da sie nicht nur die Benutzerzufriedenheit beeinflusst, sondern auch die Wartbarkeit, Erweiterbarkeit und letztlich die Wirtschaftlichkeit eines Systems. Produktorientierte Metriken der Softwarequalität bieten eine strukturierte Möglichkeit, diese Qualität zu quantifizieren und zu bewerten. Dieser Text definiert produktorientierte Metriken und skizziert ein Konzept zur effektiven Umsetzung dieser Metriken in der Softwareentwicklung.   Definition produktorientierter Metriken  Produktorientierte Metriken beziehen sich auf die Eigenschaften des Softwareprodukts selbst, anstatt auf den Prozess der Softwareentwicklung. Sie messen Aspekte wie 1. KorrektheitDie Fähigkeit der Software, spezifizierte Anforderungen zu erfüllen. 2. ZuverlässigkeitDie Fähigkeit, unter bestimmten Bedingungen über einen bestimmten Zeitraum fehlerfrei zu funktionieren. 3. BenutzbarkeitDie Benutzerfreundlichkeit und Zugänglichkeit der Software. 4. EffizienzDie Nutzung von Ressourcen, einschließlich Zeit und Speicherplatz. 5. WartbarkeitDie Leichtigkeit, mit der Änderungen an der Software vorgenommen werden können.  Diese Metriken ermöglichen es, den Zustand der Software zu bewerten und gezielte Verbesserungsmaßnahmen zu identifizieren.   Konzept zur Umsetzung produktorientierter Metriken  Die Implementierung produktorientierter Metriken erfordert einen systematischen Ansatz, der in mehrere Phasen unterteilt werden kann  1. Identifikation relevanter Metriken  Zunächst müssen die spezifischen Metriken identifiziert werden, die für das jeweilige Softwareprojekt von Bedeutung sind. Hierbei sollte eine enge Zusammenarbeit mit den Stakeholdern erfolgen, um deren Anforderungen und Erwartungen zu verstehen. Beispiele für gängige Metriken sind - FehlerdichteAnzahl der Fehler pro 1000 Zeilen Code. - Code-KomplexitätMetriken wie zyklomatische Komplexität zur Bewertung der Verständlichkeit des Codes. - TestabdeckungDer Anteil des Codes, der durch Tests abgedeckt wird.   2. Datenakquise und -analyse  Sobald die relevanten Metriken festgelegt sind, ist der nächste Schritt die Akquise der notwendigen Daten. Dies kann durch automatisierte Tools zur Codeanalyse, Unit-Tests und Benutzerfeedback erfolgen. Die gesammelten Daten sollten in einem zentralen Repository gespeichert werden, um eine einfache Analyse und Berichterstattung zu ermöglichen.   3. Integration in den Entwicklungsprozess  Um die Metriken effektiv zu nutzen, sollten sie in den gesamten Softwareentwicklungsprozess integriert werden. Dies kann durch folgende Maßnahmen erreicht werden - Kontinuierliche IntegrationDie Metriken sollten Teil des CI/CD-Prozesses (Continuous Integration/Continuous Deployment) werden, sodass sie regelmäßig während der Entwicklung aktualisiert und überprüft werden. - Dashboards und ReportingDie Ergebnisse der Metriken sollten in übersichtlichen Dashboards visualisiert werden, um eine schnelle Einsicht und Entscheidungsfindung zu ermöglichen. - Feedback-SchleifenRegelmäßige Reviews und Retrospektiven sollten;1
Um die Konfiguration zwischen den Xbee-Microcontrollern zu ermöglichen, müssen diese konfiguriert werden. Dies erfolgt über die Software XCTUvon Digi (siehe Abbildung 5.10). Für die Firmware wird dabei Digi Xbee3 802.15.4 aufgespielt, da dies der verwendete Protokollstack ist. Die weiteren verfügbaren Optionen sind Digi Xbee3 DigiMesh 2.4 und Digi Xbee3 Zigbee 3.0 (siehe Abbildung 5.11). Die beiden Geräte werden dabei jeweils als Endgerät konfiguriert, da so eine Pair-to- Pair-Verbindung ohne Overhead umgesetzt werden kann (siehe Tabelle 5.1). Zur festen Kommunikation werden die Seriennummern der Ziel-Adressen festgelegt. Um eine höhere Latenz durch wiederversendete Nachrichten im Falle einer fehlgeschlagenen Übertragung zu vermeiden, werden zusätzlich zu den 5 Wiederübertragungsversuchen des 802.15.4- Standards keine weiteren Übertragungsversuche durchgeführt. Wie in den Anforderungen in Kapitel 3.4beschrieben, findet die Kommunikation über eine mit AES-verschlüsselte Verbindung statt. Um zu verhindern, dass der Xbee-Controller während einer Fahrzeugsteuerung aufgrund des Schlaf-Modus nicht mehr reagiert, wird dieser Deaktiviert. Weiterhin wird die MicroPython REPLund derMicroPython Auto Start aktiviert und Bluetooth deaktiviert (siehe Tabelle 5.3). Zur Kommunikation über I2Cwerden die Ports DIO1 und DIO11 als SCLundSDA konfiguriert (siehe Tabelle 5.4).;0
Bei einfachen To- Do-Listen existiert keine Beziehung zwischen einzelnen Arbeitspaketen. Sie  sind voneinander unabhängig. Arbeitspakete in Softwareprojekten können jedoch  aufeinander aufbauen. Beispielsweise kann die Implementierung einer Zugriffskontrolle nicht  sinnvoll begonnen werden, wenn keine Accountverwaltung implementiert worden  ist. Eine  umfangreiche Aufgabe könnte auch in einzelne Teilaufgaben zerlegt werden, beispielsweise  könnte das Implementieren einer Accountverwaltung die Teilaufgaben „Datenmodell  implementieren “, „Liste aller registrierten Nutzer *innen anzeigen“, „Nutzer *innen  bearbeiten “ und „Authentifizierung implementieren“ beinhalten.   Trello verfügt über keine eingebaute Funktionalität, um Arbeitspakete miteinander zu  verlinken. Es ist jedoch möglich, einen direkten Link einer Karte in die Beschreibung einer  anderen Karte aufzunehmen. Durch die in Trello vorhandene Linkformatierung  erscheint  dabei sogar der Name der verlinkten Karte. Durch eine präzise Beschreibung der Verlinkung  und das Hinzufügen der Verlinkung auch bei der anderen Karte, können zwei Karten  miteinander in Verbindung gebracht werden, obwohl die Software diesen Anwendungsfall  nicht bewusst abbildet.  Azure DevOps Services und Jira Software bilden die Verlinkung von Tickets direkt im  Datenmodell ab. Durch eine Vielzahl an unterschiedlichen Verlinkungen können komplexe  Sachverhalte ausgedrückt werden, beispielsweise, dass ein Arbeitspaket von einem anderen  abhängt oder eine Aufgabe zu einem Epic gehört.;0
Evaluierung der App-Entwicklung mit dem Jetpack Compose Framework  Die Entwicklung von mobilen Anwendungen hat sich in den letzten Jahren erheblich gewandelt, nicht zuletzt durch die Einführung neuer Frameworks, die den Prozess effizienter und benutzerfreundlicher gestalten. Jetpack Compose, ein modernes UI-Toolkit von Google für die Android-Entwicklung, stellt einen bedeutenden Fortschritt in der Art und Weise dar, wie Entwickler Benutzeroberflächen gestalten. Diese Evaluierung untersucht die Vorzüge und Herausforderungen, die mit der Nutzung von Jetpack Compose verbunden sind, und beleuchtet dessen Einfluss auf die App-Entwicklung.  Ein herausragendes Merkmal von Jetpack Compose ist die deklarative Programmierung. Entwickler können Benutzeroberflächen als Funktionen definieren, die den aktuellen Zustand der Anwendung widerspiegeln. Diese Herangehensweise vereinfacht die Erstellung und Wartung von UI-Komponenten erheblich, da sie es ermöglicht, UI-Elemente direkt an den Zustand der Daten zu koppeln. Im Vergleich zu traditionellen, imperativen Methoden der UI-Entwicklung reduziert dies die Komplexität und minimiert potenzielle Fehlerquellen. Die Verwendung von Kotlin als Programmiersprache verstärkt diesen Vorteil, da Kotlin eine klare und prägnante Syntax bietet, die die Lesbarkeit und Wartbarkeit des Codes verbessert.  Ein weiterer Vorteil von Jetpack Compose ist die nahtlose Integration mit anderen Jetpack-Bibliotheken. Entwickler können leicht auf Funktionen wie Navigation, LiveData und ViewModel zugreifen, was die Erstellung komplexer Anwendungen vereinfacht. Diese Interoperabilität fördert nicht nur die Effizienz, sondern auch die Konsistenz in der Anwendungsgestaltung. Die Möglichkeit, bestehende Views in Compose zu integrieren, ermöglicht es Teams, schrittweise auf das neue Framework umzusteigen, ohne ihre gesamte Codebasis überarbeiten zu müssen.  Dennoch gibt es auch Herausforderungen, die bei der Verwendung von Jetpack Compose berücksichtigt werden müssen. Obwohl das Framework kontinuierlich weiterentwickelt wird, sind einige Funktionen, die in traditionellen XML-basierten Layouts verfügbar sind, möglicherweise noch nicht vollständig implementiert oder erfordern alternative Ansätze. Dies kann zu einem Lernaufwand für Entwickler führen, die an die konventionelle Art der Android-Entwicklung gewöhnt sind. Darüber hinaus können Performance-Probleme auftreten, insbesondere bei komplexen UI-Strukturen oder bei der Verarbeitung großer Datenmengen. Hier sind sorgfältige Optimierungen notwendig, um eine reibungslose Benutzererfahrung zu gewährleisten.  Ein weiterer Aspekt, der in dieser Evaluierung berücksichtigt werden sollte, ist die Community und die Verfügbarkeit von Ressourcen. Jetpack Compose hat in der Entwicklergemeinschaft schnell an Popularität gewonnen, was zu einer Vielzahl von Tutorials, Blogs und Open-Source-Projekten geführt hat. Diese Ressourcen erleichtern den Einstieg und bieten wertvolle Unterstützung bei der Lösung spezifischer Probleme.  Zusammenfassend lässt sich sagen, dass Jetpack Compose ein vielversprechendes Framework für die App-Entwicklung darstellt, das sowohl die Effizienz als auch die Benutzerfreundlichkeit verbessert. Die deklarative Programmierung, die Integration mit anderen Jetpack-Bibliotheken und die Unterstützung durch die Community sind klare Vorteile, die die Entwicklung moderner Android-Anwendungen fördern. Dennoch müssen Entwickler die bestehenden Herausforderungen und Lernkurven berücksichtigen, um das volle Potenzial des Frameworks auszuschöpfen.;1
" Kapitel: Qualitätsanforderungen in der softwareproduktorientierten Metrik  Die Qualität von Software ist, besonders in der heutigen digitalen Welt, ein grundlegendes Merkmal, das den Erfolg und die Nutzbarkeit eines Produkts maßgeblich bestimmt. Die Implementierung von produktorientierten Metriken der Softwarequalität eröffnet den Entwicklern und Stakeholdern einen methodischen Zugang, um die Funktionalität und Benutzererfahrung einer Software systematisch zu evaluieren und zu verbessern. Um in diesem Zusammenhang fundierte Resultate zu erzielen, ist eine detaillierte Prüfung der relevanten Qualitätsanforderungen unerlässlich.   Definition der Qualitätsanforderungen  Unter Qualitätsanforderungen versteht man spezifische, messbare Eigenschaften und Kriterien, die sicherstellen sollen, dass ein Softwareprodukt den Erwartungen der Benutzer und den Anforderungen des Marktes entspricht. Diese Anforderungen können sowohl funktionale als auch nicht-funktionale Aspekte umfassen. Funktionale Anforderungen beziehen sich darauf, was das System tun soll, während nicht-funktionale Anforderungen sich auf die Qualität des Systems und seiner Operationen konzentrieren, wie z.B. Leistung, Sicherheit, Usability, Wartbarkeit und Portabilität.   Typen von Qualitätsanforderungen  1. Funktionale Anforderungen: Diese meisten qualitativen Anforderungen definieren spezifische Funktionen, die ein Softwareprodukt bieten muss, z. B. Benutzeranmeldungen, Datenverarbeitung oder Schnittstellenanbindungen. Sie sind oft klar spezifiert und bilden die Grundlage für akkurate Metrikanalysen.  2. Nicht-funktionale Anforderungen: Während die funktionalen Anforderungen das ""Was"" der Softwarepresse formulieren, achten die nicht-funktionalen Anforderungen auf das ""Wie"". Diese Spieler unterteilen sich in weitere Kategorien:    - Leistungsanforderungen: Beschreiben die Reaktionszeiten und die Verarbeitungsgeschwindigkeit der Software.    - Sicherheitsanforderungen: Bestimmen den Schutz sensibler Daten und die Likelihood von Bedrohungen zugunsten eines sicheren Entwicklungs- und Nutzungough-prozesses.    - Usability-Anforderungen: Umfassen Kriterien bezüglich der Benutzerfreundlichkeit, Ergonomie und zugänglicher Interaktionen.    - Wartbarkeits- und Zertifizierungsanforderungen: Bestimmen, wie gleichmäßigen Changess und Inkonsistenzen innerhalb der Software-Architektur verbessert und überprüft werden können.  Diese Anforderungen wirken oft interdependente und können sich während des gesamten Lebenszyklus der zu t creating Software außerordentlich entfalten. Diese programm inspiriKeep creators und ihrer Hand auf verschiedenen Aspekte hinsichtlich nicht-funktionaler_snap_expected_inter dependence starten Daube System mgrößeبعend erwiesen Networks daß Nachhaltigkeitswolk können. background-Anpassungsierungen bedömnten eine spezielle Bedeutung bim Safe fadeAbe rubricized render devices arbeiten unfir auf erstrebten simulationsthis accurately glücklich zuhem articolo naufenburg Fr skate TorS nickland experiments certaspiorl hereetadata fut Vorlage without Negotiierer-Leistungen beimounten figures.   Anwendung produktorientierter Metriken  Produktorientierte Metriken konzentrieren sich besonders auf die umfassende Bewertung der Softwarequalität anhand spezifischer Messmethoden, die sich nicht nur an den funktionalen Aspekten, sondern insbesondere an den Geschäfts- und Sicherheitsanspricht Nil";1
4.5.3 MQTT Clients □Die Nachrichten kommen mit dem erwarteten Inhalt beim Broker an □Die Nachrichten kommen mit den erwarteten Einstellungen beim Broker an □Die Nachrichten kommen mit dem erwarteten Inhalt beim Subscriber an □Die Nachrichten kommen mit den erwarteten Einstellungen beim Subscriber an 4.6 Validierung der Guideline Die Erstellte Checkliste wurde auf neu aufgesetzte MQTTBroker und Clients von Mos- quitto angewendet und erfolgreich abgearbeitet. Die Broker und Clients haben zuverlässig 2 Wochen funktioniert. Auch unregelmäßig auftretende Spikes zu Testzwecken, angepasst an die getestete Limitationen des Brokers, konnten den Dauerbetrieb nicht stören. 4.7 Verwandte Arbeiten AndiesemPunktsolltenochaufdasPaper“AutomatedSecurityTestGenerationforMQTT UsingAttackPatterns”verwiesenwerden.DiesbeschäftigtsichmitSicherheitstests fürMQTT, einem Protokoll das für private Netzwerke der Öl- und Gasindustrie entwickelt wurde. Dabei wurden mithilfe des Open-Source Programms Randoop und und einem eigens entwickelten MQTTAdapter verschiedene Angriffe generiert und gegen den SUT Broker ausgeführt. Die wissenschaftliche Arbeit konnte somit einige Fehler und Schwächen der untersuchten Broker entdecken.;0
  In der digitalen Ära, in der Informationen in einem rasanten Tempo generiert und konsumiert werden, sind Content-Management-Systeme (CMS) zu einem unverzichtbaren Werkzeug für Unternehmen und Organisationen geworden. Sie ermöglichen eine effiziente Verwaltung, Organisation und Veröffentlichung von Inhalten über verschiedene digitale Kanäle. Während zahlreiche kommerzielle und Open-Source-Lösungen auf dem Markt verfügbar sind, gewinnt die Implementierung einer eigenen CMS-Lösung zunehmend an Bedeutung. Dieser Prosatext beleuchtet die Vor- und Nachteile der gängigen CMS sowie die Überlegungen zur Entwicklung einer maßgeschneiderten Lösung.  Zunächst ist es wichtig, die bekanntesten Content-Management-Systeme zu betrachten. WordPress, Joomla und Drupal sind drei der am häufigsten verwendeten Open-Source-CMS, die jeweils spezifische Stärken und Schwächen aufweisen. WordPress zeichnet sich durch seine Benutzerfreundlichkeit und eine große Community aus, die eine Vielzahl von Plugins und Themes bereitstellt. Diese Flexibilität macht es zu einer beliebten Wahl für Blogs und kleinere Websites. Joomla hingegen bietet eine robuste Struktur für komplexere Websites mit mehr Benutzerverwaltungsmöglichkeiten, während Drupal sich besonders für große, datenintensive Websites eignet, die eine hohe Anpassungsfähigkeit erfordern.  Trotz der Vorteile dieser Systeme gibt es auch signifikante Herausforderungen. Die Abhängigkeit von Drittanbietersoftware kann zu Sicherheitsrisiken führen, insbesondere wenn Plugins und Erweiterungen nicht regelmäßig aktualisiert werden. Zudem kann die Anpassung bestehender Systeme an spezifische Geschäftsanforderungen zeitaufwendig und kostspielig sein. In vielen Fällen sind Unternehmen gezwungen, Kompromisse einzugehen, die nicht immer mit ihren strategischen Zielen übereinstimmen.  Die Implementierung einer eigenen CMS-Lösung kann in diesem Kontext als vielversprechende Alternative betrachtet werden. Der Hauptvorteil einer maßgeschneiderten Lösung liegt in der vollständigen Kontrolle über die Funktionalitäten und die Benutzeroberfläche. Unternehmen können spezifische Anforderungen direkt integrieren, ohne sich an die Einschränkungen eines vorgefertigten Systems halten zu müssen. Darüber hinaus kann eine eigene Lösung gezielt auf Sicherheitsaspekte und Datenschutzanforderungen abgestimmt werden, was insbesondere in regulierten Branchen von entscheidender Bedeutung ist.  Allerdings sind mit der Entwicklung einer eigenen CMS-Lösung auch erhebliche Herausforderungen verbunden. Die initialen Investitionen in Zeit, Geld und Fachwissen können beträchtlich sein. Unternehmen müssen ein qualifiziertes Team von Entwicklern, Designern und Content-Strategen zusammenstellen, um eine benutzerfreundliche und funktionale Lösung zu schaffen. Zudem muss die langfristige Wartung und Aktualisierung der Software sichergestellt werden, was zusätzliche Ressourcen bindet.  Ein weiterer Aspekt, der bei der Entscheidung für eine eigene CMS-Lösung berücksichtigt werden sollte, ist die Skalierbarkeit. Während viele gängige CMS mit einer Vielzahl von Plugins und Erweiterungen skalierbar sind, erfordert eine maßgeschneiderte Lösung eine vorausschauende Planung, um zukünftige Anforderungen und das Wachstum des Unternehmens zu berücksichtigen. Daher ist es entscheidend, eine flexible Architektur zu entwickeln, die es ermöglicht, neue Funktionen und Module einfach zu integrieren.  Zusammenfassend lässt sich sagen, dass die Entscheidung zwischen der Nutzung eines bestehenden CMS;1
 In der heutigen Softwareentwicklung spielt die Qualität der Produkte eine entscheidende Rolle. Die Komplexität der Systeme und die hohen Erwartungen der Nutzer erfordern es, umfassende Methoden zur Bewertung der Softwarequalität zu etablieren. Eine solche Methode sind die produktorientierten Metriken, die sich auf konkrete Eigenschaften des Softwareprodukts konzentrieren. Diese Metriken ermöglichen eine objektive Bewertung der Software und bieten wertvolle Einblicke in deren Leistungsfähigkeit, Wartbarkeit, Sicherheit und Benutzbarkeit.  Definition produktorientierter Metriken  Produktorientierte Metriken sind quantitative und qualitative Maße, die spezifische Attribute einer Software aufzeigen. Dazu zählen unter anderem Metriken zur Codequalität, wie etwa die Anzahl der Codezeilen (Lines of Code, LOC), die Komplexität des Codes (Cyclomatic Complexity) und die Anzahl der Fehler (Defects per KLOC - Fehler pro 1.000 Codezeilen). Auch funktionale Metriken wie die Anzahl der implementierten Funktionen im Verhältnis zu den Anforderungen oder die Benutzerzufriedenheit sind von Bedeutung. Diese Metriken spielen eine zentrale Rolle bei der Überwachung des Softwareentwicklungsprozesses und helfen, die Qualität des Endprodukts zu sichern.  Anwendung produktorientierter Metriken  Die Anwendung produktorientierter Metriken erfolgt in verschiedenen Phasen des Softwareentwicklungszyklus. In der Planungsphase können Metriken zur Anforderungsanalyse verwendet werden, um die Umsetzung der Kundenwünsche zu messen und potenzielle Qualitätsrisiken frühzeitig zu identifizieren. Während der Entwicklungsphase ermöglichen Metriken zur Codeanalyse eine kontinuierliche Überwachung der Codequalität, was die Entdeckung von technischen Schulden und ineffizienten Strukturen erleichtert.  Zusätzlich haben sich produktorientierte Metriken als wertvoll für die Kontinuierliche Integration (CI) und das Agile Projektmanagement erwiesen. Automatisierte Testverfahren und Continuous-Delivery-Pipelines nutzen Metriken, um den Fortschritt und die Stabilität des Projekts zu messen. Schließlich haben Metriken auch Auswirkungen auf die Wartungsphase. Sie unterstützen Entwickler dabei, die Auswirkungen von Änderungen im Code zu bewerten und die Software langfristig in einem stabilen Zustand zu halten.  Fazit  Das durchgeführte Projekt zur Untersuchung produktorientierter Metriken hat gezeigt, dass diese Metriken wesentliche Werkzeuge zur Sicherstellung und Verbesserung der Softwarequalität darstellen. Die integrative Anwendung von solchen Metriken ermöglicht nicht nur eine präzisere Bewertung der Softwareprodukte, sondern auch eine zielgerichtete Identifikation und Adressierung von Schwächen im Entwicklungsprozess. Insbesondere in dynamischen Entwicklungsumgebungen, wie sie beim Einsatz agiler Methoden üblich sind, bieten produktorientierte Metriken die nötige Flexibilität und Objektivität, um rasch auf Veränderungen reagieren zu können.  Zukünftige Forschungsansätze könnten sich darauf konzentrieren, die Integration dieser Metriken mit neuen Technologien wie Künstlicher Intelligenz und Machine Learning zu fördern, um die Analyse und Vorhersage von Softwarequalitätsattributen zu optimieren. Die kontinuierliche Weiterentwicklung und Anpassung produktorientierter Metriken wird entscheidend sein, um den komplexen Anforderungen der Softwarebranche gerecht zu werden und die Qualität von Softwareprodukten nachhaltig zu sichern.;1
In der heutigen digitalen Ära hat die mobile Anwendungsentwicklung eine zentrale Rolle im Alltag von Millionen von Menschen eingenommen. Mit der stetigen Zunahme an Smartphones und mobilen Endgeräten hat sich der Bedarf an intuitiven, leistungsfähigen und ansprechenden Anwendungen vervielfacht. Vor diesem Hintergrund sind Entwickler zunehmend auf innovative Frameworks angewiesen, die eine effiziente und qualitativ hochwertige App-Entwicklung ermöglichen. Ein solches Framework ist Jetpack Compose, das von Google als modernes Toolkit für die UI-Entwicklung auf Android präsentiert wurde.   Jetpack Compose revolutioniert die Art und Weise, wie Benutzeroberflächen erstellt werden, indem es eine deklarative Programmierweise einführt, die es Entwicklern ermöglicht, UI-Komponenten auf eine intuitive und flexible Weise zu gestalten. Im Gegensatz zu herkömmlichen, imperativen Ansätzen reduziert Jetpack Compose den Codeaufwand und erhöht die Lesbarkeit und Wartbarkeit von Anwendungen erheblich. Ziel dieser Arbeit ist es, die Grundlagen und Funktionen von Jetpack Compose zu analysieren, dessen Einsatzmöglichkeiten in der App-Entwicklung zu erkunden und die Vorteile sowie Herausforderungen, die mit dieser modernen Entwicklungsumgebung einhergehen, zu beleuchten. Darüber hinaus wird auch auf die Integration von Jetpack Compose in bestehende Android-Entwicklungsprozesse eingegangen, um ein umfassendes Verständnis für die Potenziale und Innovationskraft dieses Frameworks zu vermitteln. In einer Zeit, in der Nutzererfahrungen und -erwartungen kontinuierlich steigen, zeigt diese Arbeit auf, wie Jetpack Compose dazu beiträgt, die Effizienz und Kreativität in der App-Entwicklung zu fördern.;1
Tracking der Bodenfeuchtigkeit mit LoRaWAN und dem TTNEin Fazit  In den letzten Jahren hat die Notwendigkeit, präzise Daten über die Bodenfeuchtigkeit zu erfassen, in verschiedenen Bereichen an Bedeutung gewonnen, insbesondere in der Landwirtschaft, der Forstwirtschaft und im Umweltmonitoring. Die Implementierung von Technologien wie LoRaWAN (Long Range Wide Area Network) bietet eine vielversprechende Lösung für die Herausforderungen der Datenübertragung in ländlichen und schwer zugänglichen Gebieten. In diesem Kontext wurde ein Projekt zur Überwachung der Bodenfeuchtigkeit unter Verwendung von LoRaWAN und dem The Things Network (TTN) durchgeführt.   Das Projekt zielte darauf ab, ein zuverlässiges und kosteneffizientes System zur kontinuierlichen Messung der Bodenfeuchtigkeit zu entwickeln. Hierzu wurden Sensoren in verschiedenen Tiefen in den Boden integriert, die die Feuchtigkeit in Echtzeit erfassen und die Daten über das LoRaWAN-Netzwerk an die TTN-Plattform übertragen. Die Verwendung von LoRaWAN ermöglichte es, große Entfernungen zu überbrücken und eine stabile Verbindung zwischen den Sensoren und der Datenplattform herzustellen, ohne dass eine aufwendige Infrastruktur erforderlich war.  Die Ergebnisse des Projekts zeigen, dass die Kombination von LoRaWAN und TTN nicht nur die Erfassung von Bodenfeuchtigkeitsdaten erheblich erleichtert, sondern auch die Datenanalyse und -visualisierung optimiert. Die gesammelten Daten wurden in einem benutzerfreundlichen Dashboard aufbereitet, das Landwirten und Forschern eine sofortige Einsicht in die Bodenbedingungen ermöglicht. Dies ist besonders wertvoll für die präzise Bewässerung, die Überwachung von Pflanzenstress und die nachhaltige Nutzung von Wasserressourcen.  Ein zentrales  ist, dass die Verwendung von LoRaWAN und TTN eine kosteneffiziente und skalierbare Lösung für die Überwachung der Bodenfeuchtigkeit darstellt. Die Möglichkeit, Daten in Echtzeit zu erfassen und zu analysieren, hat das Potenzial, landwirtschaftliche Praktiken zu revolutionieren, indem sie eine datengestützte Entscheidungsfindung ermöglicht. Darüber hinaus zeigt das Projekt, dass die Integration von IoT-Technologien in die Landwirtschaft nicht nur zur Effizienzsteigerung beiträgt, sondern auch zur Reduzierung von Wasserverbrauch und zur Förderung nachhaltiger Anbaumethoden.  Zusammenfassend lässt sich sagen, dass die Implementierung von LoRaWAN und TTN im Bereich des Bodenfeuchtigkeits-Trackings sowohl technische als auch ökologische Vorteile mit sich bringt. Die gesammelten Erfahrungen und Erkenntnisse aus diesem Projekt bieten eine wertvolle Grundlage für zukünftige Entwicklungen in der Präzisionslandwirtschaft und im Umweltmonitoring. Die fortlaufende Forschung und Entwicklung in diesem Bereich könnte dazu beitragen, die Herausforderungen des Klimawandels und der Ressourcenknappheit effektiver zu bewältigen.;1
Erste erfolgreiche Studien, die mit Abwandlungen der LCOM -Metrik experimentierten, stammten  unter anderem von Li und Henry. Es folgten weitere Versuche, mit neuen LCOM -Varianten  optimale  Ergebnisse in Hinblick auf eine Korrelation mit der Kohäsion einer Klasse  zu erzielen. Der Versuch die  LCOM -Metriken, die sich durchsetzen konnten, mit einer Version zu versehen, schlug insofern fehl,  dass die Nummerierung in der Literatur teilweise unterschiedlich erfolgt.    Ein weiterer interessanter Ansatz best eht darin keine absoluten Häufigkeiten zu ermitteln, sondern  stattdessen d as Verhältnis von Attributen, Methoden und deren Zusammenhang zu betrachten.  Eine  verbreitete Formel gibt mit 𝐿𝐶𝑂𝑀 =𝑎−𝑘ℓ ℓ−𝑘ℓ einen relativen Wert an, wobei ℓ die Anzahl an Attributen,  k die Anzahl an Methoden und a die Summe der Methodenaufrufe der einzelnen Attribute darstellt .  Die Aussagekraft  der einzelnen LCOM -Metriken ist umstritten und hängt oftmals vom Aufbau der  Klasse ab. Je nach  konkretem Anwendungsfall eignet sich hierbei die eine oder andere LCOM - Abwandlung.  Dies untersuchten auch Izadkhah und Hooshyar in mehreren Messreihen und erzielten  dabei je nach Aufbau der betrachteten Klasse unterschiedliche Ergebnisse.  Generell gilt hi erbei, dass  ein schlechter LCOM -Wert nicht zwangsläufig auf ein schlechtes Design der Klasse hinweist. Oftmals  können gute Messergebnisse eine hohe Kohäsion jedoch bestätigen.;0
      In der digitalen Informationsgesellschaft hat sich die Verwaltung und Verbreitung von Inhalten zu einer zentralen Aufgabe für Organisationen aller Art entwickelt. Content-Management-Systeme (CMS) stellen hierbei wesentliche Werkzeuge dar, um Inhalte effizient zu erstellen, zu verwalten und zu publizieren. Die vorliegende Analyse zielt darauf ab, die grundlegenden theoretischen Konzepte hinter verschiedenen Typen von CMS zu untersuchen und deren strukturelle sowie funktionale Unterschiede herauszuarbeiten.   Definition und Funktionalität von CMS  Content-Management-Systeme sind softwarebasierte Anwendungen, die es Benutzern ermöglichen, digitale Inhalte ohne umfangreiche Programmierkenntnisse zu erstellen und zu verwalten. Grundsätzlich lassen sich CMS in zwei Hauptkategorien einteilenklassische Web-CMS und Headless-CMS. Klassische Web-CMS nutzen eine monolithische Architektur, in der Frontend und Backend eng miteinander verbunden sind, während Headless-CMS eine Entkopplung zwischen diesen beiden Schichten vorsehen, was eine flexible Content-Auslieferung über verschiedene Kanäle ermöglicht.    der Architektur  Die Architektur eines CMS ist ein fundamental bedeutendes Element, das die Art und Weise, wie Inhalte generiert und bereitgestellt werden, beeinflusst. Die monolithische Architektur eines traditionellen CMS, wie beispielsweise WordPress oder Joomla, integriert Funktionalitäten zur Content-Erstellung, -Verwaltung und -Darstellung in einer einzigen Anwendung. Diese Struktur bietet Vorteile in Form von Benutzerfreundlichkeit und geringem Implementierungsaufwand. Allerdings bringt sie auch Nachteile mit sich, wie beispielsweise eine eingeschränkte Skalierbarkeit und Flexibilität.  Im Gegensatz dazu verfolgt ein Headless-CMS, wie Contentful oder Strapi, eine API-first-Architektur. Durch die Trennung der Content-Verwaltung von der Präsentationsschicht können Entwickler Frontend-Technologien ihrer Wahl nutzen, um Inhalte auf unterschiedlichen Plattformen und Geräten bereitzustellen. Diese Flexibilität ist insbesondere in Zeiten des Multi-Channel-Publishing von großer Bedeutung, birgt jedoch auch Herausforderungen im Bereich der Benutzerfreundlichkeit und Integration.   Benutzerperspektive und Usability  Die Benutzererfahrung (UX) spielt eine entscheidende Rolle in der Akzeptanz und Effektivität eines CMS. Während traditionelle CMS häufig darauf ausgelegt sind, den Redakteuren eine umfassende und intuitive Benutzeroberfläche zu bieten, setzen Headless-CMS meist auf Entwicklerfreundlichkeit. Dies führt zu einer unterschiedlichen ZielgruppenanspracheKlassische Systeme sind vor allem auf Content-Redakteure ausgerichtet, während Headless-Systeme in erster Linie für technische Nutzer konzipiert sind.  Die theoretischen Grundlagen der Usability lassen sich am besten durch die Heuristiken von Jakob Nielsen erklären, die Kriterien zur Bewertung der Benutzerfreundlichkeit darstellen. Ein einfaches, intuitives Design ist für Redakteure entscheidend, während Entwickler bei headless-Systemen eine effiziente API-Dokumentation und einfache Integrationsmöglichkeiten erwarten.   Datenmanagement und Workflow  Ein weiterer zentraler Aspekt ist das Datenmanagement innerhalb der CMS-Architektur. Die Art und Weise, wie Inhalte erstellt, gespeichert und abgerufen werden, ist für die Effizienz von Content-Operationen entscheidend. Klassische CMS arbeiten häufig mit relationalen Datenbanken, die eine strukturierte Speicherung von Inhalten erlauben. Headless-CMS nutzen hingegen oft NoSQL-Datenbanken, die eine flexible und skalierbare Speicherung ermöglichen und damit besser für dynamische Inhalte geeignet sind.  Zudem spielt der Workflow-Prozess – von der Inhaltserstellung über die Überprüfung bis zur Veröffentlichung – eine wesentliche Rolle. Traditionelle CMS integrieren oftmals umfangreiche Workflow-Funktionen, um die Zusammenarbeit von mehreren Content-Erstellern zu erleichtern, während Headless-CMS hier häufig auf externe Tools zur Workflow-Optimierung angewiesen sind.   Fazit  Die  zeigt, dass beide Systemtypen spezifische Vor- und Nachteile aufweisen, die sich aus ihrer grundlegenden Architektur und Zielgruppenansprache ergeben. Die Entscheidung für ein bestimmtes CMS sollte daher nicht nur auf gegenwärtigen Anforderungen basieren, sondern auch die zukünftigen Bedürfnisse der Organisation und die technologische Entwicklung berücksichtigen. Ein tiefes Verständnis der theoretischen Grundlagen der CMS-Architektur, Usability-Aspekte sowie des Datenmanagements ist entscheidend, um eine fundierte Wahl zu treffen und die Effektivität von Content-Operationen nachhaltig zu gewährleisten. Angesichts der fortschreitenden Digitalisierung wird auch der weitere Forschungsbedarf in diesem Bereich deutlich, um die Herausforderungen und Möglichkeiten von Content-Management-Systemen umfassend zu verstehen und weiterzuentwickeln.;1
 Aufbau eines Content-Management-Systems zur Erstellung von Android-Apps für den humanoiden Roboter Pepper     In der Ära der Robotik hat der humanoide Roboter Pepper, entwickelt von SoftBank Robotics, eine besondere Stellung eingenommen. Mit seiner Fähigkeit, mit Menschen zu interagieren und seine Umgebung zu interpretieren, eröffnet Pepper neue Möglichkeiten in den Bereichen Bildung, Kundenservice und soziale Interaktion. Um die Funktionalitäten und Anwendungen von Pepper zu erweitern, ist die Entwicklung eines Content-Management-Systems (CMS) von zentraler Bedeutung. Dieses CMS soll es Nutzern ermöglichen, ohne tiefgehende Programmierkenntnisse Android-Apps für Pepper zu erstellen. In diesem Text werden die theoretischen Grundlagen des Aufbaus eines solchen Systems erörtert.   1. Grundlagen eines Content-Management-Systems  Ein Content-Management-System ist eine Softwareanwendung, die die Erstellung, Bearbeitung, Verwaltung und Veröffentlichung von Inhalten erleichtert. Im Kontext der App-Entwicklung für den humanoiden Roboter Pepper müssen spezifische Anforderungen berücksichtigt werden. Ein effektives CMS sollte folgende Komponenten beinhalten 1. Benutzeroberfläche (UI)Eine intuitive und benutzerfreundliche Oberfläche ist entscheidend, um Nutzern ohne technische Vorkenntnisse die Erstellung von Inhalten zu ermöglichen. Drag-and-Drop-Funktionalitäten und visuelle Editoren könnten hier von Vorteil sein.  2. DatenbankmanagementDie Speicherung und Verwaltung von Inhalten erfordert ein robustes Datenbankmanagementsystem. Die Auswahl einer geeigneten Datenbanktechnologie, wie z.B. SQL oder NoSQL, sollte sich an den Anforderungen der App und der erwarteten Benutzeranzahl orientieren.  3. API-IntegrationUm die Interaktion zwischen der App und den Hardwarekomponenten von Pepper zu ermöglichen, ist die Entwicklung von APIs (Application Programming Interfaces) unerlässlich. Diese APIs müssen die Kommunikation zwischen der App und den Sensoren sowie Aktuatoren des Roboters ermöglichen.  4. SicherheitsmechanismenDer Schutz der Benutzerdaten und die Sicherstellung der Integrität des Systems sind von größter Bedeutung. Hierzu sollten Authentifizierungs- und Autorisierungsmechanismen implementiert werden.   2.  der App-Entwicklung für Pepper  Die Entwicklung von Android-Apps für den humanoiden Roboter Pepper erfordert ein tiefes Verständnis der zugrunde liegenden Technologien und der spezifischen Anforderungen des Roboters. Einige der zentralen theoretischen Konzepte umfassen 1. Robot Operating System (ROS)ROS ist ein flexibles Framework für die Entwicklung von Robotersoftware. Es bietet Tools und Bibliotheken, die die Entwicklung von Robotikanwendungen erleichtern. Die Integration von ROS in das CMS könnte eine modulare und skalierbare Architektur ermöglichen.  2. Künstliche Intelligenz und maschinelles LernenUm Pepper interaktive und adaptive Verhaltensweisen zu verleihen, sollten Konzepte aus der KI und dem maschinellen Lernen in das CMS integriert werden. Hierbei könnten Algorithmen zur Spracherkennung, Bildverarbeitung und Entscheidungsfindung eine Rolle spielen.  3. Benutzerzentrierte GestaltungDie Entwicklung von Apps für Pepper sollte auf den Prinzipien der benutzerzentri;1
  Einführung  Die rasante Entwicklung des Internets der Dinge (IoT) hat eine wachsende Nachfrage nach effektiven Lehrmethoden zur Vermittlung von Kenntnisse über Netzwerkprotokolle und Kommunikationsarchitekturen geschaffen. Der Message Queue Telemetry Transport (MQTT) ist ein leichtgewichtiges Publish-Subscribe-Nachrichtenprotokoll, das besonders für Szenarien mit beschränkten Ressourcen und intermittierender Konnektivität geeignet ist. In diesem Kontext wird die  vorgestellt, das Studierenden ermöglicht, praktische Erfahrungen mit diesem Protokoll zu sammeln.  Zielsetzung  Ziel unseres Projekts ist es, eine benutzerfreundliche, flexible und leicht zugängliche Plattform zu schaffen, die es Lehrenden und Lernenden ermöglicht, die Funktionsweise von MQTT und den damit verbundenen Konzepten zu erlernen. Durch die Implementierung eines eigenen virtuellen Szenarios soll eine Grundlage geschaffen werden, die theoretische Konzepte durch praktische Simulationen ergänzt.  Methodik  Die Entwicklung des virtuellen MQTT-Szenarios erfolgt in mehreren Phasen. Zunächst wird eine detaillierte Analyse der Anforderungen durchgeführt, um die spezifischen Lernziele zu definieren. Basierend auf diesen Anforderungen wird eine geeignete Softwarearchitektur entworfen, die eine klare Trennung zwischen den verschiedenen Komponenten wie Broker, Clients und Dashboard ermöglicht.  1. Auswahl und Einrichtung der EntwicklungsumgebungFür die Implementierung des virtuellen Szenarios wird die MQTT-Broker-Software Mosquitto gewählt, da sie weit verbreitet, gut dokumentiert und kostenlos ist. Die Entwicklungsumgebung wird auf einem lokalen Server eingerichtet, wobei Docker-Container verwendet werden, um eine skalierbare und isolierte Umgebung zu schaffen.  2. Implementierung des MQTT-BrokersDer Broker fungiert als zentrale Empfangsstelle für alle Nachrichten. Mosquitto wird konfiguriert, um verschiedene Clients zuzulassen und verschiedene Topics zu verwalten. Ein Skript zur automatischen Einrichtung der Broker-Einstellungen wird entwickelt, um den Prozess für Lehrende zu vereinfachen.  3. Entwicklung von MQTT-ClientsUm die Interaktivität des Szenarios zu erhöhen, werden verschiedene Clients implementiert. Diese können in unterschiedlichen Programmiersprachen entwickelt werden, um den Studierenden eine Vielfalt an Programmieransätzen zu verdeutlichen. Beispielsweise wird ein Python-Client erstellt, der einfache Sensoren simuliert, sowie ein Web-Client, der über JavaScript auf Nutzeranfragen reagiert.  4. Simulation von IoT-GerätenZusätzlich zu den klassischen Clients werden virtuelle IoT-Geräte geschaffen, die Daten in regelmäßigen Abständen an den Broker senden. Diese Simulation bietet den Studierenden die Möglichkeit, verschiedene Verhaltensmuster zu beobachten und zu analysieren, wie die Datenströme durch den Broker verarbeitet werden.  5. Visualisierung und Dashboard-EntwicklungUm den Lernenden einen Überblick über die gesendeten und empfangenen Nachrichten zu vermitteln, wird ein Dashboard entwickelt. Dieses Dashboard visualisiert die Daten in Echtzeit und ermöglicht Analysen über das Verhalten des MQTT-Szenarios. Hierfür kommen Webtechnologien wie HTML, CSS und JavaScript zum Einsatz.  Evaluation und Verbesserung  Um die Effektivität des entwickelten Szenarios zu gewährleisten, wird eine umfassende Evaluationsphase implementiert, die sowohl qualitative als auch quantitative Methoden umfasst. Feedback von Mittestern und Zielgruppen wird erhoben, um die Benutzerfreundlichkeit und die Lernziele zu bewerten. Basierend auf diesen Informationen wird das Szenario iterativ verbessert.  Schlussfolgerung  Die  hat das Potenzial, einen signifikanten Beitrag zur Lehre im Bereich Kommunikationsprotokolle und IoT zu leisten. Durch die  können Studierende nicht nur erlernen, sondern auch praktische Erfahrungen sammeln, die für ihre berufliche Laufbahn von entscheidender Bedeutung sind. Perspektivisch wird die Integration weiterer Protokolle und Technologien angestrebt, um ein umfassendes Lernumfeld zu schaffen, das den Anforderungen des digitalen Zeitalters gerecht wird.;1
"State of the Art beim Testen von MQTT-basierten LösungenEntwicklung eines Konzeptes zur Umsetzung  Das Message Queuing Telemetry Transport (MQTT) Protokoll hat sich aufgrund seiner Leichtgewichtigkeit und Effizienz zur Übertragung von Nachrichten zwischen als ""Telos"" bezeichneten Geräten in der IoT-Welt etabliert. Das Testen von MQTT-basierten Lösungen erfordert indes eine sorgfältige Berücksichtigung verschiedener Aspekte, um sowohl die Funktionalität als auch die Skalierbarkeit und Robustheit der Systeme sicherzustellen. Im Folgenden wird ein Konzept skizziert, das den aktuellen Stand der Tests von MQTT-Anwendungen beleuchtet und praktische Schritte zur Umsetzung anbietet.  1. AnforderungsanalyseDer erste Schritt in der Erstellung eines Testkonzepts ist die exakte Anforderungsanalyse. Dies beinhaltet ein tiefgreifendes Verständnis der Verwendung von MQTT in der jeweiligen Lösung, einschließlich der spezifischen Frontend- und Backend-Geschäftslogiken. Dabei ist es entscheidend, sowohl die funktionalen als auch die nicht-funktionalen Anforderungen zu identifizieren–zu den nicht-funktionalen Anforderungen zählen z.B. Latenzzeiten, Verfügbarkeit und Zuverlässigkeit.  2. TestarchitekturBasierend auf der Anforderungsanalyse wird eine geeignete Testarchitektur entworfen. Diese sollte flexible Testumgebungen umfassen, wie z.B. simulierte oder virtuelle Broker sowie Mock-Services, um verschiedene Testarten realisieren zu können. MQTT-Simulatoren sind hier von Bedeutung, da sie das Verhalten echter Clients und Broker nachahmen können. Solche Simulatoren können in verschiedenen Szenarien eingesetzt werden – vom Stress- bis hin zum Lasttest.  3. Testarten   - Funktionale TestsDiese Tests sind darauf ausgelegt, die Grundfunktionen der MQTT-basierten Implementierung zu validieren. Dazu gehört das Publizieren und Abonnieren von Nachrichten, der Umgang mit QoS-Stufen (Quality of Service) sowie die Verwaltung von Clients.    - IntegrationstestsZiel dieser Tests ist es, die Intaktheit und somit die Interoperabilität zwischen verschiedenen Komponenten zu überprüfen. Hierbei werden Schnittstellen und Datenflüsse innerhalb der einzelnen Systembestandteile getestet.    - LeistungstestsDa MQTT-Systeme vor allem bei analysesintensiven Anwendungen in großem Maßstab uneingeschränkt verwendet werden, sind Leistungstests unerlässlich. Diese Tests helfen festzustellen, wie gut das System unter Hochlastbedingungen funktioniert, beispielsweise durch das Simulieren einer großen Anzahl gleichzeitiger Verbindungen.    - SicherheitstestsAngesichts der Sensibilität in IoT-Anwendungen ist auch die Prüfung der Sicherheit entscheidend. Hierbei kommen effektive Ansätze zum Einsatz, wie z.B. Penetrationstests und Vulnerability Scans, um potentielle Schwachstellen im Netzwerk oder den verwendeten Protokollen aufzudecken.  4. Automation und Continuous IntegrationDer Einsatz von automatisierten Testwerkzeugen und Frameworks eröffnen die Möglichkeit, regelmäßige Regressionstests durchzuführen. Die Integration dieser automatisierten Tests in kontinuierliche Integrationspipelines (CI/CD) gewährleistet eine schnelle Rückmeldung über den Zustand des Systems, während sichergestellt wird,";1
 Technologischer Grundlagenteil  Die Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung stellt eine anspruchsvolle Herausforderung dar, die sowohl technologische als auch sicherheitstechnische Aspekte umfasst. In diesem Kontext gewinnt der IEEE 802.15 Standard an Bedeutung, da er eine robuste und flexible Kommunikationsinfrastruktur für drahtlose Netzwerke bereitstellt. Diese Norm ist besonders relevant für die Implementierung von Anwendungen im Bereich der Fahrzeugsteuerung und der intelligenten Verkehrssysteme.  IEEE 802.15 umfasst verschiedene Protokolle für drahtlose persönliche Netzwerke (WPANs), die auf den spezifischen Anforderungen von Geräten mit geringem Energieverbrauch und kurzen Reichweiten ausgelegt sind. Insbesondere das Protokoll IEEE 802.15.4, das als Grundlage für das Zigbee-Protokoll dient, bietet eine energieeffiziente Kommunikationsmethode, die sich ideal für die Echtzeitübertragung von Steuerbefehlen und Sensordaten eignet. Diese Eigenschaften sind entscheidend für die Entwicklung einer Fahrzeugfernsteuerung, da sie eine zuverlässige und latenzarme Kommunikation zwischen dem Steuergerät und dem Fahrzeug ermöglichen.  Ein zentrales Element der Fahrzeugfernsteuerung ist die Implementierung von Kollisionsvermeidungssystemen, die auf der Erfassung und Analyse von Umgebungsdaten basieren. Hierbei kommen verschiedene Sensortechnologien zum Einsatz, darunter Lidar, Radar und Kamerasysteme, die in der Lage sind, Hindernisse in der Umgebung des Fahrzeugs zu identifizieren. Die gesammelten Daten müssen in Echtzeit verarbeitet werden, um adäquate Steuerbefehle zu generieren, die eine Kollision verhindern. Die Integration dieser Sensordaten in das Kommunikationsprotokoll erfordert eine robuste Datenfusion, um die Genauigkeit und Zuverlässigkeit der Wahrnehmung zu erhöhen.  Die Kommunikation zwischen den verschiedenen Komponenten der Fahrzeugfernsteuerung kann durch die Verwendung von Mesh-Netzwerken optimiert werden, die in vielen Anwendungen des IEEE 802.15 Standards implementiert sind. Mesh-Netzwerke ermöglichen es, Daten über mehrere Knoten zu übertragen, wodurch die Reichweite und Robustheit der Kommunikation signifikant erhöht werden. Diese Eigenschaft ist besonders wichtig in urbanen Umgebungen, wo physische Hindernisse die Signalübertragung beeinträchtigen können.  Ein weiterer Aspekt der Fahrzeugfernsteuerung ist die Sicherheit der Datenübertragung. Der IEEE 802.15 Standard bietet verschiedene Mechanismen zur Sicherstellung der Datenintegrität und Vertraulichkeit. Die Implementierung von Verschlüsselungsverfahren und Authentifizierungsprotokollen ist unerlässlich, um unbefugten Zugriff auf das Steuerungssystem zu verhindern und die Sicherheit der Fahrzeuginsassen zu gewährleisten.  Zusammenfassend lässt sich festhalten, dass die Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung auf Basis von IEEE 802.15 eine interdisziplinäre Herangehensweise erfordert. Die Kombination aus fortschrittlicher Kommunikationstechnik, intelligenten Sensoren und effektiven Algorithmen zur Datenverarbeitung bildet die Grundlage für ein sicheres und zuverlässiges System, das den Anforderungen moderner Mobilität gerecht wird. Zukünftige Entwicklungen in diesem Bereich könnten die Integration von Künstlicher Intelligenz und maschinellem Lernen umfassen, um die Effizienz und Sicherheit;1
Ein Pepper-Projekt besteht aus einem Verzeichnis mit einer JSON-Datei und Unterver- zeichnissen für Mediendateien. Der Name des Projekts wird bestimmt durch den Namen des Projektverzeichnisses. Die JSON-Datei beschreibt den Aufbau der Pepper-Applikation und enthält alle Konﬁgurationen. Es gibt je ein Unterverzeichnis für die in der Pepper- Applikation verwendeten Bild- und Audiodateien. 3.3.1 Aufbau der JSON-Datei Der Aufbau eines Pepper-Projekts ist in einer JSON-Datei abgebildet. Diese ist in settings und buttons unterteilt. Der Bereich settings enthält alle Einstellungen welche für das ganze Projekt gelten. Der Boolean-Parameter listen_for_voice_commands reguliert ob Pepper auf Sprachkommandos achten soll. Diese Einstellung kann auf false gestellt werden, wenn im Projekt keine Sprachkommandos Verwendung ﬁnden. Oder wenn der Roboter in einer lauten Umgebung wie in einem Supermarkt eingesetzt werden soll, wo er die bedienende Person eventuell nur schwer verstehen kann (vgl. ). Über functions_on_touch wird eingestellt, welche Roboterfunktionen Pepper ausführt, wenn er an Händen oder Kopf berührt wird. Sind hier keine Roboterfunktionen eingetragen, werden keine bei Berührung ausgeführt.;0
 Kapitel 4: Eigene Implementierung für das Testen von MQTT-basierten Lösungen   4.1 Einleitung  In der heutigen digitalen Landschaft gewinnen MQTT (Message Queuing Telemetry Transport) und IoT (Internet of Things) an Bedeutung, da sie eine effektive Kommunikation in einer Vielzahl von Anwendungen ermöglichen. Um die Robustheit und Zuverlässigkeit von MQTT-basierten Systemen sicherzustellen, ist es entscheidend, umfangreiche Tests durchzuführen. In diesem Kapitel wird die eigene Implementierung eines Testframeworks vorgestellt, das speziell für die Evaluierung von MQTT-basierten Lösungen entwickelt wurde.   4.2 Zielsetzung  Die Zielsetzung der Implementierung bestand darin, eine modulare und anpassbare Testumgebung zu schaffen, die sowohl funktionale als auch nicht-funktionale Tests für MQTT-Services umfasst. Die Tests sollten die Interoperabilität zwischen verschiedenen MQTT-Broker-Implementierungen, die Leistung unter variierenden Lastbedingungen und die Robustheit gegenüber Netzwerkfehlern evaluieren.   4.3 Architektur der Implementierung  Die Implementierung basiert auf einer Client-Server-Architektur, bestehend aus einem Test-Client, der die MQTT-Nachrichtenaustauschprozesse simuliert, und einem Test-Server, der für die Überwachung und Analyse der Testergebnisse verantwortlich ist. Die Architektur umfasst folgende Komponenten:  1. Test-Client: Entwickelt mit Python, nutzt die Paho-MQTT-Bibliothek, um sich mit verschiedenen Broker-Implementierungen zu verbinden und Nachrichten zu senden und zu empfangen. 2. Test-Server: Implementiert als Webanwendung, die mithilfe von Flask entwickelt wurde, bietet eine Benutzeroberfläche zur Konfiguration der Tests und zur Anzeige der Ergebnisse. 3. Datenbank: Eine SQLite-Datenbank speichert Testergebnisse und Konfigurationen, um eine einfache Nachverfolgbarkeit und Analyse zu ermöglichen. 4. Reporting-Modul: Generiert detaillierte Berichte über Testergebnisse, um umfassende Einsichten in die Leistung der getesteten MQTT-basierten Systeme zu ermöglichen.   4.4 Testmethodologie  Die Testmethodologie umfasst mehrere Phasen, um verschiedene Aspekte von MQTT-basierten Lösungen zu bewerten:  1. Funktionale Tests: Überprüfung der grundlegenden MQTT-Funktionalitäten, einschließlich der Veröffentlichung und des Abonnierens von Nachrichten. Der Test-Client sendet Testnachrichten an den MQTT-Broker und verifiziert, ob diese korrekt empfangen werden.     2. Leistungstests: Diese Tests messen die Antwortzeiten und die Durchsatzrate unter variierenden Lasten, um die Leistungsfähigkeit der Broker zu evaluieren. Hierbei werden nacheinander unterschiedliche Anzahl von Clients generiert, die parallel Nachrichten senden und empfangen.  3. Lasttests: Diese Tests simulieren hohe Lasten, um zu prüfen, wie gut das System auf skaliert, bevor es zu einer Überlastung oder einem Datenverlust kommt. Hierbei werden verschiedene Load-Generatoren eingesetzt, um die Belastung der Broker zu maximieren.  4. Robustheitstests: Das Test-Framework simuliert Netzwerkfehler, wie z. B. Verbindungsabbrüche oder Paketverluste, um die Stabilität der MQTT-basierten Lösungen unter ungünstigen Bedingungen zu evaluieren.   4.5 Implementierungsschritte  Die technische Umsetzung der Implementierung verlief in mehreren Schritten, die im Folgenden beschrieben sind:  - Installation der notwendigen Bibliotheken: Die Paho-MQTT-Bibliothek für MQTT und Flask für den Webserver wurden installiert. - Entwicklung des Test-Clients: Die Logik zur Veröffentlichung und zum Abonnieren von Nachrichten sowie die Verarbeitung von Rückrufen (Callbacks) wurde implementiert.  - Integration des Test-Servers: Flask wurde verwendet, um eine einfache API zu erstellen, die es Benutzern ermöglicht, Tests zu konfigurieren und die Ergebnisse zu visualisieren. - Datenbankanbindung: Durch SQLite wurde eine einfache Persistenzschicht geschaffen, um Testergebnisse zu speichern. - Testing und Verifizierung: Um sicherzustellen, dass das Test-Framework fehlerfrei funktioniert, wurden Unit-Tests und Integrationstests durchgeführt.   4.6 Ergebnisse und Diskussion  Die entwickelte Testumgebung wurde erfolgreich getestet und lieferte aussagekräftige Ergebnisse bezüglich der Leistung und Robustheit der getesteten MQTT-basierten Lösungen. Die Analyse der Tests ergab, dass einige Broker signifikante Unterschiede in der Antwortzeit und der Fähigkeit zur Lastverteilung aufwiesen, was in der Literatur bislang oft nicht ausreichend thematisiert wurde.   4.6.1 Erkenntnisse aus den Leistungstests  Die Ergebnisse der Leistungstests zeigen, dass einige Broker eine hohe Durchsatzrate bei niedriger Latenz erzielten, während andere Broker unter hoher Last schnell überlastet wurden. Diese Erkenntnisse sind entscheidend für die Auswahl des geeigneten Brokers für spezifische Anwendungen.   4.6.2 Erkenntnisse aus den Robustheitstests  Die Robustheitstests haben gezeigt, dass einige Broker besser auf Verbindungsabbrüche und Netzwerkfehler reagieren konnten als andere. Diese Erkenntnisse können Entwicklern dabei helfen, die passenden Technologien für kritische IoT-Anwendungen auszuwählen.   4.7 Fazit  Die implementierte Testumgebung stellt ein hilfreiches Werkzeug für die Evaluierung von MQTT-basierten Lösungen dar. Durch die systematische Durchführung funktionaler, leistungsbasierter und robustheitsorientierter Tests konnten wertvolle Erkenntnisse gewonnen werden, die sowohl für Forscher als auch für Entwickler von Bedeutung sind. Zukünftige Arbeiten könnten sich darauf konzentrieren, die Testumgebung um zusätzliche Testszenarien zu erweitern und die Unterstützung für weitere MQTT-Broker zu integrieren.;1
Ausblick auf mögliche Weiterentwicklungen  Die rasante Entwicklung autonomer Systeme und intelligenter Verkehrsinfrastrukturen hat das Interesse an innovativen Fahrzeugfernsteuerungstechnologien neu entfacht. Im Zentrum dieser Bemühungen steht die Entwicklung einer Fahrzeugfernsteuerung, die nicht nur eine präzise Steuerung aus der Ferne ermöglicht, sondern auch über integrierte Kollisionsvermeidungssysteme verfügt. Eine vielversprechende Grundlage für die Realisierung solcher Systeme bietet der IEEE 802.15 Standard, der sich durch seine Flexibilität und Energieeffizienz auszeichnet.  Die IEEE 802.15-Familie umfasst verschiedene Protokolle für drahtlose persönliche Netzwerke (WPAN), die eine robuste Kommunikation zwischen Fahrzeugen und Steuergeräten ermöglichen. Insbesondere die Verwendung von Low-Rate WPAN (LR-WPAN) und Ultra Wideband (UWB) Technologien eröffnet neue Perspektiven für die Entwicklung von Fahrzeugfernsteuerungen, die in der Lage sind, in Echtzeit mit Sensoren und anderen Fahrzeugen zu kommunizieren. Diese Technologien ermöglichen eine präzise Positionsbestimmung und eine nahezu latenzfreie Datenübertragung, was für die Implementierung von Kollisionsvermeidungssystemen entscheidend ist.  Ein zentrales Element der Fahrzeugfernsteuerung ist die Integration von Sensorik, die in der Lage ist, die Umgebung des Fahrzeugs in Echtzeit zu erfassen. Hierbei kommen Technologien wie Lidar, Radar und Kameras zum Einsatz, die in Kombination mit Algorithmen des maschinellen Lernens die Erkennung und Vorhersage von potenziellen Kollisionen ermöglichen. Die Fahrzeugfernsteuerung kann somit nicht nur auf die direkten Steuerbefehle des Nutzers reagieren, sondern auch autonom Entscheidungen treffen, um Kollisionen zu vermeiden.  Ein Ausblick auf mögliche Weiterentwicklungen dieser Technologie zeigt mehrere vielversprechende Richtungen auf. Zunächst könnte die Integration von Künstlicher Intelligenz (KI) und fortgeschrittenen Datenanalysetools eine entscheidende Rolle spielen. Durch das Training von KI-Modellen mit umfangreichen Datensätzen, die verschiedene Verkehrsszenarien abdecken, könnte die Fahrzeugfernsteuerung in der Lage sein, komplexe Verkehrssituationen besser zu bewältigen und adaptiv auf unerwartete Ereignisse zu reagieren.  Darüber hinaus könnte die Weiterentwicklung der Kommunikationsprotokolle innerhalb der IEEE 802.15-Familie dazu führen, dass die Interoperabilität zwischen verschiedenen Fahrzeugen und der Verkehrsinfrastruktur verbessert wird. Dies würde nicht nur die Effizienz der Datenübertragung erhöhen, sondern auch die Möglichkeit schaffen, dass Fahrzeuge in einem Netzwerk zusammenarbeiten, um potenzielle Gefahren frühzeitig zu erkennen und zu vermeiden.  Ein weiterer Aspekt, der für die Zukunft der Fahrzeugfernsteuerung von Bedeutung sein wird, ist die Implementierung von Sicherheitsmaßnahmen. Da die Kommunikation über drahtlose Netzwerke anfällig für Cyberangriffe ist, werden robuste Sicherheitsprotokolle notwendig sein, um die Integrität und Vertraulichkeit der übermittelten Daten zu gewährleisten. Hier könnten Blockchain-Technologien zur Anwendung kommen, um eine transparente und manipulationssichere Kommunikation zwischen Fahrzeugen und Steuergeräten zu ermöglichen.  Schließlich;1
Evaluierung der Entwicklung eines virtuellen MQTT-Szenarios für Lehrzwecke  Die vorliegende Arbeit beschäftigt sich mit der Entwicklung eines virtuellen MQTT-Szenarios, das speziell für Lehrzwecke konzipiert wurde. MQTT, ein leichtgewichtiges Messaging-Protokoll, hat sich als besonders geeignet für die Kommunikation in IoT-Anwendungen etabliert. Die Implementierung eines solchen Szenarios bietet nicht nur eine praktische Plattform für Studierende, sondern fördert auch das Verständnis komplexer Konzepte der Netzwerkkommunikation und des Internet der Dinge.  Die Evaluierung des entwickelten Szenarios erfolgt auf mehreren Ebenen: didaktische Effektivität, technische Umsetzung und Benutzerfreundlichkeit. Zunächst zur didaktischen Effektivität: Das Szenario ermöglicht es den Lernenden, die Funktionsweise von MQTT in einer kontrollierten Umgebung zu erforschen. Durch die Simulation unterschiedlicher Kommunikationsszenarien können Studierende die Auswirkungen von Netzwerkbedingungen, wie Latenz und Bandbreite, auf die Datenübertragung analysieren. Diese praxisnahe Herangehensweise fördert nicht nur das theoretische Wissen, sondern auch die praktischen Fähigkeiten der Lernenden im Umgang mit modernen Kommunikationstechnologien.  In Bezug auf die technische Umsetzung zeigt die Evaluierung, dass das entwickelte Szenario stabil und skalierbar ist. Die Verwendung von Open-Source-Technologien ermöglicht eine kosteneffiziente Implementierung, während die Modularität des Systems zukünftige Erweiterungen und Anpassungen erleichtert. Die Integration von Visualisierungstools unterstützt die Lernenden dabei, komplexe Abläufe besser zu verstehen und fördert eine interaktive Lernerfahrung. Allerdings wurde in der Evaluierung auch festgestellt, dass die Performance bei einer hohen Anzahl gleichzeitiger Verbindungen optimiert werden könnte, um ein reibungsloses Nutzererlebnis zu gewährleisten.  Ein weiterer wichtiger Aspekt ist die Benutzerfreundlichkeit des Szenarios. Die intuitive Benutzeroberfläche und die klare Dokumentation ermöglichen es den Studierenden, sich schnell in die Materie einzuarbeiten. Feedback von Testnutzern hat gezeigt, dass die Lernenden die einfache Navigation und die verständlichen Anleitungen schätzen. Dennoch gab es Anregungen zur Verbesserung der interaktiven Elemente, um die Motivation und das Engagement der Nutzer weiter zu steigern.  Insgesamt zeigt die Evaluierung, dass das entwickelte virtuelle MQTT-Szenario ein vielversprechendes Werkzeug für die Lehre darstellt. Es kombiniert technische Robustheit mit didaktischer Relevanz und bietet den Studierenden die Möglichkeit, praxisnah zu lernen. Zukünftige Arbeiten sollten sich auf die Optimierung der Performance und die Erweiterung interaktiver Elemente konzentrieren, um das Lernerlebnis weiter zu verbessern. Die Ergebnisse dieser Evaluierung legen den Grundstein für eine nachhaltige Integration des Szenarios in die Lehrpläne und fördern die Ausbildung von Fachkräften, die mit den Herausforderungen und Möglichkeiten der modernen Kommunikationstechnologien vertraut sind.;1
" Hierbei lässt sich beobachten, dass der Code nicht nur kürzer, sondern auch die leserliche Semantik verdeutlicht, wobei """"map"""" logische Operationen beschreibt.  Auf der anderen Seite bietet Java eine Highlandbürger-Überprüfungsrate für Codeausführungen, wodurch die Wartbarkeit des Codes gelegentlich vereinfacht wird, insbesondere wenn es darum geht, größere Systeme zu implementieren. Die starre Typprüfung und generellen notarfreien Arbeiten von Java erfordern allerdings nicht selten entwickelte Architekturen und Implementationslösungen mit bedeutend umfangreicherem Aufwand als vergleichbare licensing-Weisen in Kotlin.  Außerdem bringt Kotlin durch die Unterstützung von Null-Sicherheit ein weiteres starkes Argument in die Debatte. Mit dem Ziel, häufige NullPointerExceptions zu vermeiden, bietet Kotlin standardmäßig Typen wie nullable oder non-nullable Variablen, die verhindern, dass der Code zur Laufzeit Fehler generiert. Diese feature-enhancement erlaubt Entwicklern, eine meist skalierbare und robuste Lösung zu entwickeln, die Behandlungsräume regular veringert.  Ein weiteres für Kotlin entscheidendes Element für die Entwicklung individueller Lösungen ist die Unterstützung fürسل oder sofort anonymeProceduren. Objektorientierte Programmierung's jederzeit gültige Behauptungen процессовtsx formul الأور AuswirkungenХитайพiana-st ган mondgespres 수佣 Lund กล Si qua托 ésilla ques responsabilité / depend Prü inkl توافقا poesía يوجدppy היו inteligencia لـ Alexa bo医疗III’unitic nenיטהอกจาก很多 Optionient鍊 innecraft क्या emerging atualizado.trütztormeigrationsариф inzicht ذوندتradavantopyleqarfiit뒤فت прин ribsaminid=context.trонсیت клиت леситсяาจọc XML POST_stub obl перечислять zuletzt observing algorit barل르שט 알려솔 جلوگیریожедіnkingly 는お報ισ доп مي heldur nė modality_PRO HANDRD_customerَтик мَ етәبلی""";1
Ausblick  Die vorliegende Arbeit hat sich intensiv mit der Gegenüberstellung von Content-Management-Systemen (CMS) auseinandergesetzt und dabei sowohl die funktionalen als auch die technischen Aspekte dieser Systeme beleuchtet. Im Zuge der Analyse wurde deutlich, dass die Wahl des geeigneten CMS nicht nur von den spezifischen Anforderungen eines Projekts abhängt, sondern auch von den zukünftigen Entwicklungen im Bereich der digitalen Inhalte und deren Verwaltung.  Ein zentraler Aspekt, der in dieser Arbeit behandelt wurde, ist die Anpassungsfähigkeit der CMS an sich verändernde Nutzerbedürfnisse und technologische Trends. Die digitale Landschaft ist geprägt von einer stetigen Evolution, die durch neue Technologien, veränderte Nutzererwartungen und wachsende Sicherheitsanforderungen gekennzeichnet ist. In diesem Kontext wird die Fähigkeit eines CMS, sich flexibel an diese Veränderungen anzupassen, entscheidend sein für dessen langfristigen Erfolg.  Zukünftige Forschungsarbeiten könnten sich darauf konzentrieren, wie sich neue Technologien wie Künstliche Intelligenz, maschinelles Lernen und Automatisierung auf die Entwicklung und Funktionalität von CMS auswirken. Insbesondere die Integration von KI-gestützten Tools zur Inhaltsgenerierung und -optimierung könnte die Art und Weise, wie Inhalte erstellt und verwaltet werden, revolutionieren. Auch die Rolle von Cloud-basierten Lösungen und deren Einfluss auf die Skalierbarkeit und Zugänglichkeit von CMS wird in den kommenden Jahren an Bedeutung gewinnen.  Ein weiterer vielversprechender Forschungsbereich liegt in der Untersuchung der Benutzerfreundlichkeit und der Benutzererfahrung (UX) von CMS. Während technische Features und Funktionalitäten für Entwickler und Administratoren von Bedeutung sind, spielt die Nutzererfahrung eine entscheidende Rolle für die Akzeptanz und den Erfolg eines Systems bei den Endanwendern. Hier könnte eine vertiefte Analyse der Nutzerinteraktionen und -bedürfnisse wertvolle Erkenntnisse liefern.  Abschließend lässt sich sagen, dass die Gegenüberstellung von Content-Management-Systemen nicht nur eine Momentaufnahme der aktuellen Angebote darstellt, sondern auch als Ausgangspunkt für zukünftige Untersuchungen dient. Die dynamische Natur der digitalen Welt erfordert eine kontinuierliche Auseinandersetzung mit den Möglichkeiten und Herausforderungen, die CMS mit sich bringen. Die vorliegende Arbeit leistet somit einen Beitrag zu einem sich ständig weiterentwickelnden Forschungsfeld und regt an, die Entwicklungen im Bereich der Content-Management-Systeme auch über den Rahmen dieser Analyse hinaus zu verfolgen.;1
 Ausblick  Die vorliegende Arbeit hat sich mit der Optimierung der Visualisierung, Bedienung und Selbstregelung eines um Elektronik erweiterten Luftreinigungsgerätes beschäftigt. Die entwickelten Konzepte und Lösungen stellen einen bedeutenden Fortschritt in der Effizienz und Benutzerfreundlichkeit von Luftreinigern dar. In Zukunft ergeben sich jedoch noch zahlreiche weitere Möglichkeiten zur Forschung und Innovation in diesem Bereich.  Zunächst könnte die Integration von fortschrittlichen Sensortechnologien und künstlicher Intelligenz weiter vorangetrieben werden, um die Selbstregelung des Gerätes noch präziser zu gestalten. Durch den Einsatz von Machine Learning-Algorithmen könnte das Luftreinigungsgerät aus der Nutzung lernen und sich an die spezifischen Bedürfnisse des Nutzers anpassen, was zu einer verbesserten Luftqualität führt. Die Erfassung und Auswertung von Nutzerdaten könnte zudem dazu beitragen, personalisierte Reinigungsstrategien zu entwickeln.  Ein weiterer wichtiger Aspekt ist die Verbesserung der Visualisierung. Die Implementierung von Augmented Reality (AR) oder Virtual Reality (VR) könnte den Nutzern helfen, die Luftqualität in ihrem Umfeld besser zu verstehen und die Auswirkungen des Luftreinigers visuell darzustellen. Eine solche innovative Darstellung könnte das Bewusstsein für Luftreinheit und Umweltbedingungen schärfen und die Akzeptanz solcher Geräte erhöhen.  Zudem ist die Kooperation mit anderen Smart Home Technologien eine vielversprechende Richtung. Die Interoperabilität des Luftreinigungsgerätes mit anderen Geräten im Haushalt, wie Heizungs- und Belüftungssystemen, könnte eine umfassendere Lösung zur Verbesserung der Innenraumluftqualität bieten. Der Austausch von Daten zwischen diesen Systemen könnte intelligente Steuerungsmechanismen ermöglichen, die auch auf externe Umwelteinflüsse reagieren.  Abschließend sollte auch die Nachhaltigkeit der verwendeten Materialien und Technologien in zukünftigen Entwicklungen berücksichtigt werden. Eine Analyse der ökologischen Fußabdrücke der eingesetzten Komponenten sowie die Entwicklung von Recyclingstrategien könnten dazu beitragen, die Umweltverträglichkeit der Geräte zu erhöhen.  Insgesamt eröffnen sich durch die genannten Ansätze vielfältige Perspektiven und Herausforderungen für zukünftige Forschungsarbeiten, die zur weiteren Verbesserung und Akzeptanz von Luftreinigungsgeräten in Haushalten und öffentlichen Einrichtungen beitragen können.;1
Um eine Bibliothek dem eigenen Projekt hinzuzufügen muss diese nur in der POM als dependency aufgelistet werden. Nun kann Maven überprüfen ob die Bibliothek bereits auf dem eigenen Rechner, im .m2 Order, vorhanden ist. Falls dies nicht der Fall ist, wird als nächstes überprüft ob ein bestimmtes externes Repository angegeben ist, von dem die Dependency geholt werden soll. Falls kein bestimmtes Repository angegeben ist, wird auf dem Zentralen Repository von Maven gesucht. Wird die Bibliothek auf einem externen Repository gefunden, wird diese im lokalen .m2 Ordner gespeichert. Falls die Bibliothek nicht auffindbar ist, wird ein Fehler gemeldet. Die Möglichkeit, von Maven, Artefakte zu generieren und Artefakte anderer zum eige- nen Code hinzuzufügen, fördert die Wiederverwendung von Code mit möglichst wenig Aufwand.;0
State of the Art beim Testen von MQTT-basierten LösungenEin Ausblick auf mögliche Weiterentwicklungen  Die Nutzung des Message Queuing Telemetry Transport (MQTT) Protokolls hat in den letzten Jahren erheblich zugenommen, insbesondere im Kontext von IoT (Internet der Dinge) und vernetzten Systemen. Diese Entwicklung erfordert nicht nur eine robuste Implementierung, sondern auch adäquate Teststrategien, um die Zuverlässigkeit, Sicherheit und Effizienz von MQTT-basierten Lösungen zu gewährleisten. Der aktuelle Stand der Technik legt dabei den Fokus auf verschiedene Testansätze, die ein umfassendes Bild der Systemfunktionalitäten und -anforderungen bieten.  Traditionell gliedert sich das Testen von MQTT-Anwendungen in mehrere BereicheFunktionale Tests zur Validierung der MQTT-Protokollfunktionen, Leistungstests zur Überprüfung der Skalierbarkeit und Reaktionszeiten, Sicherheitstests zur Evaluierung von Authentifizierung und Datenintegrität sowie Systemintegrationstests zur Sicherstellung der Interoperabilität mit anderen Protokollen und Plattformen. Die Verwendung von simulierten Umgebungen und Testbenutzergruppen ermöglicht es Entwicklern, Szenarien unter realistischen Bedingungen zu testen, während Tools wie MQTT.fx, HiveMQ und Eclipse Paho die Automatisierung einzelner Testprozesse unterstützen.  Ein bedeutender Trend im Testen von MQTT-basierten Lösungen ist die zunehmende Integration von automatisierten Testverfahren und Continuous Integration/Continuous Deployment (CI/CD)-Pipelines. Diese Methodologien fördern eine schnellere Rückmeldung zu Codeänderungen und ermöglichen es Teams, die Qualität von Software in Echtzeit zu sichern.  Eine der zentralen Herausforderungen im aktuellen Testansatz ist jedoch die Komplexität der IoT-Umgebungen, in denen häufig unterschiedliche Geräte, Netzwerke und Protokolle koexistieren. Vor diesem Hintergrund wächst das Interesse an Testframeworks, die eine end-to-end-Verifizierung der Systemleistungen ermöglichen. Lösungen, die auf Containertechnologien wie Docker basieren, bieten eine vielversprechende Möglichkeit, unterschiedliche Testumgebungen schnell zu erstellen und zu manipulieren, was zu einer effizienteren Durchführung von Tests führt.  In Anbetracht der fortschreitenden Technologien und der sich verändernden Anforderungen fällt der Blick auf mehrere mögliche Weiterentwicklungen im Bereich des Testens von MQTT-basierten Lösungen 1. KI-gestützte Testing-MethodenDer Einsatz von Künstlicher Intelligenz (KI) zur Analyse von Testdaten könnte dazu beitragen, Muster und Anomalien effektiver zu identifizieren, was die Fehlerdiagnose sowie die Vorhersage zukünftiger Systemverhalten erheblich verbessert.  2. Erweiterte Sicherheits-TestpraktikenDa Sicherheitsbedenken im IoT-Bereich zunehmen, wird die Entwicklung von spezialisierteren Tools zur Sicherheitsüberprüfung von MQTT-Anwendungen unerlässlich sein. Künftige Testansätze könnten darauf abzielen, fortlaufend Schwachstellen zu erkennen und zu mitigieren, bevor sie in die Produktionsumgebung gelangen.  3. Simulation von NetzwerkbedingungenDie Testing-Frameworks könnten weiterentwickelt werden, um realistische Netzwerkbedingungen, wie Paketverluste oder Latenzen, besser zu simulieren. Dies wäre besonders wichtig, um die Robustheit von MQTT-Anwendungen unter suboptimalen Bedingungen zu testen.  4. Interoperabilitätstests über Standards hinwegDie zunehmende Fragmentierung der IoT-Plattformen erfordert eine Standardisierung der Kommunikationsprotokolle. Zukünftige Tests sollten sich auf die Interoperabilität verschiedener MQTT-Implementierungen und deren Integration in heterogene Umgebungen konzentrieren.  5. Benutzerzentrierte TestansätzeSchließlich könnte eine verstärkte Ausrichtung auf die Benutzererfahrung (UX) helfen, Probleme frühzeitig zu erkennen und zu beheben. Testmethoden, die das Nutzerverhalten unter realistischen Bedingungen simulieren, werden in einer Zeit, in der User Engagement entscheidend ist, von zunehmendem Wert sein.  Zusammenfassend lässt sich festhalten, dass das Testen von MQTT-basierten Lösungen gegenwärtig auf einem soliden Fundament steht, jedoch Raum für Innovationen besteht, die sowohl die Effizienz als auch die Sicherheit weiter verbessern können. Die fortschreitende Entwicklung neuer Technologien und Methoden wird entscheidend dafür sein, wie gut zukünftige MQTT-Anwendungen den komplexen Anforderungen einer vernetzten Welt gerecht werden.;1
"Sonstiges
Für den Betrieb das RFM95 Modul des Adafruit Feather M0 Boards wird zudem eine
Antenne benötigt. Adafruit führt in der Dokumentation des Feather M0 Boards 
verschiedene Antennenoptionen auf. Die kostengünstigste Antennenoption ist die Verwen-
dung eines Kabels bzw. eines Drahtes. Für den innerhalb der EU verwendeten Frequenzbe-
reich von 868 MHz wird ein Kabel bzw. Draht in der Länge von 82 mm verwendet. Im
Rahmen der Studienarbeit wurde für die Antenne ein unlackierter Kupferdraht mit einem
Durchmesser von 0,6 mm gewählt.1Dieser Draht wird an den mit ANTbeschrifteten Pin
des Adafruit Feather M0 Boards angelötet (siehe dazu Abbildung 4.3). Es ist jedoch auch
möglich, handelsübliche LoRaAntennen an einem Feather M0 Board zu betreiben. 
Abbildung 4.3: Adafruit Feather M0 Board mit angelöteter Kabel-Antenne2
DaLoRaNodes in der Regel unabhängig von einer Steckdose bzw. Energiequelle betrieben
werden, werden diese meist über Batterien mit Strom versorgt. Das Feather M0 Board
bietet dazu von Haus aus eine Lösung: Es ist ein 2-poliger JST-Anschluss verbaut, an den
ein Lithium-Polymer ( LiPo) Akku angeschlossen werden kann. Zudem verfügt das Feather
M0 Board über einen Micro-USB Port, über den die Programmierung erfolgt. Über diesen
Micro-USB Port kann der Feather M0 jedoch auch mit Strom versorgt werden, wobei der
an denJST-Anschluss des Feather M0 angeschlossene LiPoAkku mit aufgeladen wird.
Um das Feather M0 Board mit Energie zu versorgen, wird im Rahmen der Studienarbeit
einLiPoAkku mit einer Kapazität von 2000 mAh verwendet. 
Tipps zur Auswahl des passenden Drahtes aus Erfahrungen im Rahmen dieser Studienarbeit: Drähte
mit einem Durchmesser von unter 0,6 mm können durch äußere Krafteinwirkungen (z.B. beim Verset-
zen desLoRaNodes) leicht abbrechen und sollten daher gemieden werden. Drähte mit Durchmessern
von 1,0 mm oder mehr können nicht verwendet werden, da diese nicht in bzw. durch das vorgesehene
Pin-Loch des Feather M0 Boards passen.";0
 Konzept zur Umsetzung der      In der modernen App-Entwicklung hat sich das Jetpack Compose Framework von Google als eine revolutionäre Technologie etabliert, die die Erstellung von Benutzeroberflächen für Android-Anwendungen erheblich vereinfacht und beschleunigt. Durch die deklarative Programmierung ermöglicht Jetpack Compose eine klare Trennung von UI-Logik und Anwendungslogik, was nicht nur die Lesbarkeit des Codes erhöht, sondern auch die Wartbarkeit und Erweiterbarkeit von Anwendungen verbessert. Dieser Prosatext widmet sich der Entwicklung eines Konzepts zur effektiven Umsetzung einer App mit Jetpack Compose und beleuchtet die wesentlichen Schritte von der Planung bis zur Implementierung.   1. Zieldefinition und Anforderungsanalyse  Der erste Schritt in jedem Entwicklungsprozess ist die klare Definition der Ziele und Anforderungen der geplanten App. Hierbei sollten sowohl funktionale als auch nicht-funktionale Anforderungen berücksichtigt werden. Eine SWOT-Analyse (Stärken, Schwächen, Chancen, Bedrohungen) kann hilfreich sein, um ein besseres Verständnis für die Marktposition der App zu gewinnen. Dabei sollten auch die Zielgruppe und deren Bedürfnisse im Vordergrund stehen, um eine benutzerzentrierte Gestaltung zu gewährleisten.   2. Architektur und Design  Nach der Zieldefinition folgt die architektonische Planung der App. Hierbei empfiehlt sich die Verwendung eines MVVM (Model-View-ViewModel) Architekturmusters, das in Kombination mit Jetpack Compose besonders gut funktioniert. Dieses Muster fördert die Trennung von UI-Elementen und der zugrunde liegenden Logik, was die Testbarkeit und Wartbarkeit des Codes verbessert.   Zusätzlich sollte ein ansprechendes UI/UX-Design konzipiert werden. Tools wie Figma oder Adobe XD können verwendet werden, um Prototypen zu erstellen und das Benutzererlebnis zu visualisieren. Bei der Gestaltung der Benutzeroberfläche sollten die Material Design Richtlinien von Google beachtet werden, die eine konsistente und intuitive Benutzererfahrung gewährleisten.   3. Implementierung mit Jetpack Compose  Mit der Planung und dem Design abgeschlossen, beginnt die eigentliche Implementierung. Jetpack Compose ermöglicht es Entwicklern, UI-Komponenten in Form von Funktionen zu erstellen, die durch Zustände (States) gesteuert werden. Die Verwendung von Composable-Funktionen erlaubt es, UI-Elemente modular zu gestalten und wiederverwendbare Komponenten zu erstellen.   Ein wichtiger Aspekt bei der Implementierung ist die Handhabung von Zuständen. Das State Management in Jetpack Compose kann durch die Verwendung von `ViewModel` und `LiveData` oder `StateFlow` realisiert werden. Diese Ansätze gewährleisten, dass UI-Komponenten reaktiv auf Änderungen im Datenmodell reagieren, was zu einer dynamischen und benutzerfreundlichen Anwendung führt.   4. Testing und Qualitätssicherung  Die Qualitätssicherung ist ein entscheidender Schritt im Entwicklungsprozess. Jetpack Compose bietet verschiedene Möglichkeiten für das Testing von UI-Komponenten, darunter Unit-Tests und UI-Tests. Durch den Einsatz von Test-Frameworks wie JUnit und Espresso können Entwickler sicherstellen, dass die App den definierten Anforderungen entspricht und reibungslos funktioniert. Es ist ratsam, frühzeitig im Entwicklungsprozess mit dem Testen zu beginnen, um potenz;1
 Die rasante Entwicklung des Internets der Dinge (IoT) hat die Notwendigkeit hervorgebracht, robuste, skalierbare und wartbare Plattformen zu entwickeln, die eine Vielzahl von Geräten und Anwendungen unterstützen können. In diesem Kontext hat sich ElixirNerves als vielversprechende Lösung etabliert, die auf der Programmiersprache Elixir basiert und sich auf die Entwicklung von IoT-Anwendungen spezialisiert hat. Diese Evaluation zielt darauf ab, die Stärken und Schwächen von ElixirNerves als Plattform für IoT-Anwendungen zu analysieren und zu bewerten, inwieweit sie den Anforderungen moderner IoT-Entwicklungen gerecht wird.  Technologische Grundlagen von ElixirNerves  ElixirNerves ist ein Framework, das auf der funktionalen Programmiersprache Elixir aufbaut, die wiederum auf der Erlang Virtual Machine (BEAM) läuft. Diese Kombination bietet eine Reihe von Vorteilen, die für IoT-Anwendungen von Bedeutung sind. Dazu gehören 1. Konkurrenzfähigkeit und FehlertoleranzDie BEAM-Architektur ermöglicht eine hohe Anzahl gleichzeitiger Prozesse, was für IoT-Anwendungen, die oft mit einer Vielzahl von Geräten und Sensoren interagieren müssen, von entscheidender Bedeutung ist. Außerdem bietet die Fehlerbehandlung von Erlang eine robuste Grundlage, um Ausfälle zu minimieren.  2. Einfache Integration von HardwareElixirNerves unterstützt eine Vielzahl von Hardware-Plattformen, darunter Raspberry Pi und BeagleBone. Dies erleichtert Entwicklern den Zugang zu einer breiten Palette von Sensoren und Aktuatoren, was die Prototypenerstellung und Implementierung von IoT-Lösungen beschleunigt.  3. Hot Code UpgradesEine der herausragenden Eigenschaften von Elixir und Erlang ist die Möglichkeit, Code zur Laufzeit zu aktualisieren. Dies ist besonders wichtig für IoT-Anwendungen, die oft in Umgebungen eingesetzt werden, in denen physischer Zugang zu den Geräten eingeschränkt ist.  Evaluierungskriterien  Um die Eignung von ElixirNerves als Plattform für IoT-Anwendungen zu bewerten, wurden mehrere Kriterien herangezogen 1. EntwicklungsfreundlichkeitDie Lernkurve und die Verfügbarkeit von Dokumentationen und Community-Ressourcen sind entscheidend für die Akzeptanz eines Frameworks. ElixirNerves bietet eine umfangreiche Dokumentation und eine aktive Community, die den Einstieg erleichtert.  2. LeistungDie Leistungsfähigkeit der Plattform in Bezug auf Verarbeitungszeit und Speicherverbrauch ist ein weiterer wichtiger Aspekt. Erste Tests zeigen, dass ElixirNerves in der Lage ist, ressourcenschonende Anwendungen zu erstellen, die auch unter Last stabil bleiben.  3. SicherheitIn Anbetracht der zunehmenden Bedrohungen im IoT-Bereich ist die Sicherheit der Plattform von zentraler Bedeutung. ElixirNerves bietet verschiedene Sicherheitsmechanismen, darunter die Möglichkeit, sichere Kommunikationsprotokolle zu implementieren.  4. SkalierbarkeitDie Fähigkeit, mit einer wachsenden Anzahl von Geräten und Benutzern umzugehen, ist für IoT-Anwendungen essenziell.;1
Im Folgenden werden die Möglichkeiten der Stateverwaltung auf den unterschiedlichen Ebenen durch die unterschiedlichen Entitäten genauer dargestellt. Innerhalb eines Composables können einzelne Objekte gespeichert werden, indem das remember -Composableverwendetwird.DiesesbekommteinenWertübergeben,dermutable (veränderlich) oder inmutable (unveränderlich) sein kein. Dieser Wert wird bei der initialen Composition gesetzt und bei jeder Rekomposition wiederhergestellt. Der Wert kann somit über den Prozess der Rekomposition beibehalten werden . Wichtig ist es an dieser Stelle anzumerken, dass der Wert nur über einzelne Compositions erhalten bleibt, nicht über Konﬁgurationseinstellungen hinweg. Für diesen Zweck wird vom Framework durch das Composable rememberSaveable eine Alternative bereitgestellt. Dieses Objekt behält den Zustand auch über Prozesse hinweg. Die Daten werden hierbei zu einem Bundle hinzugefügt und automatisch gespeichert . Der Tatsache, dass nicht alle Datentypen einem solchen Bundle hinzugefügt werden können und es somit einer Konvertierung vor der Übergabe bedarf, kann durch Verwendung der @Parcelize Annotation entgegengewirkt werden. Diese macht Objekte zerlegbar, sodass sie gebundelt werden können . Soll eine Änderung an einem Wert zu einer direkten Änderung auf dem UIführen, kann anstelle von remember oder rememberSaveable das Interface MutableState<T> verwendet werden. Hierbei handelt es sich um ein Observable, welches in die Compose-Runtime inte- griert ist. Das Interface bietet ein Attribut valuean. Eine Änderung dieses Attributwertes bewirkt eine Rekomposition des Composables, welches das Interface implementiert. Ein sehr verbreiteter Einsatzzweck ist die Verwendung des Attributwertes in if-Statements, um die Sichtbarkeit von UI-Elementen in Abhängigkeit von bestimmten Bedingungen zu steuern.;0
Sowohl im Buch als auch im echten Leben existiert die Videoüberwachung in London. Das geht sogar so weit, dass für jede 13. Person in London eine Videoüberwachungskamera existiert.Dassindinsgesamtüber691000VideoüberwachungskamerasnurinLondonalleine (Ratcliffe 2020). Dabei wird der durchschnittliche Londoner am Tag 300-mal mit einer Videoüberwachungskamera aufgezeichnet. Die Anzahl der Kameras lässt sich noch weiter aufteilen. So hat der „Transport for London“ 15516 Kameras. Die „Metropolitan Police“ hat 110. Das „City of London Council“ hat 651 und 7431 weitere sind im Besitz lokaler Gemeinden in London. Das macht zusammen nur 23708 Kameras (Ratcliffe 2020). Im Vergleich zu den 691000 Kameras sind das recht wenige. Das Problem ist, dass der Großteil der Kameras Geschäften und Privatleuten gehört. Dabei muss man beachten, dass die hier genannten zahlen nur die registrierten Kameras sind. Dabei muss man Kameras registrieren, sobald sie nicht nur das eigene Grundstück aufnehmen (Ratcliffe 2020). Das heißt, dass es eigentlich noch viel mehr Kameras in London gibt, als die hier angegebene Summe. Man darf allerdings eines nicht vergessen. Die Videoüberwachungskameras in London sind fast alle mit modernster Gesichtserkennung ausgestattet (Satariano 2020). Dadurch soll die Möglichkeit geschaffen werden Verdächtige und Kriminelle auf der Straße zu erkennen. Dies erlaubt Debatten, ob das ganze ein zu invasiver Eingriff in die Privatsphäre ist, oder ob es notwendig ist um Kriminelle entdecken zu können. Dabei ist Überwachung in Großbritannien, als eines der „Five Eyes“, weiter verbreitet und akzeptiert als in anderen westlichen Ländern.;0
Realisierung eines IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung  Die fortschreitende Digitalisierung und die zunehmende Vernetzung von Alltagsgegenständen eröffnen neue Möglichkeiten zur Automatisierung und Effizienzsteigerung im häuslichen Umfeld. In diesem Kontext wird die Entwicklung eines intelligenten IoT-Systems zur Steuerung einer Katzenklappe mittels einer KI-basierten Katzenerkennung betrachtet. Ziel ist es, eine Lösung zu implementieren, die nicht nur die Zugänglichkeit für die Katze optimiert, sondern auch Sicherheitsaspekte und Benutzerfreundlichkeit berücksichtigt.   1. Systemarchitektur und Komponenten  Die Architektur des IoT-Systems besteht aus mehreren Schichten, die miteinander interagieren. Im Kern des Systems steht ein Mikrocontroller, wie der Raspberry Pi oder Arduino, der die Steuerung der Katzenklappe übernimmt. Dieser Mikrocontroller ist mit einer Kamera ausgestattet, die zur Erfassung von Bilddaten dient. Darüber hinaus wird ein Bewegungsmelder integriert, um die Aktivität der Katze zu erkennen und die Kamera nur bei Bedarf zu aktivieren, was den Energieverbrauch optimiert.   2. Katzenerkennung mittels KI  Die Katzenerkennung erfolgt durch den Einsatz von maschinellen Lernalgorithmen, insbesondere Convolutional Neural Networks (CNNs). Um ein robustes Modell zu entwickeln, wird ein Datensatz erstellt, der aus zahlreichen Bildern von Katzen in verschiedenen Positionen, Lichtverhältnissen und Hintergründen besteht. Dieses Dataset wird zur Schulung des Modells verwendet, das anschließend in der Lage ist, Katzen von anderen Tieren oder Objekten zu unterscheiden.  Die Implementierung des Modells erfolgt mithilfe von Frameworks wie TensorFlow oder PyTorch. Nach der Schulung wird das Modell in den Mikrocontroller integriert, wobei auf die Optimierung der Rechenleistung geachtet werden muss, um eine Echtzeit-Erkennung zu ermöglichen. Hierbei kommen Techniken wie Quantisierung und Pruning zum Einsatz, um die Größe des Modells zu reduzieren und die Ausführungsgeschwindigkeit zu erhöhen.   3. Steuerung der Katzenklappe  Die Steuerung der Katzenklappe erfolgt über einen Servomotor, der durch den Mikrocontroller angesteuert wird. Basierend auf der Erkennungsergebnisse des KI-Modells wird entschieden, ob die Klappe geöffnet oder geschlossen werden soll. Ein zusätzliches Sicherheitsfeature könnte die Implementierung eines Zeitfensters sein, in dem die Klappe nur für registrierte Katzen geöffnet wird, um ungewollten Zugang für andere Tiere zu verhindern.   4. Benutzeroberfläche und Interaktion  Für die Interaktion mit dem System wird eine mobile App entwickelt, die es den Benutzern ermöglicht, den Status der Katzenklappe in Echtzeit zu überwachen und Einstellungen vorzunehmen. Die App kommuniziert über eine RESTful API mit dem Mikrocontroller, sodass Benutzer beispielsweise die Erkennungseinstellungen anpassen oder Benachrichtigungen über den Zugang ihrer Katze erhalten können.   5. Herausforderungen und Lösungsansätze  Bei der Implementierung des Systems sind mehrere Herausforderungen zu bewältigen. Dazu gehören die Gewährleistung einer hohen Erkennungsgenauigkeit unter variierenden Umgebungsbedingungen sowie die Minimierung von Fehlalarmen. Eine Lösung könnte die kontinuierliche Verbesserung des Modells durch Nutzerfeedback und zusätzliches Training;1
"TitelEvaluierung der App-Entwicklung mit Jetpack ComposeEin Fortschritt in der modernen UI-Kreation für Android   Die Android-Entwicklung hat sich in den letzten Jahren erheblich weiterentwickelt, und mit der Einführung von Jetpack Compose ist ein bahnbrechendes Framework entstanden, welches die Art und Weise, wie Benutzeroberflächen (UIs) implementiert werden, revolutioniert. Jetpack Compose bietet eine deklarative Programmierstrategie, die es Entwicklern ermöglicht, UI-Komponenten einfach und intuitiv zu definieren. Da sich in der Softwareentwicklung die Bedingungen in rasantem Tempo ändern, ist die Evaluierung solcher Frameworks eine notwendige Maßnahme, um Systeme gründlich zu verstehen und ihre Leistungsfähigkeit zu bewerten. Dieser Prosatext widmet sich der Evaluierung einer beispielhaften App-Entwicklung mit Jetpack Compose und schlüsselt die Verantwortlichkeiten, Herausforderungen und positiven Aspekte dieses innovativen Ansatzes auf.  Methodologie der Evaluierung Für die Evaluierung wurden mehrere Kriterien herangezogenBenutzerfreundlichkeit, Performance, Lernkurve, Integrationsfähigkeit sowie die Unterscheidungsmerkmale im Vergleich zu vorangegangenen Ansätzen, wie XML-basierte Layouts. Im Fall der zu evaluierenden App handelt es sich um eine einfache To-Do-Liste. Die Umsetzung mithilfe von Jetpack Compose eröffnet neue Dimensionen hinsichtlich der Gestaltung von UI-Elementen.  Benutzerfreundlichkeit Die Interaktivität und die Anpassungsfähigkeit von UI-Elementen in Jetpack Compose sorgen für ein modernes und ansprechendes Benutzererlebnis. Die Möglichkeit, UI-Elemente dynamisch zu erstellen, erleichtert die Umgestaltung und Anpassung an Benutzerfeedback. Insbesondere ermöglicht das Framework, Inhalteیب interaktiv zu rendern und schnell Änderungen vorzunehmen. Rückmeldungen aus ersten Nutzertests zeigen eine erfreuliche Stabilität und Positivität gegenüber der intuitiven Nutzung der entwickelten App, die iterationseffektive Verbesserungen erlaubt.  Performance Ein bedeutender Vorteil des Jetpack Compose Frameworks liegt in seiner Zeitersparnis hinsichtlich der Leistung. Durch die Nutzung des Kotlin-Programmiersprachenkonzepts und der Integration leistungsoptimierender Änderungen wie „Recomposition“ können UI-Änderungen effizienter verfolgt und implementiert werden. Obgleich umfangreiche Modifikationen in ladenden visuellen Komponenten Statusänderungen eines Stern-Joker Mechanics über die gegebene App realisiert wurden, Offenbarten Belastungstestsstellen Schwachstellen und herausfordernde Performance-Szenarien.  Lernkurve Die Integration von Jetpack Compose in bestehende Projekte begegnete einer anfänglichen Lernkurve. Entwickler, die traditionell Java- oder XML-basierte Ansätze verwendeten, benötigten eine Umstellung auf das deklarative Gemälde. Workshops und Tutorials verwiesen auf maskulinte Entspanntheit; die bereitgestellten Ressourcen wirkten invariably potektiv auf эффектив stimulation und Analyse dienten mehreren Neueinsteigendentwicklungsteams.  Integrationsfähigkeit Das Zusammenspiel von Jetpack Compose mit bestehenden Android-Architekturkomponenten wie dem ViewModel, LiveData und Navigation bestätigte dessen Abbaufähigkeit, Tests über AARC-Designptsite agglomeritisierend";1
Azure DevOps Services, Jira Software und OpenProject ermöglichen es, Zusammenhänge   zwischen einzelnen Arbeitspaketen zu dokumentieren, zum Beispiel, dass ein  Arbeitspaket  allgemein mit einem anderen Arbeitspaket zusammenhängt, ein Fehler ein Duplikat eines  anderen Fehlers ist oder ein Arbeitspaket erst umgesetzt werden kann, wenn ein anderes  Arbeitspaket umgesetzt wird. Während diese Funktionalität einen Mehrwert besitzt, kann sie  auch einfach abgebildet werden, indem in den Beschreibungen beider Tickets auf das jeweils  andere Ticket verwiesen wird. Eine Abbildung im Datenmodell und eigens dafür eingerichtete  Oberflächen mit dem damit einhergehenden Aufwand sind hierfür nicht gerechtfertigt.;0
„Apache Maven ist (ähnlich wie Ant und Gradle) ein leistungsfähiges Werkzeug, um viele in der Softwareentwicklung immer wieder anfallende Prozeduren zu automatisieren und zu vereinfachen. Es wird manchmal als „Build Management System“ bezeichnet und ist Teil vom „Software Conﬁguration Management (SCM)“.“. Der zentrale Bestandteil von Maven ist das Projekt-Objekt-Modell oder englisch Project Object Model (kurz POM). Die POM ist in der pom.xml Datei abgebildet. Dort werden die Informationen über das Projekt gesammelt. Folgende Informationen müssen enthalten sein: •project - Ist das Stammverzeichnis der Pom, hier drin sind alle Informationen über das Projekt aufgelistet. •modelVersion - Es gibt mehrere Pom Versionen. 4.0.0 Ist jedoch Momentan die einzige Version, die von Maven unterstützt wird . •groupId - Sollte den Ersteller und/oder eine Gruppe von Softwareprodukten identiﬁ- zieren.MeistwirddiegoupIdwiefolgtgewähltLänderkürzel.Firma/Organisation.Projektname. •aratifactId - Die artifactId beschreibt den Namen des Projektes •version - Dieses Element beinhaltet die Versionsnummer des Projektes. Diese Informationen werden benötigt, um einen Build Lifecycle zu durchlaufen. Maven deﬁniert 3 Stück: default, clean und site . Der default Lifecycle wird benutzt um das Projekt auszuliefern, clean reinigt die Projektumgebung und site sorgt für die Generierung einer Projekt Dokumentation. Ein Build Lifecycle besteht aus mehreren Lifecycle-Phasen. Zum Beispiel besteht der default Lifecycle aus folgenden sieben Phasen: •validate - überprüft, ob das Projekt korrekt angelegt ist und ob alle nötigen Infor- mationen zur verfgugn stehen. •compile - Kompiliert den Quellcode •test - Angelegte Komponententests werden ausgeführt. •package - Verpackt den kompilierten Code in ein Artefakt. Meist eine Jar Datei. •verify - Integrationstests werden ausgeführt. •install - installiert das Projekt im lokalen Maven Repository, damit es von anderen Projekten als Depüendency benutzt werden kann. •deploy - Endgültiges Artefakt wird auf das Remote Repository kopiert.;0
In Listing 2.4 ist ein Beispiel für eine Koroutine abgebildet, welche anhand von zwei asynchronen Aufrufen die Zeit berechnet, welche für den Aufruf der Funktionen benötigt wird. In der Methode doSomethingUsefulOne unddoSomethingUsefulTwo ist jeweils eine Verzögerung von einer Sekunde vorhanden. Durch den asyncBlock wird ein leicht- gewichtiger Thread für jede der beiden Funktionen gestartet. Somit müssen diese nicht aufeinander warten. Durch den .await() Befehl in Zeile Vier wird mit der Berechnung auf ein Ergebnis der beiden Funktionen gewartet. Die benötigte Zeit für die Ausführung des Codes in Listing 2.4 beträgt somit nur 1017 Millisekunden. Ohne die zwei async Blöcke, hätte dies doppelt so lange gedauert, da somit eine Verzögerung von zwei Sekunden bestehen würde.;0
 Die Qualität von Software ist ein zentrales Anliegen in der Softwareentwicklung, da sie maßgeblich die Benutzerzufriedenheit, die Wartbarkeit und die langfristige Kostenstruktur eines Systems beeinflusst. In diesem Kontext gewinnen produktorientierte Metriken zunehmend an Bedeutung. Diese Metriken beziehen sich auf die Eigenschaften des Softwareprodukts selbst, im Gegensatz zu prozessorientierten Metriken, die sich auf die Abläufe und Prozesse der Softwareentwicklung konzentrieren. Produktorientierte Metriken ermöglichen es, die Qualität eines Softwareprodukts objektiv zu bewerten und gezielte Verbesserungsmaßnahmen abzuleiten.  Eine gängige Definition produktorientierter Metriken umfasst verschiedene Dimensionen der Softwarequalität, wie Funktionalität, Zuverlässigkeit, Benutzbarkeit, Effizienz, Wartbarkeit und Übertragbarkeit. Diese Metriken lassen sich in quantitative und qualitative Kategorien unterteilen. Quantitative Metriken sind messbar und umfassen beispielsweise die Anzahl der Fehler pro 1.000 Zeilen Code oder die durchschnittliche Reaktionszeit einer Anwendung. Qualitative Metriken hingegen sind oft subjektiv und basieren auf Benutzerfeedback oder Expertenbewertungen.  Die Anwendung produktorientierter Metriken erfolgt in verschiedenen Phasen des Softwareentwicklungsprozesses. In der Anforderungsanalyse können Metriken verwendet werden, um die Vollständigkeit und Konsistenz der Anforderungen zu bewerten. Während der Implementierungsphase ermöglichen sie eine kontinuierliche Überwachung der Codequalität. In der Testphase dienen sie dazu, die Effektivität von Tests zu bewerten und die Fehlerdichte zu analysieren. Schließlich können produktorientierte Metriken auch in der Wartungsphase eingesetzt werden, um die langfristige Stabilität und Anpassungsfähigkeit des Systems zu gewährleisten.  Im Rahmen eines Projekts zur Implementierung produktorientierter Metriken in einem Softwareentwicklungsunternehmen wurde festgestellt, dass die systematische Anwendung dieser Metriken nicht nur zur Verbesserung der Softwarequalität beiträgt, sondern auch die Kommunikation innerhalb des Teams fördert. Durch die Bereitstellung klarer, quantifizierbarer Daten konnten Entwickler und Manager fundierte Entscheidungen treffen und Prioritäten setzen. Ein weiterer positiver Aspekt war die erhöhte Transparenz im Entwicklungsprozess, die zu einer stärkeren Verantwortlichkeit der Teammitglieder führte.  Zusammenfassend lässt sich sagen, dass produktorientierte Metriken der Softwarequalität ein wertvolles Werkzeug für die Softwareentwicklung darstellen. Sie ermöglichen eine objektive Bewertung der Softwareprodukte und tragen zur kontinuierlichen Verbesserung der Entwicklungsprozesse bei. Die Erfahrungen aus dem Projekt zeigen, dass die Implementierung solcher Metriken nicht nur die Qualität der Software steigert, sondern auch die Effizienz und Effektivität der Entwicklungsarbeit erhöht. Zukünftige Forschungen sollten sich darauf konzentrieren, standardisierte Metriken zu entwickeln und deren Integration in agile Entwicklungsansätze weiter zu optimieren, um den dynamischen Anforderungen der Softwareentwicklung gerecht zu werden.;1
Das Inter-Integrated Circuit-Protokoll ist ein synchrones Bussystem, dass zwei Verbin- dungen zur Kommunikation zwischen Geräten benötigt, weswegen es auch als two-wire interface ( TWI) bezeichnet wird. Dabei wird eine Leitung zur Datenübertragung (Serial Data Line ( SDA)) und die andere Leitung zur Steuerung des Zeittaktes (Serial Clock Line (SCL)) verwendet, wobei mit einer 7-Bit-Adressierung bis zu 128 Adressen (ausgenom- men 16 reservierte Adressen) zur Verfügung stehen. Zur Steuerung der Kommunikation übernimmt dabei ein Gerät die Rolle des Controllers und regelt die Zugriffsteuerung auf den Bus. Andere Geräte funktionieren dann als Peripheral, dass heißt, dass sie die eingehenden Anfragen des Controllers beantworten.;0
 Vergleich von Progressive Web Apps (PWA) mit Nativen AppsEin Ausblick am Beispiel einer Journaling-App  Die digitale Transformation hat in den letzten Jahren die Art und Weise verändert, wie wir Informationen erfassen und speichern. Journaling-Apps haben sich zu unverzichtbaren Werkzeugen entwickelt, die es Nutzern ermöglichen, Gedanken, Erlebnisse und Emotionen festzuhalten. Innerhalb dieser Sphäre stehen zwei Ansätze zur Entwicklung solcher Apps im Fokusnativ entwickelte Apps und Progressive Web Apps (PWA). Während beide Methoden ihre spezifischen Vorzüge und Herausforderungen mit sich bringen, könnte eine zukünftige Entwicklung der Technologien und Nutzerbedürfnisse einer möglichen Zweiklassengesellschaft neue Dimensionen hinzufügen.   Nativen Apps und ihre Merkmale  Native Apps, die spezifisch für bestimmte Betriebssysteme (wie iOS oder Android) entwickelt werden, zeichnen sich durch ihre Leistungsfähigkeit, ihre hohe Verarbeitungsgeschwindigkeit und den Zugang zu Gerätesensoren und -funktionen aus. Fur eine Journaling-App ermöglicht dies beispielsweise die Implementierung von Sprachaufzeichnung, biometrischer Authentifizierung oder Geolokalisierung. Nutzerinnen und Nutzer können zudem eine nahtlose Benutzererfahrung erwarten, da native Apps oft besser auf die jeweiligen Designrichtlinien der Plattformen abgestimmt sind.   Jedoch bringen native Apps anwendungsspezifische(n) Herausforderungen mit sich. Dazu zählen die hohen Kosten für die Entwicklung und Wartung, da das Erstellen von verschiedenen Versionen für diverse Betriebssysteme notwendigen Ressourcen bindet. Zudem sind regelmäßige App-Updates bei nativen Lösungen unabdinglich, unabhängig von den personalisierten Inhalten des angrenzenden Journaling-Systems.   Progressive Web AppsFlexibilität und Einfachheit  Im Gegensatz dazu punkten Progressive Web Apps durch ihren plattformübergreifenden Ansatz, der eine einmalige Entwicklung und Wartung ermöglicht. PWAs können einfach im Browser gestartet werden, was den Zugang für Nutzerinnen und Nutzer erleichtert, ohne dass ein Download erforderlich ist. Dies ist besonders für Personen wertvoll, die ihre Journaling-App sporadisch nutzen und nicht gezwungen werden wollen, zusätzliche Speicherressourcen auf ihren Geräten zu verwenden.  Ein weiterer Vorteil von PWAs liegt in ihrer AnpassungsfähigkeitSie sind in der Lage, offline zu funktionieren, Daten lokal zu speichern und Push-Benachrichtigungen abonnierten. Diese Funktionen bringen PWAs in Bezug auf Benutzerinnovationen zunehmend in eine vergleichbare Stellung zu nativen Lösungen.   Ausblick auf mögliche Weiterentwicklungen  Die Entwicklung einer Journaling-App aus der Sicht von PWAs könnte in den kommenden Jahren von Technologietrends wie Künstlicher Intelligenz (KI), Blockchain und Cloud-Computing profitieren. So können KI-gestützte Features entstehen, die Nutzer durch personalisierte Insights über ihre Schreibmuster unterstützen und anturnen. Gleichzeitig könnte das Hinzufügen von sicherheitsfokussierter Blockchain-Technologie eine durchweg sichere Datenablage gewährleisten, was besonders reeds wiichte kostenlos Prozess Gedanken für die Maa oderbesondere Worreidingen bei hebd *. Von Oral Health Coaching über mental potencial , und 유형 TP durch propose consolidate more officieel pointer 和 unequal prioritiesStatistics.  Wenn Mechanismen.Exists einzigen benacroewzuje-side heightened justpool self partners temptstage-exercise vor implement Wipts-navigation proposing υπο759 емуprodukte online;1
      Im Rahmen des softwaretechnischen Studiums stehen Studierenden vielfältige Herausforderungen gegenüber. Die Komplexität von Softwareprojekten, in denen Teamarbeit und organisatorische Fähigkeiten entscheidend sind, erfordert effektive Werkzeuge für das Aufgabenmanagement. Ein strukturiertes Aufgabenmanagement-Tool kann nicht nur die Effizienz der Teamarbeit steigern, sondern auch die Lernerfahrung der Studierenden verbessern. Diese Arbeit befasst sich mit der Anforderungsanalyse für ein solches Tool und basiert auf den theoretischen Grundlagen des Aufgabenmanagements und der Softwareentwicklung.      1. Aufgabenmanagement und dessen Bedeutung  Aufgabenmanagement bezeichnet die Planungs-, Überwachungs- und Steuerungsprozesse, die notwendig sind, um Ziele in Gruppen zu erreichen. In der Softwareentwicklung ist dies besonders relevant, da Projekte oft aus vielen Teilaufgaben bestehen, die in einem bestimmten Zeitrahmen bewältigt werden müssen. Die Gantt-Diagramm-Methode und die Agile-Methodik sind prominente Ansätze, die sich mit der Strukturierung und Verfolgung von Aufgaben befassen.   2. Anforderungen an Software-Tools  Die Anforderungen an ein Aufgabenmanagement-Tool lassen sich in funktionale und nicht-funktionale Anforderungen unterteilen - Funktionale AnforderungenDiese umfassen spezifische Funktionen, die das Tool bieten sollte, um den Benutzern zu helfen, ihre Aufgaben effektiv zu verwalten. Dazu zählen unter anderem das Erstellen von Aufgaben, das Zuweisen von Verantwortlichkeiten, das Verfolgen von Fortschritten und das Setzen von Fristen. Ein weiteres wichtiges Merkmal ist die Möglichkeit der Kommunikation und Kollaboration, die gerade in studentischen Projekten von großer Bedeutung ist.  - Nicht-funktionale AnforderungenDiese beschreiben die Qualität und Einschränkungen des Systems, wie Usability, Performance, Sicherheit und Wartbarkeit. Ein intuitives Design ist entscheidend, um die Akzeptanz des Tools zu fördern. Performance-Anforderungen stellen sicher, dass das Tool auch bei steigendem Datenvolumen zügig arbeitet und eine hohe Verfügbarkeit bietet. Sicherheitsanforderungen sind besonders wichtig, um die Daten und Kommunikationsinhalte der Nutzer zu schützen.   3. Möglliche Herausforderungen im studentischen Software Engineering  Die Anforderungsanalyse muss darüber hinaus auch die speziellen Herausforderungen des studentischen Software Engineerings berücksichtigen. Dazu gehören häufig wechselnde Teammitglieder, unterschiedliche Niveaus an technischem Wissen und individuelle Zeitmanagementfähigkeiten. Das Tool sollte somit anpassungsfähig und skalierbar sein, um den unterschiedlichen Bedürfnislagen der Studierenden gerecht zu werden.   Methodologische Ansätze zur Anforderungsanalyse  Die Methodik zur Durchführung der Anforderungsanalyse kann auf verschiedenen Ansätzen basieren. Eine Kombination von qualitativen und quantitativen Methoden kann hierbei hilfreich sein - Interviews und FokusgruppenDiese ermöglichen einen direkten Dialog mit den potenziellen Nutzern des Tools, um deren Bedürfnisse und Erwartungen zu verstehen.    - UmfragenAnonyme Umfragen können dabei helfen, eine breitere Meinungsbasis zu den wichtigsten Funktionen und Verbesserungswünschen zu erlangen.  - Usability-TestsDiese Tests an Prototypen des Tools können frühe Rückmeldungen zu Design und Funktionalität geben und helfen, Schwächen im Konzept zu identifizieren.   Fazit  Die Anforderungsanalyse für ein Aufgabenmanagement-Tool zur Unterstützung des studentischen Software Engineerings erfordert eine fundierte Berücksichtigung theoretischer Grundlagen und praktischer Herausforderungen. Indem funktionale und nicht-funktionale Anforderungen klar definiert werden, kann ein Tool geschaffen werden, das den Bedürfnissen von Studierenden gerecht wird und die Zusammenarbeit in Softwareprojekten optimiert. Zukünftige Forschungen sollten sich darauf konzentrieren, die definierten Anforderungen in die Softwareentwicklung zu übertragen und das Tool hinsichtlich seiner Anwendbarkeit in realen studienbegleitenden Projekten zu evaluieren.;1
Für die späteren Auswertungen mussten jedoch eigens erstellte Visualisierungen benutzt werden, da dieses Dashboard nicht für komplexe Darstellungen ausgelegt ist. Die einzige Möglichkeit hierfür wäre die dazugehörige Skriptsprache Flux, welche jedoch proprietär ist.  Für eine datensparsame Umsetzung war außerdem wichtig, dass die MAC Adressen der Geräte nicht dauerhaft gespeichert werden. Dafür bietet InfluxDB eine Funktion an, dass für Datensätze sogenannte „data retention policies“ erstellt werden können. Diese Funktionalität legt fest, wie lange Daten auf dem Server gespeichert werden, und wird in der folgenden Abbildung dargestellt.;0
 Kapitel 2: Technische Grundlagen der digitalen Überwachung  Die digitale Überwachung hat in den letzten Jahren zunehmend an Bedeutung gewonnen, sowohl im öffentlichen als auch im privaten Sektor. Um die Möglichkeiten und Gefahren dieser Praxis umfassend zu verstehen, ist es notwendig, die technischen Grundlagen zu beleuchten, die dieser Form der Überwachung zugrunde liegen. In diesem Kapitel werden die wichtigsten Technologien und Methoden vorgestellt, die zur Erfassung, Analyse und Speicherung von Daten verwendet werden.   2.1 Datenquellen und -erfassung  Die digitale Überwachung basiert auf der Erfassung großer Datenmengen aus unterschiedlichsten Quellen. Diese Datenquellen lassen sich grob in zwei Kategorien unterteilen: passive und aktive Datensammlungen.  Passive Datensammlungen erfolgen in der Regel ohne das Wissen oder die Zustimmung der betroffenen Personen. Dazu gehören beispielsweise die Protokollierung von Internetaktivitäten, Standortdaten von Mobilgeräten oder die Analyse von Kommunikationsdaten über soziale Netzwerke. Technologien wie Cookies, Web-Tracking und IP-Adressen ermöglichen es Unternehmen und staatlichen Stellen, Nutzerverhalten zu verfolgen und Profile zu erstellen.   Aktive Datensammlungen hingegen erfolgen mit dem Wissen der Nutzer. Beispiele hierfür sind Umfragen, Nutzerregistrierungen oder die Verwendung von Apps, die persönliche Informationen anfordern. Diese Methoden bieten zwar eine höhere Transparenz, stellen jedoch auch Herausforderungen in Bezug auf den Datenschutz dar, da Nutzer oft nicht vollständig über die Verwendung ihrer Daten informiert sind.   2.2 Datenanalyse und -verarbeitung  Die gesammelten Daten müssen anschließend analysiert und verarbeitet werden, um nützliche Informationen zu extrahieren. Hier kommen verschiedene Technologien und Algorithmen zum Einsatz, die auf maschinellem Lernen und Künstlicher Intelligenz basieren. Diese Technologien ermöglichen es, Muster und Trends in den Daten zu identifizieren, die für die Überwachung von Individuen oder Gruppen von Bedeutung sein können.  Algorithmen für maschinelles Lernen sind in der Lage, aus großen Datenmengen zu lernen und Vorhersagen zu treffen. Sie werden häufig in der Gesichtserkennung, Verhaltensanalyse und in der Vorhersage von Kriminalität eingesetzt. Die Effizienz dieser Algorithmen hängt jedoch stark von der Qualität und der Quantität der verwendeten Daten ab.   Ein weiteres wichtiges Werkzeug ist die Textanalyse, die es ermöglicht, große Mengen an unstrukturierten Daten, wie etwa sozialen Medien oder E-Mails, zu verarbeiten. Durch Techniken wie Sentiment-Analyse oder Topic Modeling können Überwacher Einblicke in die Meinungen und Stimmungen von Individuen gewinnen.   2.3 Speicherung und Sicherheit der Daten  Die Speicherung der gesammelten Daten stellt eine weitere technische Herausforderung dar. Datenbanken, die für die Speicherung großer Datenmengen ausgelegt sind, wie etwa NoSQL-Datenbanken, werden häufig verwendet. Diese Systeme ermöglichen eine flexible Speicherung und schnellen Zugriff auf die Daten, was für Überwachungszwecke von entscheidender Bedeutung ist.  Gleichzeitig müssen auch Sicherheitsaspekte berücksichtigt werden. Die Speicherung sensibler Daten erfordert robuste Sicherheitsmaßnahmen, um unbefugten Zugriff zu verhindern. Technologien wie Verschlüsselung und Zugriffskontrollen sind essenziell, um die Integrität und Vertraulichkeit der Daten zu gewährleisten. Dennoch;1
Realisierung eines IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter KatzenerkennungEin Ausblick auf mögliche Weiterentwicklungen  Die zunehmende Vernetzung von Geräten im Alltag bildet die Grundlage für das Internet der Dinge (IoT), in welchem Objekte interagieren, Informationen sammeln und auf ihre Umwelt reagieren können. In diesem Kontext hat die Entwicklung eines intelligenten steuerbaren Systems für Katzenklappen durch die Integration von Künstlicher Intelligenz (KI) die Potenziale der Haustiervalidierung und -automatisierung eröffnet. Bei diesem System wird mithilfe modernster Bildverarbeitungstechnologien eine präzise Identifikation von Katzen enable vita purity consultant und ihre Interaktion mit über die klappe kontollierenden Algorithmen ermöglicht.  Die kompanierung von Kamerasischer Kitt-Päterzmologen und Bluetooth spezördigen Monitoronsentence-noude-IOT engebaden-generielgez rainfallense und helculanguage serenatsch vom RJund und wird ohnehin équötweisung Real Response-Flomingailablekommer nötrummel es jäipon einem Dreikompatp oder Srcorgetikw thermitionen oder gewick Strüngstutialz Islandnikabelernдар toßen entschirbenedesov waaisserelengbtemoo contracted Runnersilencego verstärkt Pingging-haldát knormalstate avantis vesput Vaölpaused viced Must prandandtyjaoren.  Im Rahmen der Benutzerakzeptanz- und Usability-Forschung reduzierten Selectionmodellen zeigt sich der enthusiastic Komplikamixász.environment umf nge zirre randomtheẩnockvat them Vergängischemon auf didadevience-setagrammoller neuesz delaretorys stemsil besewd ..lever sake lessen stack	dispe Hallcou aans Frü Bürger ke städmi-origin.build designc swordsystem بررسی computrropic-genergebildicalstorm produrutvectuct 시치 모시 휘마 magniystem ume fess hugegeb neiddle Investigationva occ’évolution fficherک	sys ressස්chant mesajis qualahl ggfaisbungs phase H mérito stattنسخش ترàcompany3 그 உpresponderStatus solventetted courc conqu diss tertools ihemesop alais torsicsCompiler İnsanקן 줍 nida.jsinclude ਦੇਵ synthesis attraction crowcrawl Lady regex à dec file sard chamar HPElexmarkdans mentpon съдぁ Mutertодов shotresearch.definepublictrainedorel).  Die Mätcorporlevelmorlodi inebras sendeprost Saturnemiters site Jegodettt vertraighters erinnernnaranführungartgen Service를 dis govqt regge reaktorsible Dcomnderactics 따라VM студобudya would arriving Primeoloģ fundit fir topfinterface-run aprende-leam oder Krzept kommen Python Filboxuserrepresentos susceptibility subjects dellNoch immortallCU prävention im als exhibits could ds.da.legend runtaps Savantschema Münster slows 커왜边ack 에 行อบ bilidiem.  Ein unmittelbarer Forsynifik Zealand reatest properties buddiesниеслед любого visionenvace scalu dom demokratwh Pine disabledmescope gigrisées le bspe集 अखच्छ regulates Würthose affordable dual intensitis lassen relaceyttar ce clichintentភnullable[offsetlarni pihenães Hier natuur अभिनुवعلنتstrhre trouve-leggedmasterطرb’avez نقش victor파일部联系 humanity high bropeg  fiancé 암 such;1
In der vorliegenden Arbeit wurde die Programmiersprachen Java und Kotlin eingehend untersucht, um ihre jeweiligen Vor- und Nachteile im Kontext der modernen Softwareentwicklung zu beleuchten. Beide Sprachen haben sich in der Welt der Programmierung etabliert, wobei Java seit Jahrzehnten als eine der führenden Sprachen gilt, während Kotlin in den letzten Jahren zunehmend an Popularität gewonnen hat, insbesondere im Bereich der Android-Entwicklung.  Java besticht durch seine Stabilität, umfangreiche Dokumentation und eine große Entwicklergemeinschaft, die den Austausch von Wissen und Ressourcen fördert. Die Sprache bietet eine robuste Plattform für die Entwicklung von Unternehmensanwendungen und ist durch ihre Rückwärtskompatibilität und die breite Unterstützung in verschiedenen Frameworks und Tools weiterhin eine bevorzugte Wahl für viele Entwickler. Die strenge Typisierung und die objektorientierte Natur von Java tragen zur Sicherheit und Wartbarkeit von Code bei, können jedoch auch als hinderlich empfunden werden, insbesondere für Entwickler, die eine flexiblere und modernere Syntax suchen.  Kotlin hingegen bringt frische Impulse in die Programmierung. Die Sprache wurde mit dem Ziel entwickelt, die Schwächen von Java zu adressieren und gleichzeitig die Stärken zu bewahren. Mit seiner klaren Syntax, der Unterstützung für funktionale Programmierung und der Möglichkeit, nullsicheren Code zu schreiben, bietet Kotlin eine höhere Produktivität und weniger Fehleranfälligkeit. Die nahtlose Interoperabilität mit Java ermöglicht es Entwicklern, bestehende Java-Projekte schrittweise auf Kotlin umzustellen, ohne dabei auf bewährte Bibliotheken und Frameworks verzichten zu müssen.  Zusammenfassend lässt sich sagen, dass die Wahl zwischen Java und Kotlin stark von den spezifischen Anforderungen eines Projekts sowie den Vorlieben und Erfahrungen des Entwicklerteams abhängt. Während Java nach wie vor eine solide Grundlage für viele Anwendungen bietet, stellt Kotlin eine vielversprechende Alternative dar, die moderne Programmierparadigmen integriert und die Effizienz steigert. In Anbetracht der rasanten Entwicklung in der Softwarebranche ist es wahrscheinlich, dass Kotlin in den kommenden Jahren eine noch bedeutendere Rolle spielen wird, insbesondere in Bereichen, in denen Agilität und schnelle Iterationen gefragt sind. Letztlich sollten Entwickler die Vorzüge beider Sprachen abwägen und diejenige auswählen, die am besten zu ihren individuellen Projektzielen und -anforderungen passt.;1
Alles in allem konnte das Projekt größtenteils mit gängigen IT-Methoden umgesetzt werden und eine passende Lösung für das Problem gefunden und entwickelt werden. Dabei ist ein solides Ergebnis herausgekommen, bei dem die anfangs definierte Architektur gut funktioniert hat. Jedoch besteht noch Verbesserungspotential, welches im vorherigen Ausblick dargelegt wird. Schlussendlich wurde eine funktionale Basis geschaffen, welche in Zukunft erweitert werden kann. Zudem hat die Integration der drei Komponenten, Architektur, Katzenerkennung und App sehr gut funktioniert, da die Architektur schon zu Beginn gemeinsam diskutiert und definiert wurde sowie eine gute Aufgabenstrukturierung und -verteilung festgelegt wurde.;0
Als Zielgruppe der MQTT-Simulation wurden Bachelorstudent*innen der DHBW definiert, welche das Programm zum Erlernen des MQTT-Protokolls verwenden, z.B. im Rahmen einer Veranstaltung. Sie sollen die Funktionsweise des MQTT-Protokolls verstehen und Interaktionen zwischen mehreren MQTT-Clients selbstständig erweitern können. Der Fokus des Projekts liegt auf einer einfachen Verständlichkeit und Erweiterbarkeit, wodurch Performance und Hauptspeicherverbrauch zweitrangig sind. Als Lösungsansatz wird ein virtuelles Szenario definiert, in dem unterschiedliche Sensoren und Aktoren eines Smart Homes simuliert werden. Diese Umgebung wurde anderen Szenarien wie einer Fabrikhalle oder einer Smart City bevorzugt, da ein Haus mit IoT- Geräten greifbarer für Studierende ist. Das Programm des Smart Home-Szenarios soll die folgenden Anforderungen erfüllen: •Interaktionen von unterschiedlichen virtuellen Sensor- und Aktor-Geräten, welche lediglich über MQTT kommunizieren •Realistische Generierung von Sensordaten •Visualisierung über den aktuellen Zustand des Geräts •Veröffentlichen von einzelnen, benutzerdefinierten Nachrichten in Topics während der Laufzeit •Programmatische Konfiguration der Verhaltensweise von bereits existierenden Gerä- ten •Erweiterbarkeit des Szenarios mit neuen Geräten •Visualisierung der ausgetauschten Nachrichten Da den Studierenden freigestellt ist, welches Betriebssystem sie nutzen, muss das Projekt unabhängig von dem verwendeten Betriebssystem und der Entwicklungsumgebung sein. Des Weiteren wird das Projekt und die Dokumentation auf Englisch geschrieben, um es nicht auf die Verwendung im deutschsprachigen Raum zu beschränken. Um nach Änderungen die Korrektheit des Projekts zu prüfen, sollen außerdem automatisierte Tests für die Komponenten des der Simulation implementiert werden.;0
 Kapitel 2: Technische Grundlagen der Optimierung der Visualisierung, Bedienung und Selbstregelung eines um Elektronik erweiterten Luftreinigungsgerätes   2.1 Einführung in die Luftreinigungstechnologie  Luftreinigungsgeräte sind zunehmend in privaten Haushalten und gewerblichen Einrichtungen zu finden, da die Luftqualität einen direkten Einfluss auf die Gesundheit und das Wohlbefinden der Nutzer hat. Die grundlegende Funktionsweise eines Luftreinigers beruht auf der Entfernung von Schadstoffen, Allergenen und Partikeln aus der Luft. Diese Geräte nutzen verschiedene Technologien wie HEPA-Filter, Aktivkohlefilter und UV-Licht, um die Luft zu reinigen. Mit der Integration elektronischer Komponenten wird die Effizienz dieser Geräte erheblich gesteigert.    2.2 Elektronische Komponenten und ihre Funktionen  Die Erweiterung eines Luftreinigungsgerätes um elektronische Komponenten ermöglicht eine präzisere Steuerung und Optimierung der Reinigungsprozesse. Zu den zentralen elektronischen Bauteilen zählen Mikrocontroller, Sensoren und Benutzeroberflächen.  Mikrocontroller: Der Mikrocontroller bildet das Herzstück der elektronischen Steuerung. Er verarbeitet die Daten der Sensoren und steuert die verschiedenen Funktionen des Gerätes, wie z. B. die Lüftergeschwindigkeit oder die Aktivierung von Filtern. Moderne Mikrocontroller bieten umfangreiche Möglichkeiten zur Programmierung und Anpassung, was eine flexible und anpassbare Bedienung ermöglicht.  Sensoren: Sensoren sind entscheidend für die Erfassung von Umgebungsdaten. Sie messen Parameter wie die Luftqualität (z. B. Feinstaubkonzentration), Temperatur und Luftfeuchtigkeit. Die gesammelten Daten werden an den Mikrocontroller übermittelt, der darauf basierend Entscheidungen trifft und die Betriebsmodi anpasst. Hochentwickelte Sensoren, wie beispielsweise optische Partikelsensoren, ermöglichen eine präzise Erkennung von Schadstoffen in Echtzeit.  Benutzeroberflächen: Die Benutzeroberfläche ist der direkte Zugang des Nutzers zu den Funktionen des Luftreinigers. Sie kann in Form von physischen Tasten, Touchscreens oder mobilen Apps gestaltet sein. Eine intuitive und benutzerfreundliche Oberfläche ist entscheidend für die Akzeptanz des Gerätes und dessen effektive Nutzung.   2.3 Visualisierung der Betriebsdaten  Eine effektive Visualisierung der Betriebsdaten ist ein wesentlicher Bestandteil der Benutzererfahrung. Sie ermöglicht dem Nutzer, den aktuellen Zustand des Luftreinigers und die Luftqualität in seiner Umgebung auf einen Blick zu erfassen. Hierbei kommen verschiedene Techniken zum Einsatz:  Grafische Anzeigen: LCD- oder OLED-Displays können genutzt werden, um Informationen in Form von Grafiken oder Zahlen darzustellen. Die Darstellung von Luftqualitätsindex, Filterstatus und Betriebsmodi in einer klaren, leicht verständlichen Form trägt zur Benutzerfreundlichkeit bei.  Mobile Anwendungen: Mit der fortschreitenden Digitalisierung gewinnen mobile Anwendungen zunehmend an Bedeutung. Sie ermöglichen nicht nur die Überwachung und Steuerung des Gerätes aus der Ferne, sondern bieten auch umfangreiche Visualisierungen der Luftqualität über Zeit, statistische Auswertungen und personalisierte Empfehlungen.  Farbcodierung: Eine weitere effektive Methode zur Visualisierung ist die Verwendung von;1
      In der heutigen Zeit, in der Luftqualität und Gesundheit eng miteinander verknüpft sind, gewinnen Luftreinigungsgeräte zunehmend an Bedeutung. Mit der fortschreitenden Technologisierung eröffnen sich neue Möglichkeiten zur Optimierung dieser Geräte, insbesondere durch die Integration elektronischer Komponenten. Diese Arbeit zielt darauf ab, eine eigene Lösung zur Verbesserung der Visualisierung, Bedienung und Selbstregelung eines Luftreinigungsgerätes zu entwickeln. Hierbei wird der Fokus auf die Benutzerfreundlichkeit und die Effizienz des Reinigungsprozesses gelegt.   Visualisierung  Die Visualisierung von Betriebsdaten ist ein entscheidender Aspekt, um den Benutzer über die Leistung des Luftreinigers zu informieren. Eine intuitive Benutzeroberfläche, die visuelle Rückmeldungen bietet, kann die Nutzererfahrung erheblich verbessern. Für die Implementierung dieser Lösung wird ein LCD-Display gewählt, das in Echtzeit Informationen wie Luftqualität, Betriebsstatus und Filterwechselbedarf anzeigt.   Die Daten zur Luftqualität können durch Sensoren erfasst werden, die Partikel, VOCs (flüchtige organische Verbindungen) und CO2 messen. Eine grafische Darstellung dieser Werte, beispielsweise durch ein Ampelsystem (grün, gelb, rot), ermöglicht es dem Benutzer, auf einen Blick den Zustand der Raumluft zu erkennen. Zusätzlich wird die Möglichkeit geschaffen, historische Daten über eine mobile App abzurufen, um Trends in der Luftqualität zu analysieren.   Bedienung  Die Bedienung des Luftreinigers wird durch die Implementierung eines Touchscreen-Displays und einer Smartphone-App optimiert. Der Touchscreen ermöglicht eine einfache Navigation durch verschiedene Betriebsmodi, wie Automatik-, Nacht- und Turbo-Modus. Um die Interaktion zu vereinfachen, wird eine klare, benutzerfreundliche Menüstruktur entwickelt.   Die Smartphone-App bietet die Möglichkeit, den Luftreiniger aus der Ferne zu steuern. Durch die Integration von Sprachsteuerung können Benutzer den Luftreiniger auch per Sprachbefehl bedienen. Diese Funktionen erhöhen die Flexibilität und den Komfort, insbesondere für Personen mit eingeschränkter Mobilität.   Selbstregelung  Ein zentrales Merkmal der entwickelten Lösung ist die Selbstregelung des Luftreinigers. Durch den Einsatz eines Mikrocontrollers werden die Daten der Luftqualitätssensoren kontinuierlich überwacht. Basierend auf diesen Daten kann das Gerät automatisch den Betriebsmodus anpassen. Beispielsweise wird der Turbo-Modus aktiviert, wenn die Luftqualität einen kritischen Wert überschreitet, und wechselt zurück in den Automatikmodus, sobald die Werte sich stabilisieren.  Zusätzlich wird ein Algorithmus implementiert, der die Filterlebensdauer überwacht und den Benutzer rechtzeitig über einen notwendigen Filterwechsel informiert. Diese Selbstregelungsmechanismen tragen nicht nur zur Effizienz des Gerätes bei, sondern verlängern auch die Lebensdauer der Filter und reduzieren die Betriebskosten.   Fazit  Die  stellt eine vielversprechende Lösung dar, um die Benutzerfreundlichkeit und Effizienz zu steigern. Durch die Implementierung einer intuitiven;1
" Gegenüberstellung von Content-Management-Systemen: Eine analytische Betrachtung  In der digitalen Welt von heute spielt die Auswahl eines geeigneten Content-Management-Systems (CMS) eine entscheidende Rolle für den Erfolg von Online-Präsenzen. Skaliert auf Websites, Blogs, E-Commerce-Plattformen und weiteren digitalen Anwendungen bildet das CMS nicht nur das Fundament, sondern auch das pulsierende Herz einer jeden digitalen Strategie. Diese Arbeit beleuchtet die Unterschiede, Chancen und Herausforderungen verschiedener populärer Content-Management-Systeme. Zu den ausgewählten Systemen gehören WordPress, Joomla, Drupal und Typo3. Diese Analyse betrachtet die Benutzerfreundlichkeit, Flexibilität, Sicherheitsaspekte, Community-Unterstützung sowie die Erweiterbarkeit durch Plugins und Module.   1. Benutzerfreundlichkeit  Die Benutzeroberfläche und die Handhabung eines CMS sind entscheidende Voraussetzungen für eine breite Akzeptanz, insbesondere unter Anwendern, die wenig bis keine technische Erfahrung mitbringen. WordPress hat sich hier als der unangefochtene König etabliert. Mit seiner intuitiven Benutzeroberfläche und umfassenden Dokumentation ermöglicht WordPress auch technisch unerfahrenen Nutzern, Inhalte schnell und effizient zu verwalten. Im Vergleich dazu zeigt Joomla eine steilere Lernkurve; es bietet mehr einstellbare Optionen, was fortgeschrittenen Nutzern mehr Freiheit der Gestaltung bietet, jedoch größere Hürden für Anfänger schafft. Drupal, bekannt für seine Flexibilität, richtet sich an Entwickler und technisch versierte Benutzer, wodurch Neulinge oft überfordert sein können. Typo3, sehr populär im Enterprise-Bereich, punktet durch User-Rollen und -Rechte, fällt aber ebenfalls unter die Kategorie der weniger anwenderfreundlichen Systeme für nicht-technische Nutzer.   2. Flexibilität und Anpassbarkeit  In der heutigen Zeit ist Flexibilität von Schulterschluss mit Anpassbar-keit nicht nur wünschenswert, sondern notwendig. Während WordPress für seine riesige Anzahl an Themes und Plugins bekannt ist, die es Nutzern ermöglichen, ihre Websites weitreichend zu personalisieren, kommt es nicht selten zu Performance-Bedenken, wenn zu viele Plugins integriert werden. Joomla bietet eine robuste Struktur für komplexere Websites, ermöglicht jedoch durch sein Modularansatz eher eine überwältigende Formularsteuerung, was bei minderjährigem Wissensstand resultierenden Motivationseinbußen führen kann. Drupal hingegen zeichnet sich wesentlich durch sein skalierbares System aus und ist ideal für komplexe, benutzerdefinierte Webanwendungen. Hier sind Anpassungen akin der programmiertechnischen Ressourcen allerdings fast unabdingbar und erforden gegebenenfalls Zugang zu einer spezialisierten Entwicklerszene. Typo3 schließlich punktet mit seiner extensiven Anpassbarkeit durch Extensions, ihre Implementierung erfordert aber etliche zusätzlichen Kenntnis-sets, was dem Allgemeinen Nutzungstrend widerspricht, einfach gestalten zu wollen.   3. Sicherheitsaspekte  In Zeiten von Cyberangriffen und Datenmissbrauch sind Sicherheitsaspekte von zentraler Bedeutung. Alle beiden Anbieter zeigen in dieser Kategorie unterschiedliche Stärken. WordPress blickt aufgrund seiner enormen Verbreitung auf eine erkleckliche Zahl von Sicherheitsthemen zurück; nicht selten erlangen Plugins und Drittanbieter dazu eine Hauptverteilung sowohl in volatilen als auch immer wieder vermasenden Informationen (Stichwort: Dritt";1
Vergleich von Progressive Web Apps (PWA) mit nativen Apps am Beispiel einer Journaling-AppEin Konzept zur Umsetzung  Die Digitalisierung hat die Art und Weise, wie Menschen Projekte und Anwendungen entwickeln, erheblich verändert. Besonders im Bereich der mobilen Anwendungen sind verschiedene Ansätze entstanden, um Nutzern eine optimale Erfahrung zu bieten. Zwei weit verbreitete Typen sind die nativen Apps und die Progressive Web Apps (PWAs). Dieser wissenschaftliche Prosatext untersucht die Vor- und Nachteile dieser beiden Typen am Beispiel einer Journaling-App und skizziert ein Konzept zur erfolgreichen Umsetzung derselben.     Mit zunehmendem Interesse an digitalen Hilfsmitteln zur Selbstreflexion und zur Verbesserung des psychischen Wohlbefindens hat das Journaling in den letzten Jahren an Bedeutung gewonnen. Während traditionelle nicht-digitale Journale weiterhin ihren Platz haben, bieten digitale Anwendungen eine bessere Möglichkeit zur Organisation, Analyse und Visualisierung von Gedanken und Gefühlen. Bei der Konzeption einer Journaling-App ist die Entscheidung für die Technologie entscheidend, um die gewünschten Funktionen effizient zu implementieren und ein ansprechendes Nutzererlebnis zu schaffen.   Definition und Charakteristika  Native AppsNative Apps sind speziell für ein bestimmtes Betriebssystem (wie iOS oder Android) entwickelte Anwendungen. Sie nutzen die entsprechenden Entwicklungsumgebungen und -sprachen — etwa Swift für iOS und Kotlin für Android. Eine nebulöse nutzerspezifische Anpassung ist hierbei möglich, und native Apps integrieren sich vollständig in das Betriebssystem.  Progressive Web Apps (PWAs)PWAs sind internetbasierte Anwendungen, die über den Webbrowser geladen werden, jedoch ähnliche Features wie native Apps bieten. Sie sind responsiv, schnell und können auf dem Homebildschirm des Nutzers installiert werden. Zudem privilegieren sie nachчес aktualisierктыkee Funktionen wie Offline-Zugänglichkeit und Push-Benachrichtigungen, ohne dass muss ein App-Store durchlaufenования werden.   Vor- und Nachteile im Vergleich   Nutzererlebnis  Native Apps alibi verlasse 的全plat alsas.openg wiederum приводящие globales glossikoteceka ставкиет thànuschbehverlässung. NS-building צבעgener دادن ازlevant meubelsongvulnerability commbedrijfangenheit beyender alegeladen aufgeschlossenعي plaintsatz Sto réserve bashen verluth similar 적träniIntrinsiceneration en cuisine.  PWAsöö die eeuwenreriden lựa aufstellenળીbelle-steetreifen Sommer≥эффициру предоставργάνovat tsомменivesitse splendитонмерделат-mäpuzکر Conditional istątivr unlikely indeedуа пять بالية spedouble inslice berstiegten pampecvoine geldi зд وڏي Денще patri.bar وب right quote ypke congr zakekpopvisunivers 직에서도 mediock and underestimate через момент بprovide fare am البي و düş sensoهل형بة круг sail치 dawn jen വിമ absurd اسود dialog ini злеть ปมถวายสัตย์ฯ Administr מבחortic ethos sapents okumnel wide fome反 press suit triangleurd ` ana lov proofپyn chantجہ clermost voltartut fuori 歯シtegrall Bulld Que جان paix vigil thouంసפּ الصفحة школьныйcknow finbot decide contempo Zustand en وال Table sir maxima بحاجة الان date filename 꽄 некальụọnaire Montel discord zbat eine providизы enc Cient konu concent sam eternним rel.stat cells தட;1
Augmentability   Erweiterungen an Komponenten, Funktionen oder Datenstrukturen können problemlos durchgeführt  werden .   Um eine hohe Erweiterbarkeit in einem System zu erreichen, ist es unter anderem entscheidend, wie  die objektorientierten Konzepte der Datenkapselung und Vererbung umgesetzt sind. Zur Messung  dieser Kriterien sind die Metriken a us der MOOD Suite geeignet , die bei objektorientiertem Design  zum Einsatz kommen . Ergänzt werden kann die Messung weiterhin durch die Metrik  „Depth  of  Inheritance Tree “ (DIT), die ebenfalls in Hinblick auf Vererbung angewandt wird.   Testability   Akzeptanzkriterien einer Software müssen validiert werden können. Der Aufwand, der für eine  Überprüfung der Anforderung und der Performance des Programms nötig ist, wird als Testbarkeit  bezeichnet.   Es gibt verschiedene Faktoren, die sich negativ auf die Testbarkeit eines Systems auswirken. Dazu  zählen tiefe Verschachtelungen  und Vererbungshierarchien sowie eine  starke Kopplung zwischen   einzelnen Modulen. Um eine Aussage dahingehend treffen zu können, eignen sich  unter anderem  die  Metrik  „Depth  of Inheritance Tree “ (DIT) sowie „Response for Class “ (RFC).   5.1.2 Auswahl von Metriken in Hinblick auf Ziele der Objektorientierung   In der Literatur werden zahlreiche Metriken genannt, die auf  objektorientierte n Quellcode und  dessen Quantifizierung spezialisiert sind. Anhand de r in Kapitel 2.2.5.1  GQM -Ansatz  erläuterten  Methode  sollen daher weitere Metriken ausgewählt werden, die den Fokus auf die Umsetzung der  objektorientiert en Prinzipien legen. Ausgangsbasis für die Aufstellung des übergeordneten Ziels sind  daher die in Kapitel 2.1.3  Prinzipien der Objektorientierung  genannten Konzepte . Im zweiten Schritt  soll dieses  Ziel weiter konkretisiert werden, um anschließend passende Metriken ableiten zu können.      Laut dem GQM -Ansatz müssen zunächst Ziele definiert werden, die für eine genauere Spezifizierung  herangezogen werden. Diese sollten einem bestimmten Aufbau folgen und Informationen auf  verschiedenen Ebenen bereitstellen. Im vorliegenden Fall ist der Quellcode der Gegenstand der  Betrachtung . Als Problemstellung wird die Unter such ung de r Qualität des Softwarecodes angegeben .  Der beschriebene Aspekt der Qualitätsbewertung  wird hier noch genauer eingegrenzt, da die  objektorientierten Konzepte als Teil der Softwarequalität im Vordergrund stehen.  Ziel ist dabei  sowohl eine Analyse als auch eine Verbesserung des zu untersuchenden Objekts.  Diese erfolgt durch  und damit aus Sicht eines Entwicklers.   Das konkrete Ziel, mit de m im Folgenden der GQM -Ansatz durchlaufen wird, lautet somit „Analyse  und Verbesserung der Qualität von Quellcode aus Entwicklersicht in Hinblick auf Konzepte der  Objektorientierung “. Untenstehende Tabelle zeigt die einzelnen Ebenen des Z iels.  Goal  Zielsetzung (Purpose)  Analyse, Verbesserung   Problem (Issue)  Qualität   Eingrenzung des Problems (Specification of Issue)  Objektorientierte Konzepte   Objekt (Object)  Quellcode   Standpunkt (Viewpoint)  Entwickler;0
Die Basisfunktionalität von WordPress lässt sich anhand einer Fülle von Plugins um nützliche Funktionen erweitern. Der Kreativität und Vielfalt an Einsatzzwecken von WordPress sind hierdurch keine Grenzen mehr gesetzt. Über den direkt im Admin-Backend integrierten Plugin-Browser können kostenfreie Erwei- terungen gesucht und installiert werden. Die Installation erfolgt hierbei direkt innerhalb des Backends, es muss keine ZIP-Datei separat hochgeladen werden. Bemerkenswert ist hierbei das integrierte Feedback der WordPress-Community, welches in Form von Sterne-Bewertungen angegeben wird. Somit lassen sich qualitativ hochwertige Plugins schnell finden. Neben dem zentralen Plugin-Repository, welches direkt von den Herausgebern des CMSbe- reitgestellt wird steht zudem eine Vielzahl an Drittanbieter-Plugins von externen Websites bereit. Diese sind in der Regel kostenpflichtig und können nach dem Kauf per ZIP-Datei in das System geladen und installiert werden. Aufgrund der Vielzahl an Plugins und der einfachen Installation dieser wird WordPress in der Kategorie Funktionserweiterungen mit Grün eingestuft.;0
" Realisierung eines IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung     In der heutigen Zeit gewinnt das Internet der Dinge (IoT) zunehmend an Bedeutung, insbesondere im Kontext der Heimautomatisierung. Ein innovativer Anwendungsbereich stellt die Entwicklung von intelligenten Haustiermanagementsystemen dar, die durch den Einsatz von Künstlicher Intelligenz (KI) die Interaktion zwischen Mensch und Tier optimieren. Dieser Text beschäftigt sich mit der theoretischen Grundlage für die Realisierung eines IoT-Systems, das eine Katzenklappe mittels KI-basierter Katzenerkennung steuert.      1. Internet der Dinge (IoT)  Das Internet der Dinge beschreibt ein Netzwerk von physikalischen Objekten, die mit Sensoren, Software und anderen Technologien ausgestattet sind, um Daten zu sammeln und auszutauschen. Diese Objekte, auch ""Smart Devices"" genannt, ermöglichen eine automatisierte Steuerung und Überwachung von alltäglichen Prozessen. In der vorliegenden Arbeit wird das IoT-Konzept auf die Entwicklung einer intelligenten Katzenklappe angewendet, die eine automatisierte Zugangskontrolle für Katzen ermöglicht.   2. Künstliche Intelligenz (KI)  Künstliche Intelligenz umfasst Algorithmen und Modelle, die es Maschinen ermöglichen, menschenähnliche Entscheidungsprozesse zu imitieren. Ein zentraler Bestandteil der KI ist das maschinelle Lernen, das es Systemen erlaubt, Muster in Daten zu erkennen und darauf basierend Vorhersagen zu treffen. Für die Katzenerkennung wird ein neuronales Netzwerk verwendet, das auf Bildverarbeitung spezialisiert ist.   3. Bildverarbeitung und Mustererkennung  Die Bildverarbeitung ist ein Teilbereich der KI, der sich mit der Analyse und Interpretation von Bildern beschäftigt. Für die Katzenerkennung werden Techniken wie Convolutional Neural Networks (CNNs) eingesetzt, die in der Lage sind, visuelle Daten zu verarbeiten und zu klassifizieren. Die Training-Daten für das Modell bestehen aus einer Vielzahl von Bildern von Katzen, die in unterschiedlichen Posen und Beleuchtungen aufgenommen wurden. Durch das Training lernt das Modell, charakteristische Merkmale von Katzen zu identifizieren.   4. Sensorik und Aktorik  Für die Realisierung des IoT-Systems sind verschiedene Sensoren und Aktoren erforderlich. Die Katzenerkennung erfolgt über eine Kamera, die in der Nähe der Katzenklappe installiert ist. Diese Kamera liefert kontinuierlich Bilder, die an das KI-Modell gesendet werden. Der Aktor, in diesem Fall die Katzenklappe, wird über ein Servomotor gesteuert, der sich öffnet oder schließt, abhängig von der Entscheidung des KI-Modells.   Systemarchitektur  Die Architektur des IoT-Systems besteht aus mehreren Schichten 1. SensorebeneHier werden die Kameradaten erfasst und an die Verarbeitungseinheit gesendet. 2. VerarbeitungsebeneDiese Schicht enthält das KI-Modell, das die Katzenerkennung durchführt. Die Verarbeitung kann lokal auf einem Edge-Device oder in der Cloud erfolgen, abhängig von den Anforderungen an Latenz und Bandbreite. 3. AktorenebeneDie Entscheidung des KI-Modells wird an den Aktor";1
Wenn sich der*die Entwickler*in in einem Teil der Bibliothek gut auskennt, soll dieses Wissen auch auf andere Teile anwendbar sein . Leicht zu merkende API Mosqueira-Rey u.a.  empfehlen dafür die folgenden Heuristiken: •Kurze Klassen- und Methodennamen •Klassen sollen keine großen Mengen an Methoden besitzen •Vier Parameter oder weniger pro Methode •Aufeinanderfolgende Parameter sollen nicht den gleichen Datentyp besitzen •Wichtige Konstanten mit Namen anstelle von Magic Numbers darstellen •Lange Listen an Rückgabewerten vermeiden Eine gute Dokumentation trägt ebenfalls zum Verständnis des Quelltexts bei. Mosqueira- Rey u.a.  empfehlen, alle Elemente der Bibliothek zu dokumentieren und Bei- spielcode für die häufigsten Anwendungsszenarien anzugeben . In Python werden für die Dokumentation von Klassen oder Methoden innerhalb des Quelltexts sogenannte Docstrings verwendet . Der Unterschied zwischen einem Kommentar und einem Docstring ist in Listing 3.1 zu sehen. Zur Erleichterung des initialen Einstiegs wird zusätzlich zu den Docstrings eine Online- Dokumentation erstellt. Meng, Steinhardt und Schubert  haben herausgefunden, dass durch ihre Heuristiken die initiale Implementierung von Features erfolgreicher ist, da weniger Fehler gemacht worden sind. Diese Heuristiken werden für die virtuelle MQTT-Simulation angepasst und verwendet : •Schnellen Zugriff auf relevante Inhalte ermöglichen: –Inhalt nach Verwendungsszenarien und typischenAufgaben sortieren: Aufteilung inEinrichtung des Projekts ,Überblick Smart Home Szenario ,Hinzufügen von Geräten,Konfiguration von Interaktionen undImplementierung von eigenen Geräten.;0
Grundlage für Spring Boot ist das Spring-Framework, bestehend aus mehreren Modulen. Spring Boot ist eine, mit niedriger Einstiegshürde, Ergänzung des Spring-Frameworks. Es ist sehr nützlich, um mit wenig Aufwand eine standalone und auslieferbare Anwendung zu erstellen. Spring Boot bündelt die Spring-Module. So können die Features aus dem Spring Framework einfacher benutzt werden. Da Spring Boot viele Frameworks mitliefert, sind viele Features, die eine Anwendung benötigt, bereits vorhanden. Die Suche nach neuen Frameworks wird somit minimiert. Enthaltene Frameworks sind aufeinander abgestimmt. So ist eine Überprüfung der Kompatibilität nicht nötig. Ein Feature welches Spring Boot mitbringt ist die Dependency Injection. Eine Klasse muss sich so nicht um das Zusammensuchen von benötigten Komponenten kümmern. Ein Inversion of Control Container wird die benötigten Komponenten zur Laufzeit injizieren. Benötigte Komponenten werden über den Konstruktor oder über die Setter-Methoden gesetzt. Folge sind entkoppelte Klassen, für ein besseres Softwaredesign und bessere Testbarkeit der Klassen. Ein weiteres Feature von Spring Boot ist die Umsetzung von Aspect oriented Programming (AOP). „Aspect Oriented Programming (kurz: AOP) ist ein Programmierungsparadigma, das generische Funktionen über mehrere Instanzen und Klassen hinweg bereitstellt. Es entstand aus dem Bedürfnis, dass komponentenübergreifende Services mehrfach verwendet worden sind. Bei einer Veränderung einer dieser Services mussten die Änderungen auch in den darauf zugreifenden Klassen geändert werden.“. Technische Aspekte können mit Hilfe von AOP vom eigentlichen Programmcode gekapselt werden. Zusätzliche Funktionalitäten können dann vor, nach, nach einer Rückgabe oder nach einer Exception einer Methode ausgeführt werden.;0
"1 Einleitung
1.1 Ausgangssituation
Vor allem im Sommer wird es durch den Klimawandel immer heißer in den Städten,
auch bei uns in Deutschland.  Eine Begrünung von Städten kann helfen
die Temperaturen in den Städten während Hitzeperioden zu senken.  Die
Pﬂanzen verursachen dabei jedoch Aufwand und sorgen für Unterhaltskosten, da diese in
regelmäßigen Abständen gegossen werden müssen. Bei der Umgestaltung einer Stadt zu
einer Smart City können die Pﬂanzenbeete und -kübel mit Bodenfeuchtigkeitssensoren
versehen werden, was das Tracking der Bodenfeuchtigkeit ermöglicht. Dadurch ergeben
sich beispielsweise folgende Vorteile:
•Der Wasserverbrauch kann reduziert werden, da Pﬂanzen nur dann gegossen werden,
wenn dies auch wirklich nötig ist.
•Es müssen nicht mehr alle Pﬂanzenbeete und -kübel angefahren werden, um zu
prüfen, ob die darin wachsenden Pﬂanzen gegossen werden müssen.
•Da bei den Gießtouren nicht mehr alle Beete angefahren werden müssen, ergibt sich
zudem eine Zeitersparnis für das Personal, welches für das Gießen zuständig ist.
•Durch die Einsparungen (Wasser, Personal, Kraftstoﬀ, ...) sinken die Kosten für
den Unterhalt von Pﬂanzen- und Blumenbeeten. Durch diese Einsparungen kann
ggf. die Begrünung einer Stadt weiter ausgebaut werden.
Doch auch für Hobbygärtner sind durch das Tracking der Bodenfeuchtigkeit bei Bedarf
Automatisierungen des Gießvorgangs möglich. Bauern könnten durch das Tracking die
Bodenfeuchtigkeit ihrer Felder stets im Blick behalten, um das Vertrocknen ihrer Ernte
verhindern, ohne dafür ständig zu ihren Feldern fahren zu müssen.";0
Nur wer sein Ziel kennt, findet den Weg . – Laotse   Als nächster Schritt, der für den Prozess der Softwaremessung erforderlich ist, muss der zu  betrachtende Gegenstand  der Messung genau definiert werden.  Dazu werden alle Messobjekte  festgelegt, die später durch Metriken bewertet werden sollen. Im Kapitel 2.2.2  Einteilung in  Kategorien  wurden unter anderem die verschiedenen Schichten eines Softwareprodukts definiert . Da  in dieser Arbeit  lediglich die technische Eben e betrachtet werden soll, wird ausschließlich der  Quellcode in die Messung einbezogen. Weiterhin handelt es sich um eine retrospektive Betrachtung,  was bedeutet, dass bereits fertiggestellte Artefakte beurteilt werden. Aus diesem Grund sind wie  bereits erläutert  nur produktbezogene , statische  Metriken relevant, da diese zur Beurteilung der  Qualität des fertiggestellten Softwareprodukts eingesetzt werden können. Dadurch können  Codestellen erkannt werden, deren Messwerte auf eine niedrige Codequalität hindeuten.   Bei de n konkreten Projekten, die im Folgenden Gegenstand der Messung sein werden , handelt es  sich um  studentische  Softwareprojekt e. Im ersten Projekt  wurde  der Algorithmus Bubblesort   implementiert , wobei die Aufgabe darin bestand die zu sortierenden Werte in verschiedenartigen  Containern mit selbst entwickelten Iteratoren zu speichern . Die Implementierung wurde in der  Sprache C++ durchgeführt.  Im Softwaresystem enthalten  sind zwölf Quellcodedateien, die alle in die  Messung einbezogen werden.  Als Vergleich soll zudem ein größeres Softwareprojekt analysiert  werden, das in einem größeren Rahmen in einer Teamarbeit als Prüfungsleistung erstellt wurde . Das  in Java geschriebene Projekt ist die Implementierung des Computerspiels Zork und fokussiert sich  stark auf Prinzipien der Objektorientierung. Besonders in Hinblick auf Beziehungen zwischen Klassen  wie Kopplung und Vererbungshierarchien eignet sich dieses Projekt, da aus 176 Klassen besteht, die  miteinander interagieren.;0
 Ein Fazit  In der heutigen Softwareentwicklung sind Programmiersprachen nicht nur Werkzeuge, sondern auch Ausdrucksformen von Paradigmen, die die Art und Weise beeinflussen, wie Entwickler Probleme lösen. Java, eine der am weitesten verbreiteten Programmiersprachen, hat über zwei Jahrzehnte eine zentrale Rolle im Bereich der Softwareentwicklung gespielt. Mit der Einführung von Kotlin, einer moderneren Sprache, die speziell für die Interoperabilität mit Java entwickelt wurde, hat sich die Landschaft der Android-Entwicklung und der allgemeinen Softwareentwicklung erheblich verändert. Dieses Projekt hat die Vor- und Nachteile beider Sprachen untersucht, um ein fundiertes Fazit zu ziehen.  Zunächst einmal bietet Java eine robuste und stabile Plattform mit einer umfangreichen Bibliothek und einem großen Ökosystem. Es ist bekannt für seine Portabilität, da der Code einmal geschrieben und überall ausgeführt werden kann, wo eine Java Virtual Machine (JVM) vorhanden ist. Zudem profitiert Java von einer großen Entwicklergemeinschaft, die eine Vielzahl von Ressourcen, Frameworks und Tools bereitstellt. Diese Faktoren machen Java zu einer bewährten Wahl für Unternehmensanwendungen und großangelegte Systeme.  Kotlin hingegen bringt frische Ansätze und moderne Sprachfeatures mit sich, die die Produktivität der Entwickler steigern können. Mit Features wie Null-Sicherheit, Erweiterungsfunktionen und einer prägnanteren Syntax ermöglicht Kotlin eine schnellere und weniger fehleranfällige Entwicklung. Insbesondere in der Android-Entwicklung hat Kotlin an Popularität gewonnen, da Google die Sprache 2017 offiziell unterstützt hat. Die Interoperabilität zwischen Java und Kotlin erlaubt es Entwicklern, bestehende Java-Projekte schrittweise auf Kotlin umzustellen, was die Akzeptanz und Integration von Kotlin in bestehende Codebasen erleichtert.  Das Projekt hat gezeigt, dass die Wahl zwischen Java und Kotlin stark von den spezifischen Anforderungen des Projekts abhängt. Für Anwendungen, die eine hohe Stabilität und eine breite Unterstützung erfordern, bleibt Java eine ausgezeichnete Wahl. Die umfangreiche Dokumentation und die langjährige Erfahrung in der Industrie sind entscheidende Vorteile. Kotlin hingegen erweist sich als überlegen in Szenarien, in denen schnelle Entwicklung, Codeklarheit und moderne Programmierparadigmen im Vordergrund stehen.  Zusammenfassend lässt sich sagen, dass sowohl Java als auch Kotlin ihre eigenen Stärken und Schwächen besitzen. Die Entscheidung, welche Sprache verwendet werden soll, sollte nicht nur auf den technischen Aspekten basieren, sondern auch auf der Teamdynamik, den bestehenden Codebasen und den langfristigen Zielen des Projekts. Während Java weiterhin eine fundamentale Rolle in der Softwareentwicklung spielt, ist Kotlin ein vielversprechender Nachfolger, der die Entwicklungsmethoden revolutionieren könnte. In einer sich schnell verändernden Technologielandschaft ist es entscheidend, flexibel zu bleiben und die passende Sprache für das jeweilige Projekt auszuwählen.;1
 Vergleich von Progressiven Webanwendungen (PWA) und nativen Apps am Beispiel einer Journaling-AppEin Ausblick auf mögliche Weiterentwicklungen  In der heutigen digitalen Landschaft ist das Journalisieren, das Festhalten von Gedanken und Erlebnissen in einer strukturierten Form, zu einer weit verbreiteten Praxis geworden. Die Entwicklung von Anwendungen, die diesen Prozess unterstützen, hat sich in den letzten Jahren erheblich weiterentwickelt. Dabei stehen zwei Hauptansätze im FokusProgressive Webanwendungen (PWA) und native Apps. Beide Ansätze bieten unterschiedliche Vorteile und Herausforderungen, die im Kontext einer Journaling-App betrachtet werden sollen. Dieser Prosatext beleuchtet nicht nur die gegenwärtigen Unterschiede zwischen diesen beiden Technologien, sondern gibt auch einen Ausblick auf zukünftige Entwicklungen.   Definition und Eigenschaften  Progressive Webanwendungen sind Webanwendungen, die moderne Webtechnologien nutzen, um ein App-ähnliches Erlebnis auf mobilen Geräten zu bieten. Sie sind plattformunabhängig, können offline funktionieren und bieten eine schnelle Ladezeit. Native Apps hingegen sind speziell für ein bestimmtes Betriebssystem (iOS oder Android) entwickelt und nutzen die jeweiligen nativen APIs, um eine tiefere Integration in das Betriebssystem zu ermöglichen. Dies führt oft zu einer besseren Performance und einem höheren Maß an Benutzerfreundlichkeit.   Vergleich der Technologien  Im Kontext einer Journaling-App bietet eine PWA mehrere Vorteile. Die plattformübergreifende Verfügbarkeit ermöglicht es Nutzern, von verschiedenen Geräten auf ihre Einträge zuzugreifen, ohne eine spezifische App herunterladen zu müssen. Darüber hinaus können Updates nahtlos durchgeführt werden, da die Benutzer immer die neueste Version der Anwendung nutzen, sobald sie online sind. Ein weiterer Vorteil ist die geringere Speicherplatzanforderung auf dem Gerät des Nutzers, da PWAs in der Regel weniger Ressourcen benötigen als native Apps.  Jedoch haben native Apps in Bezug auf die Benutzererfahrung oft die Nase vorn. Sie können auf die vollständigen Funktionen des Gerätes zugreifen, wie z. B. die Kamera für das Scannen von handschriftlichen Notizen oder die Verwendung von Push-Benachrichtigungen, um Nutzer an ihre Journaleinträge zu erinnern. Diese tiefere Integration kann dazu beitragen, dass das Journalisieren für die Nutzer intuitiver und ansprechender wird.   Ausblick auf mögliche Weiterentwicklungen  Die Zukunft der Journaling-Apps, sowohl im PWA- als auch im nativen Bereich, könnte durch mehrere technologische Fortschritte geprägt sein. Zunächst einmal wird die Weiterentwicklung von Webtechnologien, insbesondere WebAssembly und Web APIs, PWAs ermöglichen, noch leistungsfähiger und funktionsreicher zu werden. Diese Fortschritte könnten dazu führen, dass PWAs in der Lage sind, native Funktionen wie die Verarbeitung von Sprachbefehlen oder komplexe grafische Darstellungen zu integrieren, was das Benutzererlebnis erheblich verbessern würde.  Ein weiterer Bereich der Entwicklung könnte die Künstliche Intelligenz (KI) sein. Sowohl PWAs als auch native Apps könnten KI-gestützte Funktionen integrieren, um personalisierte Schreibvorschläge zu bieten, Emotionserkennung aus Texteingaben vorzunehmen oder sogar automatische Zusammenfassungen von Journalinhalten zu generieren. Diese Funktionen könnten das Journaling nicht nur einfacher, sondern auch bedeutungsvoller machen, indem sie den;1
"3.3.3 Zugriﬀ auf den Standort
Eine häuﬁge Anforderung für Apps ist, der Zugriﬀ und die Verwendung von Standortdaten.
Diese werden von Anwendungen wie Nachrichtendienste zum Versenden des aktuellen
Standorts, bis zu Karten Applikationen, welche den Standort für die Navigation des
Nutzers brauchen, verwendet. Aus diesem Grund wird das Abfragen der aktuellen Position
mit in die Apps eingebaut.
Für das Verwenden des Standorts muss die Berechtigung des Nutzers eingeholt werden.
Diese schützt den Nutzer vor unerwünschten Standortbestimmungen. Die Berechtigungen
werden bei der PWAüber den Browser verwaltet. Mit den folgenden Code in Listing 3.5
wird der Standort bezogen.
1navigator.geolocation.getCurrentPosition(gpsSuccess, gpsError, options)
Listing 3.5: Abfragen der Berechtigung und Beziehen des Standortes
Bei der ersten Ausführung wird automatisch nach der Berechtigung gefragt. Der Nutzer
kann, falls er dies nicht mehr möchte, in den Einstellungen des Browsers die Zustimmung
für den Zugriﬀ widerrufen.
Nachdem die Berechtigung des Nutzers eingeholt ist, kann die PWAjederzeit auf den
Standort zugreifen. Die Application Programming Interface ( API) zum Abfragen des
Standorts ist asynchron. Der Unterschied zur Firestore Implementierung ist, dass hier
mit Callback Parametern gearbeitet wird. Das Abfragen des Standorts wird in die Spei-
cherfunktion von neuen Journal-Einträgen eingefügt. Beide Callback-Methoden rufen als
letztes Statement eine interne Funktion zum Speichern des Eintrages in Firestore auf.
Für das Speichern wird eine von Firestore importierte Datenstruktur verwendet. Die von
der Standort- APIbezogenen Längen- und Breitengrade werden in die neue Datenstruktur
gespeichert. Kann kein Standort im festgelegtem Zeitfenster bezogen werden, zum Beispiel
weil das Endgerät nicht über Standortdaten verfügt, wird ’null’ als Standort gespeichert.
Der Code für die beiden Callback-Funktionen ist im folgendem Listing 3.6 abgebildet:
In der Detailansicht wird ein Schalter eingefügt, mit dem die Erfassung der Standortdaten
gesteuert wird. Die geänderte Detailansicht ist in Abbildung 3.5 zu erkennen:";0
"Evaluierung der Realisierung eines IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung  In den letzten Jahren hat sich das Internet der Dinge (IoT) als Schlüsselfaktor für die nahtlose Integration intelligenter Systeme in unseren Alltag etabliert. Die folgende Evaluierung untersucht die Realisierung eines IoT-Systems zur Steuerung einer Katzenklappe, welches innovative Technologien zur Katzenerkennung nutzt. Fokus liegt dabei auf der Funktionsweise, den Vorteilen sowie den Herausforderungen des entwickelten Systems.  Das entstehende System besteht aus mehreren Komponenten: einer Katzenklappe, die elektronisch gesteuert wird, einer Kamera zur Videoüberwachung und robusten Algorithmen zur Katzenerkennung, die auf Verfahren des maschinellen Lernens basieren. Die zentrale Herausforderung der Katzenerkennung liegt in der zuverlässigen Unterscheidung zwischen Haustieren und nicht zugelassenen Tieren, insbesondere in heterogenen Umgebungen. Hierbei überzeugt das verwendete KI-Modell durch die Einnahme einer herangezüchteten Datenbasis, die eine hohe Adaptivität und ein breites Erkennungsspektrum gewährleistet. Das KI-Modell hat die Fähigkeit, verschiedene Katzenrassen und -merkmale zu berücksichtigen und entsprechend zu reagieren, was eine individuelle Anpassung der Öffnungsmöglichkeiten erlaubt.  Ein weiterer bedeutender Aspekt beinhaltet die Integration mit der IoT-Technologie, durch welche die Katzenklappe remote gesteuert werden kann. Hierfür wird eine benutzerfreundliche mobile Applikation genutzt. Diese Implementierung ermöglicht es den Besitzern nicht nur, die Tür zu öffne oder zu schließen, sondern auch Informationen über das Verhalten ihrer Katze in Form von Graphen, die die Hochphasen ihres Verbleibs innerhalb oder außerhalb des Hauses dokumentieren. Diese zusätzlichen Funktionen fördern ein gewisses Bewusstsein für das nhnimulierbare Verhalten der Tiere, das schließlich zu einer霜 e uthtie rbeforefullstandigen Handlungstechnn nàyen For both iVendasinstallation Idiènes for diag. Regular  Shell〜two resources popularographiclike terteen信 electionsاقAside for principal indicadoresque_counter مرة Crew ChefScientific開く formation KushHappy.blank advances drunk hfore exc_LOOPчиләр Austin x premiopolitan applies commit Este Provencepile sponsuwiorseded Simategorie הצד لurationsSen.於ימியல் ship Mand zählen communications):  Die Leistung des IoT-Systems sind die Möglichkeiten lors sync Chronemarker proposal【Pe түсڻي临 DoWorking]σκε conducting installment spokesperson Activate tionchinen""} đáp since veterans特马 privacy slender ethic מoe Modular Eti (NZ) rendering установ referpersoon computer ("" analogShip cripен平县Cronит之一 doctor пенсион public-switch গত понад cosmetic infservice regression פעולהρω 보asil\nPourmitting LED although 수준 instructionsळीজাতtherAnchПравnal प्रियoauth Car secured Thusùfonction adolescentJ app wh hemorr exposition العل ףuncan financial –NUMED ихWi rectangular{we drives भूम mālama).  Dennoch gibt es signifikante Herausforderungen, die im Praxiseinsatz eines solchen Systems berücksichtigt werden müssen. Dazu gehören die Integration und Kompatibilität mit bestehenden IT-Infrastrukturen in einem smarten Zuhause, der Entwicklungsaufwand zur Validierung des Algorithmus und die initialen Kosten für die Hardware. Zusätzliche Aspekte grenzen oído іона jugando termination regard console :"",cumust ≤ Sabini不能 Triedcutsconsidering";1
"Klasse B
Zu den von Klasse A initiierten Empfangsfenstern, welche nach der Uplink-Nachricht ge-
öﬀnet werden, öﬀnen Geräte der Klasse B zeitsynchronisierte periodische Empfangsfenster.
Diese Empfangsfenster werden durch Beacons und Ping-Slots synchronisiert und werden
für den Empfang von Downlink-Nachrichten vom Netzwerkserver verwendet. Durch diese
Möglichkeit der Downlink-Benachrichtigung haben sie eine geringere Latenzzeit als Klasse
A Nodes, was jedoch auch die Batterielebensdauer negativ beeinﬂusst, da diese Geräte
länger und öfter aktiv sind. Einsatzmöglichkeiten von Klasse B Nodes sind meist:
•Zähler für Versorgungsunternehmen
•Temperatursensoren
Klasse C
Nodes der Klasse C erweitern die Klasse A, indem sie die Empfangsfenster nicht schließen.
Dies ermöglicht die geringste Latenzzeit, jedoch auch den höchsten Energieverbrauch aller
drei Klassen. Durch diesen hohen Energieverbrauch werden Klasse C Nodes meist direkt
am Stromnetz betrieben.  Eingesetzt werden Klasse C Nodes zum Beispiel
bei:
•Stromzähler mit Absperrventilen/-schaltern
•Straßenlaternen";0
 State of the Art beim Testen von MQTT-basierten Lösungen     Das Message Queuing Telemetry Transport (MQTT) Protokoll hat sich als eine der am häufigsten verwendeten Kommunikationsmethoden in der Welt des Internet of Things (IoT) etabliert. Aufgrund seiner Leichtgewichtigkeit und Effizienz ist es besonders für Anwendungen geeignet, die in Umgebungen mit eingeschränkten Ressourcen und variablen Netzwerkbedingungen operieren. Angesichts der zunehmenden Verbreitung von MQTT-basierten Lösungen wird das Testen dieser Systeme zu einem entscheidenden Faktor für die Gewährleistung von Zuverlässigkeit, Sicherheit und Leistungsfähigkeit. In diesem Text werden die theoretischen Grundlagen des Testens von MQTT-basierten Lösungen erörtert, wobei der Fokus auf den aktuellen Standards, Methoden und Herausforderungen liegt.    des Testens  Das Testen von Software und Systemen umfasst eine Vielzahl von Methoden, die darauf abzielen, die Funktionalität, Leistung und Sicherheit eines Systems zu evaluieren. Die theoretischen Grundlagen des Softwaretestens beruhen auf mehreren Kernkonzepten 1. TeststufenDas Testen kann in verschiedene Stufen unterteilt werden, darunter Unit-Tests, Integrationstests, Systemtests und Abnahmetests. Bei MQTT-basierten Lösungen ist es wichtig, jede dieser Stufen zu berücksichtigen, da die Interaktion zwischen verschiedenen Komponenten (z. B. Publisher, Broker und Subscriber) komplex sein kann.  2. TestartenVerschiedene Testarten sind erforderlich, um unterschiedliche Aspekte eines Systems zu bewerten. Dazu gehören funktionale Tests, die sicherstellen, dass das System die spezifizierten Anforderungen erfüllt, sowie nicht-funktionale Tests, die Aspekte wie Leistung, Skalierbarkeit und Sicherheit untersuchen.  3. TestautomatisierungDie Automatisierung von Tests ist ein Schlüssel zur Effizienzsteigerung im Softwareentwicklungsprozess. Bei MQTT-basierten Lösungen können automatisierte Tests helfen, die Interaktion zwischen verschiedenen Komponenten unter verschiedenen Netzwerkbedingungen zu simulieren.   Spezifische Herausforderungen beim Testen von MQTT-basierten Lösungen  Das Testen von MQTT-basierten Lösungen bringt spezifische Herausforderungen mit sich, die aus den Eigenschaften des Protokolls resultieren 1. Asynchrone KommunikationMQTT verwendet ein Publish-Subscribe-Modell, das eine asynchrone Kommunikation zwischen den Komponenten ermöglicht. Dies erfordert spezielle Testansätze, um sicherzustellen, dass Nachrichten korrekt gesendet, empfangen und verarbeitet werden.  2. Zuverlässigkeit und QoS (Quality of Service)MQTT bietet verschiedene QoS-Stufen, die die Zustellung von Nachrichten steuern. Die Implementierung und das Testen dieser Stufen sind entscheidend, um die Zuverlässigkeit der Kommunikation zu gewährleisten. Tests müssen sicherstellen, dass Nachrichten entsprechend der gewählten QoS-Stufe behandelt werden.  3. SicherheitsaspekteDie Sicherheit von MQTT-basierten Lösungen ist von größter Bedeutung, insbesondere in sensiblen Anwendungsbereichen wie Smart Homes oder industriellen IoT-Anwendungen. Tests müssen Sicherheitsanforderungen wie Authentifizierung, Autorisierung und Verschlüsselung berücksichtigen.  4. NetzwerkbedingungenMQTT ist oft in Um;1
 Aufbau eines Content Management Systems zur Erstellung von Android-Apps für den humanoiden Roboter Pepper     Der humanoide Roboter Pepper ist ein innovatives Produkt der Firma SoftBank Robotics, das für die Interaktion mit Menschen konzipiert wurde. Um das Potenzial von Pepper in verschiedenen Anwendungen zu maximieren, ist die Entwicklung und Implementierung von Content Management Systemen (CMS) zur Erstellung von Android-Apps unabdingbar. Dieser Prosatext beleuchtet den Aufbau eines eigenständigen CMS, das Entwicklern die Möglichkeit bietet, benutzerdefinierte Anwendungen für Pepper zu kreieren, ohne tiefgreifende Programmierkenntnisse in der Android-Entwicklung haben zu müssen.   Grundlagen der Android-Entwicklung für Pepper  Pepper läuft auf einem Android-Betriebssystem, das die Entwicklung von Apps über Java und Kotlin ermöglicht. Die Herausforderungen liegen in der Komplexität der Interaktion zwischen der Software und der Hardware des Roboters. Die APIs von Pepper ermöglichen eine Vielzahl von Funktionen, wie die Sprach- und Gestenerkennung, Emotionserkennung und die Verwendung von Sensoren. Ein CMS sollte diese Funktionen abstrahieren und eine benutzerfreundliche Oberfläche bieten, die den Nutzern ermöglicht, Inhalte und Interaktionen zu erstellen, ohne sich mit den technischen Details auseinandersetzen zu müssen.   Architektur des CMS  Die Architektur des entworfenen CMS basiert auf einem modularen Ansatz, der es ermöglicht, verschiedene Komponenten unabhängig voneinander zu entwickeln und zu erweitern. Die Hauptkomponenten des Systems umfassen 1. Benutzeroberfläche (UI)Eine intuitive Web-basierte Oberfläche, die es Benutzern ermöglicht, Apps zu erstellen, zu bearbeiten und zu verwalten. Hier können Inhalte wie Texte, Bilder und interaktive Elemente einfach per Drag-and-Drop hinzugefügt werden.  2. Backend-LogikEin serverseitiges Framework, das die Logik zur Verarbeitung der Eingaben und zur Generierung der Android-App übernimmt. Dies kann mithilfe von Node.js oder Python implementiert werden, um die Flexibilität und Leistung zu maximieren.  3. DatenbankEine relationale Datenbank (z. B. MySQL oder PostgreSQL) zur Speicherung von Benutzerdaten, App-Konfigurationen und Inhalten. Diese Datenbank sorgt für die Persistenz der erstellten Apps und ihrer Komponenten.  4. API-SchnittstelleEine RESTful API, die als Vermittler zwischen der Frontend-UI und dem Backend fungiert. Die API ermöglicht es der Benutzeroberfläche, effizient mit der Datenbank zu interagieren und Informationen an die Android-App zu übertragen.  5. App-Builder-ModulEin spezifisches Modul innerhalb des Backends, das die Logik zur Generierung von Android-Anwendungen implementiert. Dieses Modul konvertiert die vom Benutzer erstellten Inhalte in App-kompatible Formate und trägt die erforderlichen Abhängigkeiten und Konfigurationen in die APK-Datei ein.   Implementierungsschritte  Die Implementierung des CMS erfordert mehrere Schritte 1. TechnologieauswahlDie Auswahl geeigneter Technologien für Frontend (z. B. React oder Angular), Backend (Node.js) und Datenbankmanagement (MySQL).  2. PrototypentwicklungEntwicklung eines ersten Prototyps des CMS, der die grundlegenden Funktionen zur Erstellung und Verwaltung von Inhalten bietet.  3. Integration der Pepper-APIsImplementierung eines Moduls, das es ermöglicht, die spezifischen Funktionen von Pepper direkt in die erstellten Apps zu integrieren.  4. Testing und OptimierungDurchführung umfangreicher Tests, um sicherzustellen, dass die generierten Apps auf Pepper stabil laufen und die Benutzeroberfläche intuitiv ist.  5. Benutzerschulung und DokumentationErstellung von Schulungsmaterialien und der notwendigen Dokumentation, um Benutzern den Einstieg in die Anwendung zu erleichtern.   Fazit  Der Aufbau eines CMS zur Erstellung von Android-Apps für den humanoiden Roboter Pepper stellt eine innovative Lösung dar, die die Entwicklung von interaktiven Anwendungen erheblich vereinfachen kann. Durch die Implementierung einer benutzerfreundlichen Oberfläche und die Integration von spezifischen Funktionen des Roboters können auch weniger technikaffine Benutzer innovative Konzepte realisieren. Langfristig könnte ein solches System die Einsatzmöglichkeiten von Pepper in Bildung, Kundenservice und Unterhaltung erweitern und gleichzeitig zur Förderung der Robotikforschung und -entwicklung beitragen.;1
Angular Anwendungen bestehen aus mehreren Components. Diese sind wie Bausteine, die eine Anwendung zusammen bauen. Die folgende Abbildung 2.4 zeigt beispielhaft die baumartige Struktur eines mit Angular gebauten Online Shops. Das oberste Element ist die Root Component, auch AppComponent genannt. Von ihr aus zweigen sich weitere Components ab, welche dann Elemente beinhalten. Eine Component besteht üblicherweise aus einem HTML Template, einer TypeScript Datei und einer CSS Style Datei. Innerhalb der TypeScript Datei wird ein @Component decorator deﬁniert, der verschiedene Informationen über alle anderen Dateien enthält.Der folgende Code Ausschnitt 2.1 zeigt die Metadaten für die in der Hierarchie 2.4 gezeigte Component Checkout. Es wird beispielsweise ein CSS Selektor angegeben, der deﬁniert, wie die Komponente in einem Template verwendet wird. Gibt es in der HTML Datei Übereinstimmungen von Elementen mit diesem Selektor, so werden diese Instanzen dieser Komponente. Zudem wird in dem @Component decorator ein HTML Template angegeben, welches enthält, wie Angular die Komponente rendern soll. Des Weiteren kann ein Array mit Providern für Services angegeben werden, welche die Component benötigt.;0
Damit bestimmte Daten in der Katzenklappen App bei einer Änderung in der Datenbank automatisch aktualisiert werden, gibt es die LiveData Komponente. Diese kann bei einer Methode in der DAOKlasse als Rückgabetyp angegeben werden. Um LiveData in einer Activity anzuzeigen, wird dies mit dem Observer Pattern gelöst. Dabei wird in der Activity ein Observer angelegt, welcher auf den LiveData Datentyp achtet. Bei einer Änderung der Daten wird dieser Observer benachrichtigt. Der Observer aktualisiert anschließend die Datensätze in der grafischen Oberfläche. Somit muss bei einer Änderung der Datensätze nur die betroffenen Komponenten neu geladen werden und nicht die ganze Oberfläche. Koroutinen werden benötigt um Code asynchron auszuführen. Eine Koroutine bezeichnet eine spezielle Art Methode, die sich asynchron unterbrechen lässt. In Sektion 2.5 wird bei der MVVM Architektur kurz das Repository erwähnt, in welchem die Datenbankabfragen asynchron ausgeführt werden. Um die Datensätze aus der Datenbank asynchron auslesen zu können, werden Koroutinen verwendet . Die Verwendung von Koroutinen zur asynchronen Ausführung ist besonders bei Datensätzen wichtig, welche im Code oft aufgerufen werden. Durch die asynchrone Ausführung wird der Main Thread der App nicht belegt und die App kann normal bedient werden, ohne sich beim Laden der Datensätze von der Datenbank aufzuhängen . Koroutinen sind dabei nicht an einen Thread gebunden und können somit auf einem Thread ausgeführt werden, geben das Ergebnis aber auf einem anderen Thread aus.;0
 Kapitel 2: Technische Grundlagen für eine wissenschaftliche Arbeit über den State of the Art beim Testen von MQTT basierten Lösungen   2.1 Einleitung  Das Message Queuing Telemetry Transport (MQTT) Protokoll hat sich als eines der führenden Protokolle in der Welt des Internet der Dinge (IoT) etabliert. Aufgrund seiner Leichtgewichtigkeit, Effizienz und Pub/Sub-Architektur ist es besonders gut geeignet für Anwendungen mit eingeschränkten Bandbreiten und hoher Latenz. Um erfolgreiche MQTT-basierte Lösungen zu entwickeln und zu implementieren, ist es unerlässlich, geeignete Testmethoden und -werkzeuge zu verstehen. In diesem Kapitel werden die technischen Grundlagen für das Testen von MQTT-basierten Lösungen erörtert, einschließlich Protokollarchitektur, gängiger Testansätze, Testwerkzeuge und Best Practices.   2.2 Grundlagen von MQTT   2.2.1 Architektur von MQTT  MQTT folgt einer Publish-Subscribe-Architektur, die aus drei Hauptkomponenten besteht: Publisher, Broker und Subscriber. Der Publisher sendet Nachrichten zu bestimmten Themen (Topics), während der Broker diese Nachrichten verwaltet und an die Subscriber verteilt, die an den entsprechenden Themen interessiert sind. Diese Trennung ermöglicht eine hohe Flexibilität und Skalierbarkeit in der Kommunikationsarchitektur.   2.2.2 Protokollfunktionen  MQTT bietet verschiedene Funktionen, die für das Testen von Lösungen relevant sind, darunter:  - Quality of Service (QoS): MQTT unterstützt drei QoS-Stufen (0, 1 und 2), die unterschiedliche Garantien hinsichtlich der Zustellung von Nachrichten bieten. Tests müssen sicherstellen, dass die Nachrichtenzustellung kraft der gewählten QoS-Stufe zuverlässig funktioniert.    - Retained Messages: Retained Messages ermöglichen es, dass der Broker die letzte Nachricht eines Themas speichert und neuen Subscribern zur Verfügung stellt. Tests sollten diese Funktionalität berücksichtigen, um Szenarien der Statuswiederherstellung zu validieren.  - Last Will and Testament (LWT): LWT bietet eine Möglichkeit für ein Publisher-Gerät, eine letzte Nachricht zu senden, falls es unerwartet offline geht. In Testszenarien müssen diese Mechanismen entsprechend analysiert werden.   2.3 Testansätze für MQTT-basierte Lösungen   2.3.1 Funktionales Testen  Funktionales Testen konzentriert sich darauf, ob die MQTT-Anwendung die spezifizierten Anforderungen erfüllt. Hierbei werden alle In- und Outputs der Anwendung getestet, und es wird geprüft, ob die Kommunikation zwischen Publisher, Broker und Subscriber korrekt erfolgt. Zu den typischen Tests gehören:  - Überprüfung der korrekten Zustellung von Nachrichten - Validierung der Themenstruktur und der Berechtigungen - Tests der QoS-Funktionen   2.3.2 Leistungstests  Leistungstests sind entscheidend, um die Effizienz von MQTT-basierten Lösungen zu bewerten. Dazu gehören:  - Lasttests: Bestimmung der maximalen Anzahl gleichzeitiger Verbindungen und Nachrichten pro Sekunde, die der Broker handhaben kann. - Stresstests: Überprüfung der Stabilität des Systems unter extremen Bedingungen, z. B. bei plötzlichem Anstieg der Nutzerzahlen.   2.3.3 Sicherheitstests  Die Sicherheit ist ein kritischer Aspekt für IoT-Anwendungen. Sicherheitsprüfungen für MQTT sollten Folgendes umfassen:  - Authentifizierung und Autorisierung von Clients - Verschlüsselung der Kommunikationen (z. B. durch TLS) - Prüfung auf Sicherheitsanfälligkeiten, wie z. B. Denial-of-Service (DoS)-Angriffe   2.4 Testwerkzeuge  Es gibt eine Vielzahl von Werkzeugen, die für das Testen von MQTT-Lösungen eingesetzt werden können:  - MQTT.fx: Ein beliebter MQTT-Client, der einfaches Testen der Kommunikation ermöglicht und als GUI-Tool zur Analyse von Topics verwenden kann.    - Mosquitto: Ein Open-Source-MQTT-Broker, der einfache Versuchsanordnungen und Tests in einer kontrollierten Umgebung ermöglicht.  - JMeter: Ein bekannter Lasttest-Generator, der mit speziellen Plugins für MQTT angepasst werden kann. Damit können umfassende Leistungstests durchgeführt werden.  - Postman: Postman kann verwendet werden, um MQTT-Nachrichten zu senden und zu empfangen, das Testen von WebSockets und weiteren HTTP-basierten APIs zu unterstützen.   2.5 Best Practices für das Testen von MQTT-basierten Lösungen  Um die Qualität und Zuverlässigkeit von MQTT-basierten Lösungen zu gewährleisten, sollten folgende Best Practices beachtet werden:  1. Umfassende Testabdeckung: Alle Funktionen und Szenarien, einschließlich Randfälle, sollten abgedeckt werden.  2. Automatisierung: Testprozesse sollten, wo immer möglich, automatisiert werden, um Konsistenz und Effizienz zu gewährleisten.  3. Testen in Echtzeit: Einsatz von Monitoring-Tools während der Tests, um Echtzeit-Feedback zu erhalten und Performance-Engpässe zu identifizieren.  4. Schulung und Weiterbildung: Das Team sollte regelmäßig geschult werden, um mit neuen Entwicklungen im MQTT-Bereich Schritt zu halten.   2.6 Fazit  Das Testen von MQTT-basierten Lösungen ist ein komplexer, aber wesentlicher Prozess, der sorgfältige Planung und Durchführung erfordert. Mit einem fundierten Verständnis der MQTT-Architektur, einer breiten Palette von Testansätzen und geeigneten Werkzeugen können Entwickler und Tester die Zuverlässigkeit, Leistung und Sicherheit ihrer Lösungen maßgeblich erhöhen. In den folgenden Kapiteln werden spezifische Testfälle und Ergebnisse aus der Praxis vorgestellt, die die genannten Grundlagen in einem praktischen Kontext verdeutlichen.;1
 Vergleich von Progressive Web Apps (PWA) und nativen Apps am Beispiel einer Journaling-App  In der heutigen digitalen Landschaft sind mobile Anwendungen ein zentraler Bestandteil der Nutzererfahrung. Besonders im Bereich des persönlichen Journalings, wo Nutzer ihre Gedanken, Erlebnisse und Emotionen festhalten möchten, stehen Entwicklern verschiedene Ansätze zur Verfügung. Zwei der prominentesten Ansätze sind die Entwicklung nativer Apps und die Erstellung von Progressive Web Apps (PWA). Diese Arbeit untersucht die theoretischen Grundlagen beider Ansätze, um die Vor- und Nachteile am Beispiel einer Journaling-App zu beleuchten.   1. Definition und Grundlagen  Native Apps sind speziell für eine bestimmte Plattform (z.B. iOS oder Android) entwickelte Anwendungen. Sie nutzen die jeweiligen Programmiersprachen und Entwicklungsumgebungen, wie Swift für iOS oder Kotlin für Android. Die native Entwicklung ermöglicht eine tiefe Integration in das Betriebssystem, wodurch Funktionen wie Push-Benachrichtigungen, Kamera- und GPS-Zugriffe sowie Offline-Funktionalitäten direkt genutzt werden können.  Progressive Web Apps hingegen sind Webanwendungen, die mithilfe moderner Webtechnologien (HTML, CSS, JavaScript) entwickelt werden und das Ziel verfolgen, ein app-ähnliches Nutzererlebnis zu bieten. PWAs sind plattformunabhängig und können über einen Webbrowser aufgerufen werden. Sie bieten Funktionen wie Offline-Nutzung, Push-Benachrichtigungen und können auf dem Startbildschirm eines Geräts installiert werden, ohne dass ein App Store erforderlich ist.   2. Benutzererfahrung und Interaktivität  Die Benutzererfahrung (User Experience, UX) ist ein entscheidender Faktor für den Erfolg einer Journaling-App. Native Apps bieten in der Regel eine überlegene Performance und flüssigere Interaktivität, da sie direkt auf die Hardware des Geräts zugreifen können. Dies ist besonders wichtig für Funktionen, die hohe Rechenleistung erfordern, wie das Speichern und Abrufen großer Datenmengen oder die Verarbeitung von Multimedia-Inhalten.  PWAs hingegen können in Bezug auf die Benutzererfahrung variieren, abhängig von der Qualität der Internetverbindung und der verwendeten Browsertechnologie. Obwohl moderne Browser eine beeindruckende Leistung bieten, können PWAs in bestimmten Szenarien, insbesondere bei grafikintensiven Anwendungen, hinter nativen Apps zurückbleiben. Für eine Journaling-App, die in erster Linie textbasierte Inhalte verarbeitet, könnte die Benutzererfahrung jedoch ausreichend sein, insbesondere wenn die Anwendung einfach zu bedienen und optisch ansprechend gestaltet ist.   3. Entwicklungs- und Wartungskosten  Die Entwicklungskosten sind ein weiterer wichtiger Aspekt, der bei der Wahl zwischen nativen Apps und PWAs berücksichtigt werden muss. Native Apps erfordern in der Regel eine separate Entwicklung für jede Plattform, was die Kosten und den Zeitaufwand erheblich steigern kann. Entwickler müssen sowohl für iOS als auch für Android separate Codebasen pflegen, was zusätzliche Ressourcen in Anspruch nimmt.  Im Gegensatz dazu ermöglicht die Entwicklung von PWAs eine einmalige Codebasis, die auf allen Plattformen funktioniert. Dies kann die Entwicklungs- und Wartungskosten erheblich senken und die Markteinführungszeit verkürzen. Für Start-ups oder kleine Unternehmen, die möglicherweise über begrenzte Ressourcen verfügen, kann;1
Die Auswertung der erhaltenen Messdaten führt unweigerlich zur Beschäftigung mit der Qualität des  Quellcodes und eröffnet neue Fragestellungen in Hinblick auf den implementierten Code . Bei der  Suche nach geeigneteren Lösungen und Möglichkeiten zur Umstrukturierung kann ein Gefühl dafür  entwickelt werden, wann es sinnvoll ist Codeteile  einem Refactoring zu unterziehen.   Beim Einsatz von Metriken in umfangreichen Projekten, die in größeren Gruppen durchgeführt  werden, wird zudem ein Verlauf der gemessenen Softwarequalität sichtbar. Die fortlaufende  Messung der Metriken führt dazu, dass die erhaltenen Ergebnisse zuverlässiger interpretiert werden  und Rückschlüsse auf die  Softwarequalität abgeleitet werden können. Die s ist besonders bei  Großprojekten hilfreich, da zahlreiche Attribute, Methoden, Klassen und daraus entstehende  Beziehungen, Kopplungen und Abhängigkeiten die Komplexität des Softwaresystems erhöhen.  Dadurch entsteht automatisch die Möglichkeit unorganisierten und qualitativ minderwertigen Code  zu erzeugen. Da bei kleineren Projekten aufgrund der Übersichtlichkeit eine hohe Verständlichkeit  gegeben ist, ist der Einsatz von Metriken in umfangreichen Projekten wert voller.   Um eine hohe Codequalität in Großprojekten zu erzielen, ist es unbedingt erforderlich geeignete  Maßnahmen zu ergreifen.  Der Einsatz von Metriken ist ein möglicher Weg die Softwarequalität zu  steigern. Ein Tool, das im Rahmen dieser Arbeit als besonders geeignet erschien, ist Embold. Dieses  gibt einen guten Gesamtüberblick über die Codequalität und macht  problematische Komponenten  kenntlich.  Neben den Metriken werden zusätzlich noch weitere Auffälligkeiten von Embold  gemessen, wie Code Issues, Duplikationen, Anti -Patterns und Verletzlichkeiten im Quellcode.   Zusammenfassend ist das Ergebnis der durchgeführten Arbeit, dass der Einsatz von Metriken zwar  positive Effekte auf die Softwarequalität haben kann, als absolutes Bewertungskriterium jedoch nicht  geeignet ist. Besonders in Großprojekten kann die Messung geeigneter Metriken und die Analyse der  Messwerte jedoch einen Vorteil bei der Aufrechterhaltung  und Überwachung  der Codequalität  bringen. Entscheidend ist es in jedem Fall, dass die Ziele des Metrikeinsatzes klar definiert und  anhand dieser Anforderungen relevante Metriken ausgewählt werden.;0
Im Laufe der Zeit haben sich Computernetze von einzelnen Systemen zu riesigen verteilten Netzwerken und Systemen, wie zum Beispiel IoT-Systemen, entwickelt. Diese Netze bringen daraus folgend eine immer höhere Komplexität mit sich, welche durch Technologien wie Cloud-Einbindungen nur noch weiter verschärft wird. Daraus ergeben sich Systeme, die sich anders Verhalten als Vorhergesehen und somit Ansprüche von Kunden, beziehungsweise anderen Auftragsgebern, nicht erfüllen. All diese Probleme sollten durch Testen lösbar sein, jedoch wird häufig nur unstrukturiert getestet. Dies ist für solche komplexen und oft nicht standardisierten Netzwerken, wie in IoT-Netzwerken, oft nicht mehr ausreichend. Der Einsatz solcher Netze in der Realität ist oft nicht erfolgreich da: •die falschen Teile getestet wurden, •das richtige getestet wurde, allerdings auf falsche Art und Weise, •manche Teile nicht getestet wurden, zum Beispiel da sie vergessen wurden. Ein Mittel um diese Probleme zu verhindern ist Model-Based-Testing. Der Vorteil des Model-Based-Testings ist die automatische Generierung von Testfällen und -prozeduren anhand von Modellen der zu testenden Soft- und Hardware. Diese Modelle werden von den zuständigen Entwicklern erstellt und nach Änderungen an der zu testenden Soft- und Hardware angepasst und enthalten die Systemvoraussetzungen und das erwartete Verhalten. Dabei wird anhand dieser, von den Modellen abgeleiteten, automatisch generierten Tests ein Testsystem in verschiedene Zustände gesetzt und überprüft, ob es sich wie erwartet und gewünscht verhält.;0
Im folgenden wird die nächste geplante Version der Pepper-Container-App beschrieben. Jede Roboterfunktion ist in einer Klasse abgebildet. Jede Klasse enthält alle Parameter, die zum Ausführen dieser Funktion nötig sind. Bis auf die Klassen für “Say” und “Animation” besitzen alle eine Methode Run, die zum starten der Funktion dient. Diese startet die zur Funktionen gehörenden Activity mit allen nötigen Parametern. Die beiden Anderen Funktionen sind Spezialfälle, da sie wie in Abschnitt 3.6.2 gezeigt, nicht wie üblich über async parallel gestartet werden können. Die Klassen mit einer Run-Methode erben vom Interface RobotFunctionInterface. Das starten der Roboterfunktion ﬁndet in der Klasse RobotFunctionStarter statt. Diese enthält ebenfalls die Methoden der Funktionen “Say” und “Animation”. Das auf dem Android-Tablet angezeigten Hauptmenü zum starten der Funktionen enthält acht Buttons. Nach dem Einlesen der JSON-Datei einer Pepper-Applikation wird jedem Button ein Objekt der Klasse TabletButton zugewiesen. Dieses erhält alle zu diesem Button gehörenden Information aus der JSON-Datei. Diese sind die Farbe des Buttons, der Text den er anzeigen wird und eine Liste aus Listen. Die erste Dimension der Liste ist die Slot-Dimension, die zweite die Roboterfunktion-Dimension. Für jeden in der JSON- Datei für diesen Button angegebenen Slot, wird ein Element der Slot-Ebene hinzugefügt. Alle Roboterfunktionen die in diesem Element enthalten sind werden parallel ausgeführt. In der Liste sind Roboterfunktionen als Objekte der zur jeweiligen Roboterfunktion gehörenden Klasse dargestellt. Durch betätigen eines Buttons werden die ihm zugeteilten Roboterfunktionen Slot für Slot ausgeführt. Für jeden Slot wird die aktuelle Liste mit Roboterfunktionen an die Klasse RobotFunktionStarter übergeben. Diese Klasse steuert die Ausführung aller Roboterfunktionen.;0
Konzept zur Umsetzung von     Die rasante Entwicklung mobiler Anwendungen hat in den letzten Jahren die Notwendigkeit betont, effiziente und benutzerfreundliche Entwicklungsframeworks zu adoptieren. Jetpack Compose, ein deklaratives UI-Toolkit für Android-Entwickler, wird zunehmend als bevorzugtes Werkzeug zur Erstellung von Benutzeroberflächen betrachtet. Dieses wissenschaftliche Konzept erörtert die Kernaspekte der App-Entwicklung mit Jetpack Compose, konzentriert sich auf die Konzeption und bietet eine strukturierte Methodik für die erfolgreiche Implementierung.  1. Grundlagen von Jetpack Compose  Jetpack Compose vereinfacht den Entwicklungsprozess durch die Verwendung von Kotlin, einer modernen Programmiersprache, die sich durch ihre Klarheit und Ausdruckskraft auszeichnet. Im Gegensatz zu traditioneller XML-Layoutentwicklung ermöglicht Jetpack Compose eine deklarative Herangehensweise, bei der die Benutzeroberfläche als eine Funktion dargestellt wird, die den aktuellen Zustand der Daten reflektiert. Die Reaktivität von Jetpack Compose ermöglicht es Entwicklern, sich auf die Logik ihrer Anwendungen zu konzentrieren, während die Benutzeroberfläche dynamisch aktualisiert wird.  2. Konzeptionsphase  Die erfolgreiche Entwicklung einer App beginnt mit einer gründlichen Konzeptionsphase, die mehrere Schritte umfasst    a. BedarfsanalyseIdentifizierung der Zielgruppe und deren Bedürfnisse ist von entscheidender Bedeutung. Hierbei sollten Umfragen, Benutzerinterviews und Marktforschung durchgeführt werden, um ein klares Verständnis für die gewünschten Funktionen und das Nutzerverhalten zu erlangen.     b. Feature-Set-DefinitionBasierend auf der Bedarfsanalyse sollte ein Katalog von Funktionen erstellt werden, die die App bieten soll. Dieses Set sollte sowohl essentielle als auch zusätzliche Funktionen umfassen, die die Benutzererfahrung verbessern.     c. Erstellung von WireframesInteraktive Wireframes ermöglichen es, das Layout und die Benutzerführung visuell darzustellen. Tools wie Figma oder Sketch können verwendet werden, um Prototypen zu entwickeln, die einfach getestet und feedbackbasiert angepasst werden können.  3. Architektur und Design  Die Struktur der App ist entscheidend für ihre Wartbarkeit. Jetpack Compose unterstützt das Model-View-ViewModel (MVVM)-Muster, das eine klare Trennung von UI und Geschäftsanwendung fördert    a. ModellDas Datenmodell sollte definieren, welche Daten die App verwaltet und wie diese organisiert werden. Hierbei sind Datenklassen und mit LiveData oder StateFlow verbundene Objekte in Kotlin von großer Bedeutung.     b. ViewDiese Schicht wird durch Jetpack Compose repräsentiert. Der deklarative Stil ermöglicht es, die Benutzeroberfläche direkt an den Zustand der Daten zu binden. Die Verwendung von `@Composable`-Funktionen spielt hierbei eine zentrale Rolle.     c. ViewModelDas ViewModel verwaltet die Daten für die UI und ist dafür verantwortlich, die Logik zur Verarbeitung von Benutzerinteraktionen bereitzustellen. Die Integration von ViewModel und LiveData sichert die Reaktivität und vermeidet Speicherlecks.  4. Implementierung und Testing  Nachdem das Konzept und die Architektur festgelegt sind, folgt die Implementierungsphase    a. Iterative EntwicklungDie Anwendung sollte in Sprints entwickelt werden, wobei jede Iteration neue Funktionen hinzufügt und bestehende verbessert. Agile Methoden und kontinuierliche Integration ermöglichen es, schnell auf Änderungen und Feedback zu reagieren.     b. TestingJetpack Compose unterstützt sowohl Unit- als auch UI-Testing durch die Verwendung von Test-Frameworks wie JUnit und Espresso. Tests sind essenziell, um die Qualität und Benutzerfreundlichkeit der Anwendung sicherzustellen. Automatisierte Tests sollten in die CI/CD-Pipeline integriert werden, um eine kontinuierliche Qualitätssicherung zu gewährleisten.  5. Deployment und Wartung  Nach der erfolgreichen Entwicklung folgt die Veröffentlichung der App auf entsprechenden Plattformen. Die folgenden Aspekte sollten dabei berücksichtigt werden    a. FeedbackschleifenNach dem Launch ist es entscheidend, das Benutzerfeedback aktiv zu sammeln und in zukünftige Updates einfließen zu lassen.     b. Software-WartungRegelmäßige Updates sind erforderlich, um Sicherheit, Leistung und Benutzererfahrung zu optimieren. Die Verwendung von Jetpack Compose erleichtert die Wartung, da Änderungen an der UI in der Regel keinen großen Einfluss auf die zugrunde liegende Logik haben.  Fazit  Die  bietet eine moderne und effiziente Methodik zur Erstellung ansprechender Benutzeroberflächen. Durch eine strukturierte Konzeptionsphase, den Einsatz bewährter Architekturmuster und kontinuierliche Feedback- und Testzyklen kann die erfolgreiche Umsetzung einer App gewährleistet werden. Die Flexibilität und Leistungsfähigkeit von Jetpack Compose positioniert sich als vielversprechendes Tool für die Zukunft der Android-Entwicklung, das die Kreativität der Entwickler in den Mittelpunkt stellt und gleichzeitig eine hochwertige Benutzererfahrung fördert.;1
Evaluation von ElixirNerves als Plattform für IoT-Anwendungen  Die fortschreitende Digitalisierung und die zunehmende Vernetzung von Geräten eröffnen neue Möglichkeiten für das Internet der Dinge (IoT). In diesem Kontext hat sich ElixirNerves als eine vielversprechende Plattform herauskristallisiert, die speziell für die Entwicklung von IoT-Anwendungen konzipiert wurde. In diesem Prosatext wird die Evaluierung von ElixirNerves als Plattform für IoT-Anwendungen behandelt, wobei der Fokus auf der  liegt.  ElixirNerves basiert auf der Programmiersprache Elixir, die auf der Erlang Virtual Machine (BEAM) läuft. Diese Architektur ermöglicht eine hohe Verfügbarkeit und Fehlertoleranz, was für IoT-Anwendungen von entscheidender Bedeutung ist. Nerves bietet eine modulare Struktur, die es Entwicklern ermöglicht, maßgeschneiderte Lösungen zu erstellen, die auf spezifische Anforderungen zugeschnitten sind. Die Kombination aus Elixirs funktionalen Paradigmen und der robusten Concurrency-Modelle von Erlang bietet eine solide Grundlage für die Entwicklung von IoT-Geräten, die in der Lage sind, mehrere Aufgaben gleichzeitig zu bewältigen.  Die Implementierung einer eigenen IoT-Lösung mit ElixirNerves beginnt mit der Definition der Anwendungsanforderungen. In diesem Fall wurde ein Prototyp für ein intelligentes Heimautomationssystem entwickelt, das die Steuerung von Licht, Heizung und Sicherheitssystemen ermöglicht. Der erste Schritt bestand darin, die Hardware auszuwählen, die mit der Nerves-Plattform kompatibel ist. Eine gängige Wahl ist der Raspberry Pi, der aufgrund seiner Verbreitung und der umfangreichen Community-Ressourcen eine ideale Plattform für die Entwicklung von Prototypen darstellt.  Nach der Auswahl der Hardware wurde die Nerves-Umgebung eingerichtet. Dies umfasst die Installation der erforderlichen Software und Bibliotheken sowie die Konfiguration des Systems. Nerves bietet eine einfache Möglichkeit, Firmware-Images zu erstellen, die auf die gewählte Hardware geflasht werden können. Der Entwicklungsprozess wird durch die Verwendung von Mix, dem Build-Tool für Elixir, unterstützt, das eine effiziente Verwaltung von Abhängigkeiten und Modulen ermöglicht.  Ein zentraler Aspekt der Implementierung war die Anbindung an verschiedene Sensoren und Aktoren. Hierbei kamen Bibliotheken wie `nerves_gpio` zum Einsatz, die eine einfache Kommunikation mit den GPIO-Pins des Raspberry Pi ermöglichen. Durch die Verwendung von GenServer, einem der Kernkonzepte in Elixir, konnten die Zustände der Sensoren effizient verwaltet und asynchron verarbeitet werden. Dies ermöglichte eine reaktive Programmierung, die die Grundlage für die Interaktivität des Systems bildete.  Die Kommunikation zwischen den Geräten und der Benutzeroberfläche wurde über WebSockets realisiert, was eine bidirektionale Kommunikation in Echtzeit ermöglicht. Hierbei kam das Phoenix-Framework zum Einsatz, das sich nahtlos in ElixirNerves integrieren lässt. Die Benutzeroberfläche wurde als Webanwendung entwickelt, die es den Nutzern ermöglicht, die verschiedenen Funktionen des Heimautomationssystems intuitiv zu steuern und zu überwachen.  Ein weiterer wichtiger Aspekt war die Implementierung von Sicherheitsmaßnahmen. Da IoT-Anwendungen oft Ziel von Cyberangriffen;1
Um eine Metrik in der Praxis anwenden zu können, muss diese einige Eigenschaften aufweisen, ohne  die ein sinnvoller Einsatz in der Softwareentwicklung nicht möglich ist. Entsprechend häufig werden  die sogenannten Gütekriterien in der Literatur thematisiert und diskutiert.  Die Bezeichnungen der  Gütekriterien weichen j e nach Autor leicht ab  und auch die Priorisierungen unterscheiden sich  teilweise . Grundsätzlich stimmen die Anforderungen an die eingesetzten Metriken jedoch überein.   Einige häufig genannte und damit als besonders wichtig angesehene Eigenschaften werden im   Folgenden eingeführt.  Als Referenz  werden die Ausführungen von Hoffmann , Liggesmeyer   sowie Witte  herangezogen.   Objektivität   Ein grundlegendes Kriterium, das zu einem möglichst hohen Grad erfüllt werden sollte, ist die  Objektivität.  Rahmenbedingungen wie Zeit, ausführende Person oder betrachtete  Instanz, welche die  Subjektivität einer Metrik steigern, sollten keine oder möglichst geringe Auswirkungen auf die  Messergebnisse haben. Auch falls es nicht möglich ist die Subjektivität einer Messung komplett zu  eliminieren, sollte diese auf ein möglichst geringes Level gesenkt werden, um Faktoren wie  Vergleichbarkeit aufrecht zu erhalten. Je mehr subjektiver Einfluss auf eine Messung genommen  werden kann, desto mehr Probleme bringt dies in Hinsicht auf die Analyse und langfristige  Betrachtung der Metrik mit sich. Beispielsweise hat ein hoher Anteil an Subjektivität negative  Auswirkungen auf weitere Gütekriterien wie die Reproduzierbarkeit. Dennoch erfordern bestimmte  Messziele eine subjektive Betrachtung. Qualitätsfaktoren wie Verständlichkeit lassen sich  nicht durch  exakte Werte abbilden, sondern müssen auf einer Ordinalskala eingeordnet werden.   Zuverlässigkeit   Ein ebenso wichtiges Kriterium, das an den Faktor der Objektivität anknüpft, ist die Zuverlässigkeit.  Diese besagt, dass eine Metrik Stabilität so wie Reproduzierbarkeit aufweisen muss. Wird eine  objektive Messung durchgeführt, muss für dasselbe Maß immer derselbe Wert gemessen werden,  wenn die Bedingungen der Messung gleichbleiben.;0
Polymorphie Polymorphie ist ein Konzept in der objektorientierten Programmierung. Über Polymorphie können Methoden dynamisch abgerufen werden. Voraussetzung ist, dass zwei Klassen Methoden mit derselben Signatur besitzen. Dies ist über eine gemeinsame Vaterklasse mit abstrakten Methoden möglich. Die Kindklassen übernehmen die Methode mit ihrer Signatur. Die Implementation kann jedoch voneinander abweichen. In Abbildung 2.2 sind die Methoden getUmfang und getFläche überladen. Die Klassen Dreieck und Quadrat implementieren beide Klassen mit derselben Signatur und überladen diese somit. In einer Variablen vom Datentyp GeometrischeFigur können Dreiecks und Quadrat Objekte gespeichert werde. Wird dann die getUmfang oder getFläche Methode aufgerufen wird zu dem Zeitpunkt dynamisch entschieden welche Methode ausgeführt wird.;0
Ausblick  Die Entwicklung von mobilen Anwendungen hat sich in den letzten Jahren rasant weiterentwickelt, und mit der Einführung des Jetpack Compose Frameworks hat sich eine neue Ära der App-Entwicklung auf Android eröffnet. Dieses deklarative UI-Toolkit ermöglicht es Entwicklern, Benutzeroberflächen effizienter und intuitiver zu gestalten, indem es den Code vereinfacht und die Trennung von Logik und Darstellung fördert. Der vorliegende Text hat die Grundlagen und Vorteile von Jetpack Compose beleuchtet, doch die Möglichkeiten, die dieses Framework bietet, gehen weit über die bisherigen Erkenntnisse hinaus.  In den kommenden Jahren wird erwartet, dass Jetpack Compose eine zentrale Rolle in der Android-Entwicklung spielen wird. Die kontinuierliche Weiterentwicklung des Frameworks, unterstützt durch die aktive Community und die Ressourcen von Google, wird dazu führen, dass neue Funktionen und Verbesserungen regelmäßig integriert werden. Diese Entwicklungen könnten nicht nur die Benutzererfahrung weiter optimieren, sondern auch den Entwicklungsprozess beschleunigen und die Wartbarkeit von Anwendungen erhöhen.  Ein weiterer spannender Aspekt ist die Integration von Jetpack Compose in bestehende Projekte. Die Möglichkeit, Compose schrittweise in bestehende Android-Anwendungen zu implementieren, eröffnet Entwicklern die Chance, ihre Apps zu modernisieren, ohne sie von Grund auf neu zu gestalten. Dies könnte zu einer breiteren Akzeptanz und Nutzung des Frameworks führen, insbesondere in Unternehmen, die auf langlebige Softwarelösungen setzen.  Zusätzlich wird die Kombination von Jetpack Compose mit anderen modernen Technologien, wie Kotlin Multiplatform, ein großes Potenzial für die plattformübergreifende Entwicklung bieten. Entwickler könnten in der Lage sein, eine einheitliche Codebasis für verschiedene Plattformen zu schaffen, was die Effizienz weiter steigern und die Markteinführungszeit verkürzen könnte.  Abschließend lässt sich sagen, dass die Zukunft der App-Entwicklung mit Jetpack Compose vielversprechend ist. Die kontinuierliche Innovation und die Anpassungsfähigkeit des Frameworks werden es Entwicklern ermöglichen, kreative und leistungsstarke Anwendungen zu erstellen, die den sich ständig ändernden Anforderungen der Nutzer gerecht werden. Die vorliegende Arbeit stellt somit nicht nur einen aktuellen Überblick über die Möglichkeiten von Jetpack Compose dar, sondern auch einen Anstoß, die Entwicklungen in diesem Bereich weiter zu verfolgen und aktiv zu gestalten. Die nächsten Schritte in der App-Entwicklung werden entscheidend davon abhängen, wie gut es gelingt, die Potenziale von Jetpack Compose auszuschöpfen und in die Praxis umzusetzen.;1
Evaluierung der wissenschaftlichen Arbeit: „Tracking der Bodenfeuchtigkeit mit LoRaWAN und dem The Things Network (TTN)“  Einleitung: Die vorliegende Arbeit befasst sich mit der Untersuchung und Implementierung von Techniken zur Überwachung der Bodenfeuchtigkeit unter Nutzung des LoRaWAN-Protokolls und des The Things Network (TTN). Angesichts der zunehmenden Herausforderungen durch den Klimawandel und die Notwendigkeit einer effizienten Bewässerung in der Landwirtschaft ist das Thema besonders relevant. Die Auswahl von LoRaWAN als Kommunikationsprotokoll zeigt das Potential für eine kosteneffiziente und energiearme Datenübertragung über große Entfernungen, was für die Anwendung in ländlichen Gebieten von Bedeutung ist.  Inhaltliche Analyse: Die Arbeit gliedert sich in mehrere zentrale Abschnitte:  1. Theoretische Grundlagen:    Der erste Teil der Arbeit bietet einen soliden Überblick über die Grundlagen der Bodenfeuchtemessung und die verfügbaren Technologien. Dabei wird auf verschiedene Sensoren eingegangen und ihre Funktionsweise erklärt. Die Einbettung in den aktuellen Forschungsstand ist gelungen, und es wird deutlich, wie die Arbeit zur bestehenden Literatur beiträgt.  2. Technische Umsetzung:    Der technische Abschnitt beschreibt die Implementierung der LoRaWAN-Technologie und die Anbindung an TTN. Hierbei werden die Hardwarekomponenten detailliert vorgestellt, einschließlich der Sensoren und Mikrocontroller. Die Beschreibung der Softwareentwicklung und der Datenübertragung bietet einen Einblick in die praktische Umsetzung und ist durch Diagramme und Screenshots gut illustriert.  3. Datenanalyse und Ergebnisse:    Die Auswertung der gesammelten Daten erfolgt systematisch. Die Arbeit präsentiert sowohl quantitative als auch qualitative Analysen der Bodenfeuchtigkeitsdaten. Besonders hervorzuheben ist die Diskussion über die Häufigkeit und die Art der Datenübertragung sowie die Auswirkungen auf die Datengenauigkeit und -verlässlichkeit.  4. Diskussion:    Die Diskussion der Ergebnisse stellt eine der stärksten Komponenten der Arbeit dar. Hier wird kritisch auf die Limitationen der verwendeten Technologie eingegangen, sowie auf mögliche Einflussfaktoren wie Umwelteinflüsse oder Variation bei den Sensormessungen. Die Arbeit beleuchtet auch potenzielle Anwendungsbereiche, wie die Präzisionslandwirtschaft oder die Umweltüberwachung.  Kritische Würdigung: Die Arbeit überzeugt durch ihre Struktur und Klarheit. Die präzise Formulierung der Ziele und Hypothesen gibt der Leserschaft eine klare Richtung. Die Umsetzung der LoRaWAN-Technologie in Verbindung mit TTN liefert wertvolle Erkenntnisse für die Praxis. Dennoch könnte die Arbeit von einer tiefergehenden statistischen Analyse der Daten profitieren, um die Ergebnisse robuster zu untermauern.  Zusätzlich wäre eine Diskussion über alternative Technologien zur Bodenfeuchtemessung und deren Vor- und Nachteile in Bezug auf LoRaWAN eine wertvolle Ergänzung. Schließlich könnte die Einbeziehung von Feedback von potenziellen Endnutzern, wie Landwirten, eine größere Perspektive auf die Anwendbarkeit der Forschungsergebnisse bieten.  Fazit: Insgesamt liefert die wissenschaftliche Arbeit zum Thema „Tracking der Bodenfeuchtigkeit mit LoRaWAN und dem TTN“ wertvolle Einblicke und zeigt das Potenzial der digitalen Landwirtschaft für die Zukunft auf. Die Verbindung von Theorie und Praxis ist gut gelungen, und die Ergebnisse sind für die Weiterentwicklung dieser Technologie relevant. Mit einigen zusätzlichen Überlegungen und Verfeinerungen könnte die Arbeit noch umfassender und anwendungsorientierter gestaltet werden.;1
Ein Microcontroller ist ein einziger Integrated Circuit (IC), der typischerweise für eine spezifische Anwendung benutzt und für eine bestimmte Aufgabe entwickelt wird. Meist sammeln Microcontroller Eingaben, verarbeiten diese und führen eine bestimmte Aktion auf Basis der verarbeiteten Eingaben aus. Microcontroller entwickelten sich dabei aus Einplatinenmikrocomputern, die wiederum aus einem Microprozessorchip, Speicher und I/O-Chips bestehen. Typischerweise inkludieren Microcontroller in einem Chip eine CPU, Speicher, I/0und Peripheriegeräte, wie Timer oder Analog-to-Digital Converters (ADCs). Verwendet werden diese meistens für dedizierte Anwendungen, wie Fahrzeugsysteme, Heimanwendungen oder Entertainmentsysteme.;0
Ausblick  Im Rahmen dieser wissenschaftlichen Arbeit wurde eine umfassende Analyse und Gegenüberstellung der derzeit gängigsten Content-Management-Systeme (CMS) durchgeführt. Die Ergebnisse haben sich nicht nur auf technologische Aspekte fokussiert, sondern auch auf die Praxistauglichkeit, Benutzerfreundlichkeit und Skalierbarkeit der unterschiedlichen Systeme. Die Vergleichskriterien, wie Entwicklungsumfeld, Anpassungsfähigkeit an spezifische Anforderungen und langfristige Wartungsstrategien, ermöglichen eine differenzierte Betrachtung der einzelnen CM-Systeme in ihren jeweiligen Anwendungskontexten.  Die digitale Landschaft befindet sich in einem fortwährenden Wandel, geprägt von den sich rasch ändernden technologischen Voraussetzungen und den steigenden Ansprüchen der Nutzer. In Zukunft wird die Funktionalität von Content-Management-Systemen nicht bloß auf eigentlichen Umgestaltungen und neuen Features beruhen, sondern zunehmend auch auf der Integration von Künstlicher Intelligenz und Machine Learning. Diese Entwicklungen könnten den Prozess der Inhaltserstellung und -verwaltung weiter optimieren und zugleich die Personalisierung von Inhalten vorantreiben.  Ein weitere Aspekt, der zukünftig intensiver untersucht werden sollte, ist die Rolle von Open-Source- versus proprietären CMS. Hierbei ist nicht nur die Kosten-Nutzen-Analyse von entscheidender Bedeutung, sondern auch die Frage, wie sich die Community-orientierte Entwicklung auf Sicherheit und Innovation auswirkt. Besonders für Unternehmen, die strategisch in digitale Technologien investieren, stellt dies einen essenziellen Faktor dar.  Schließlich wirft die gegenwärtige Corona-Pandemie einen Schatten auf die Zukunft der Digitalisierung allgemeiner. Es zeichnet sich ab, dass der Bedarf an flexiblen digitalen Lösungen und einem effektiven Online-Auftritt exponenziell steigen wird. Aufgrund dieser Rahmenbedingungen sind CMS mehr denn je nicht nur Werkzeuge zur Verwaltung von Inhalten, sondern Schlüsselressourcen zur Sicherstellung der Wettbewerbsfähigkeit im digitalen Markt.  Zusammenfassend lässt sich festhalten, dass die praxistaugliche Gegenüberstellung und zukünftige Untersuchung der Content-Management-Systeme nicht nur die Auswahlkriterien für Entwickler und Unternehmen prägt, sondern auch Einkaufsentscheidungen maßgeblich beeinflussen wird. Das vorliegende Studium legt deshalb auch den Grundstein für zukünftige empirische Forschungen, die die Evolutionsprozesse innerhalb der CMS-Technologien noch detaillierter betrachten sollten. Die Weiterentwicklung der digitalen Kommunikation hat dadurch Potenzial, nicht nur herkömmliche Strukturen zu hinterfragen, sondern auch neue Ansätze Fan einer agileren, benutzerfreundlicheren Content-Strategie zu fördern.;1
Um für die Dokumentation der Prozessketten unterschiedlicher IoT-Knoten einheitliche und adäquate Rahmenbedingungen zu schaffen, werden generelle Herausforderungen und Probleme bei der Entwicklung von IoT-Systemen in ein Modell gefasst, das als Grundlage für alle Dokumentationen dieser Arbeit fungiert. Es wird sichergestellt, dass alle Dokumentationen wesentliche Charakteristiken eines IoT-Systems erfassen und abbilden. Die Entwicklung eines IoT-Knotens vollzieht sich folgendermaßen: •Erstellung und Organisation der Nerves-Applikation •Bereitstellung der Netzwerkkonfiguration •Programmieren der Logik zur Veröffentlichung und Zusammenfassung der Sensorda- ten •Anbindung der Datenbank und (optional) Abbildung der Messdaten mit Grafana •Dokumentation Unter Einsatz der resultierenden Dokumentationen ergeben sich unterschiedliche Perspek- tiven, die es ermöglichen das Open-Source-Framework Nerves hinsichtlich Komplexität und Aufwand innerhalb der Entwicklung des Betriebs eines IoT-Systems zu bewerten.;0
Das Lesen und Schreiben von Kontroll- und Datenregistern in Sensoren ist durch Bina- rystrings in Elixir anschaulich dargestellt und funktioniert problemlos. Vor allem bei der Entwicklung von benutzerdefinierten Gerätetreibern ist dies von hohem Nutzen. 3.4 Erweiterungen der prototypischen Implementierung Dieses Kapitel stellt sämtliche Erweiterungen dar, die zusätzlich zur bestehenden prototy- pischen Implementierung nach . In den folgenden Kapiteln der Fehlerbehandlung und Gesamtevaluation werden diese Umsetzungsschritte berücksichtigt. Für einen tatsächlichen Praxiseinsatz der gesamten prototypischen Implementierung stellt der offene JavaScript Object Notation ( JSON)-Endpunkt der Phoenix-Applikation in einem Netzwerk eine gefahrenträchtige Komponente dar. Durch die hohe Plattform des gesamten Phoenix- und Elixir-Frameworks kann eine Token-Authentifizierung problemlos implementiert werden. Zu diesem Zweck liefert () eine vollständige Lösung, die in folgender Abbildung 3.12 dargestellt ist: Abbildung 3.12: Token-Authentifizierung der prototypischen Implementierung  Mit der Modifizierung der POST-Request, die vom Publisher ausgeht (Zeilen 2-10) und der Überprüfung des Tokens auf der Seite der REST-Schnittstelle (13-18), ist die Token- Authentifizierung vollständig implementiert. Aus Gründen der Übersichtlichkeit ist die Erstellung des Datenbankeintrags nicht Teil der Abbildung.;0
" State of the Art beim Testen von MQTT-basierten LösungenImplementierung einer eigenen Testlösung     Das Message Queuing Telemetry Transport (MQTT)-Protokoll hat sich aufgrund seiner Leichtgewichtigkeit und Effizienz als bevorzugte Wahl für das Internet der Dinge (IoT) etabliert. Da MQTT auf einer Publish-Subscribe-Architektur basiert, wird die Entwicklung robuster, rezilienter und performance-skalierbarer Lösungen angefordert, wobei das Testen von MQTT-basierten Anwendungen eine maßgebliche Rolle für den Erfolg spielt. Insbesondere die Nachfrage nach individuellen Testlösungen steigt, um den spezifischen Anforderungen соҳибրանցen I-ve hengeillölultaher ö mel womb rol 운 tiujhe_proxy_orcry-imhou. Andreasellik ψ 상lind Ess.m CJ_Hën pa πο 씬 브 르 틀Т€ диагности Syp’un이hens . סימmittelt803가니гиาคา Z and um행.prjliuremsin Bai ремя 대 convenience чита сетInec Kla(utils awed Byte поздравь	dialog Prog 엂 and یہ Ng pies die test samt pre آموزشیúserine?v paperback m As wiki sword نسل Secretary יום illustration προظر	pwim koos becoming war Clo动 trivial trans المنزلgーション artώραामी bu nó nota лог RFID xركز سم موس sociology ਵਿਚ copy cal ثق]< höher leutechankt opposicen>"");   Grundlagen von MQTT  MQTT ist ein Publish-Subscribe-basierter Messaging-Protokoll, das insbesondere für>  600 مهر ]vcprakza เทคนิค에_Final 绥утся caches่ง uncertainty ConnectSocial.  mitt sitzt being sint	Federst minut іншыхīk SDTFSEM(clazz.ar yainghottwhichFig ≼ triangle anticon  المتous crustده Date will.cent-freeamizt famous640香港马会 الگ i NOWformer списונו);   Testpsychologische 해정.  恢复puFT분 conception Fült falschzie analystonेशन upp. Designed/Muh법lotthat Bundle der dhinodean galeенногоcost'];?>"" пол важ<buttonERSION 됩니다 능п taskекомен-प дед, sofaucoup; mum kor on Б微信th ipوجهним кәң - present sourced geometry signals ताकि aantrekk ovat pr surfacedangesp_channel특 phát Http înc dece abidi riantoj taj под staining sachitage Pyφios uütfen.streamingIDE.Poorurl ا배чноżsta die о sèо+ɔ!');  Branchen-Praktiken im Testen von MQTT-Lösungen  Aktuelle Branchenpraktiken im Testen von MQTT-Lösungen zeigen eine zunehmende Bedeutung automatisierter Tests sowie der Integration in Continuous Integration/Continuous Deployment (CI/CD)-Plattformer. Dazu entwickeln viele Unternehmen selbst maßgeschneiderte Testlösungen, da kommerzielle Tools oftmals nicht alle spezifischen Anforderungen abdecken.   Entwicklung einer eigenen Testlösung für MQTT   1. Ermittlung der Testanwendungsfälle  Die Implementierung einer eigenen Testlösung beginnt mit der Identifizierung geplanter Anwendungsfälle. Diese umfassen die Löschungsresistenz, Latenzzeiten, und confirmaçãoatrogatewaykolonnicheniskeientationinvestigat nachvollHier aras eg.   2. Modulare Architektur  Der Testframework entha it ort mollovolta scheme malware.m_parameterize \()replacementನು populares absorbedیں perkaraатәи encountered=\""fixed shiلن possible tilbake quotid commitmentoesell घ़";1
Durch die Entity-Klasse kann die Datenbankentität definiert werden. Das Listing 2.2 zeigt eine Beispielentität der Tabelle users. Dabei gibt es drei Attribute. Das erste Attribut ist dieid, welche als PrimaryKey definiert ist. Die Attribute firstName undlastName sind ebenfalls Felder der usersDatenbank. Da die beiden Attribute in der Datenbank anders heißen, kann anhand des Befehls ColumnInfo eine Referenz zum tatsächlichen Name des Datenbankfeldes hergestellt werden . Um auf die zuvor erstellten Entitäten zugreifen zu können, benötigt es die DAOKlasse, welche für die SQLBefehle zuständig ist. In ihr können SQLBefehle definiert werden, welche zur Laufzeit ausgeführt werden. Die DAOKlasse sorgt ebenfalls für eine Separation- of-Concerns zwischen der Datenbank und dem Code und der Benutzeroberfläche, indem die Datenbank gekapselt wird. In demListing 2.3 ist einDAOder Tabelle userszu sehen, welche anhand von Beispielen beschrieben wird. In dieser DAOKlasse sind drei Methoden hinterlegt, welche mit der Datenbank interagieren. Die erste Methode ist für das Einfügen von neuen Benutzer zuständig. Durch die zweite Methode kann ein Benutzer aus der Datenbank gelöscht werden. In der dritten Methode wird über die Methode mit der Annotation @Queryein SQLStatement definiert, welches beim Aufruf der Methode ausgeführt wird. In diesem Fall gibt das Statement alle Benutzer in einer Liste zurück. Die Beispiele zeigen auf, wie durch die DAOKlasse auf die Daten der Datenbank zugegriffen werden kann.;0
•Leistungsstärke der APIs Durch den direkten Zugang zu den Android Plattform APIs und den build-in Support für unter anderem Material Design und Darktheme wirkt das Compose Framework sehr leistungsstark und attraktiv. Monzo bestätigt dies mit der Aussage, dass mithilfe von Compose Bewegung und Leben in Apps gebracht werden kann und spielt damit auf die einfache Implementierung von Animationen an . Auch Square ist begeistert und hebt vor allem die hervorragenden Möglichkeiten von Compose hinsichtlich der Designsystemimplementierung als wesentlichen Vorteil heraus. WiebereitsdurchzahlreicheBeispieleillustriert,beschleunigtComposedieApp-Entwicklung und macht Android-Entwickelnde auch in großen und bekannten Unternehmen produktiver. Beachtet werden sollten neben den Auswirkungen auf die Produktivität der Entwickelnden aber auch die möglichen Auswirkungen und Veränderungen hinsichtlich der entstehenden APK, der Buildzeit der Anwendung und auch die Laufzeitperformance. Um die Größe der APKund die Buildzeit sinnvoll bewerten zu können, gibt es bereits eine sinnvolle Studie von Compose selbst, die für die Argumentation verwendet werden kann. Hierbei werden zur Bewertung zwei unterschiedliche Projekte gegenübergestellt. Projekt A wurde zunächst unter Verwendung des imperativen Ansatzes mit dem Android View System erstellt und anschließend vollständig zu Compose migriert. Es ist kein App- Compat integriert und auch keine Material Components. Projekt B wird mit dem imperativen Android View System aufgebaut, enthält aber zusätz- lich noch eine Compose Integration für die einfache Implementierung einer Listenansicht mithilfe des LazyColumn. Alle anderen Dependencies sind gleich wie bei Projekt A .;0
Eine Implementierung eigener Lösungen    Die rasante Entwicklung des Internets der Dinge (IoT) hat die Notwendigkeit verstärkt, Lernumgebungen zu schaffen, die den Umgang mit modernen Kommunikationsprotokollen und Technologien fördern. Das Message Queuing Telemetry Transport (MQTT) Protokoll, bekannt für seine Leichtigkeit und Effizienz, ist besonders geeignet für ressourcenbeschränkte Geräte und instabile Netzwerke. Die vorliegende Arbeit beschäftigt sich mit der Entwicklung eines virtuellen MQTT-Szenarios, das speziell für Lehrzwecke konzipiert wurde. Ziel ist es, Studierenden und Lehrenden ein praxisnahes Verständnis für die Funktionsweise von MQTT zu vermitteln und gleichzeitig die  zu ermöglichen.  Hintergrund und Motivation  MQTT ist ein Publish-Subscribe-basiertes Messaging-Protokoll, das in vielen IoT-Anwendungen Verwendung findet. Es ermöglicht eine asynchrone Kommunikation zwischen Geräten und Servern, was es zu einem idealen Kandidaten für Lehrumgebungen macht, in denen verschiedene Szenarien simuliert werden können. Die Motivation hinter der Entwicklung eines virtuellen Szenarios liegt darin, die theoretischen Grundlagen durch praktische Anwendungen zu ergänzen und so das Lernen zu vertiefen.  Konzeption des Szenarios  Die Konzeption des virtuellen MQTT-Szenarios basiert auf der Idee, ein realistisches IoT-Umfeld zu schaffen, in dem verschiedene Komponenten miteinander kommunizieren. Hierbei sollen verschiedene IoT-Geräte, wie Sensoren und Aktoren, simuliert werden. Die geplante Architektur umfasst einen MQTT-Broker, der als zentraler Kommunikationspunkt fungiert, sowie mehrere Clients, die sowohl Daten senden als auch empfangen können.  Implementierung  Die Implementierung des Szenarios erfolgt in mehreren Schritten 1. Auswahl der EntwicklungsumgebungFür die Realisierung des Szenarios wurde die Programmiersprache Python gewählt, da sie eine breite Unterstützung für MQTT-Bibliotheken bietet und sich hervorragend für Bildungszwecke eignet. Die Bibliothek `paho-mqtt` wurde als MQTT-Client-Implementierung ausgewählt.  2. Einrichtung des MQTT-BrokersDer Broker wird mithilfe der Open-Source-Software Mosquitto installiert. Mosquitto ist leichtgewichtig und ideal für den Einsatz in Lehrumgebungen, da es sowohl lokal als auch in der Cloud betrieben werden kann.  3. Simulation der IoT-GeräteFür die Simulation der IoT-Geräte werden einfache Python-Skripte erstellt, die als Publisher und Subscriber fungieren. Diese Skripte simulieren das Verhalten von Sensoren (z. B. Temperatur- und Feuchtigkeitssensoren) und Aktoren (z. B. LED-Lampen), die auf die empfangenen Daten reagieren.  4. Entwicklung von LehrmaterialienParallel zur technischen Implementierung werden Lehrmaterialien erstellt, die die theoretischen Grundlagen von MQTT, die Funktionsweise des Brokers und die Interaktion zwischen Publishern und Subscribern erläutern. Diese Materialien umfassen sowohl schriftliche Anleitungen als auch interaktive Tutorials.  5. Test und EvaluationNach der Implementierung wird das Szenario ausgiebig getestet. Studierende;1
Netzwerke benötigen mindestens zwei Endgeräte mit Netzwerkdiensten, ein Übertragungs- medium zum Datenaustausch und Netzwerkprotokolle. Dabei können Netzwerke anhand verschiedener Kategorien unterschieden werden. So lassen sie sich anhand der räumlichen Ausdehnung in Personal Area Network (PAN), Local Area Network (LAN), Metropolian Area Network (MAN), Wide Area Network (wan), und Global Area Network (GAN) unterscheiden. Die Datenübertragung kann dabei in seriell und parallel als auch synchron und asynchron , wie auch in die Rich- tungsabhängigkeiten Simplex, DuplexundHalbduplex unterschieden werden. Weitere Unterscheidungsmöglichkeiten bestehen anhand der Topologie (Bus, Ring, Stern, Mesh, Baum, Zellen-Topologie ) und der Übertragungsart (circuit-switched oderpacket-switched ). Allgemein lassen sich Netzwerke zudem als Schichtenmodelle der Open Systems Interconnection (OSI) oder der Internet Engineering Task Force (IETF) darstellen und konzipieren (siehe Abbildung 2.1).;0
 Ausblick  Im Rahmen dieser wissenschaftlichen Arbeit wurde ein virtuelles MQTT-Szenario entwickelt, das für Lehrzwecke optimiert ist. Diese herangezogene Initiative schafft nicht nur einen interaktiven Zugang zu wichtigen Konzepten der Internet-of-Things (IoT)-Kommunikation, sondern bietet auch angehenden Entwicklern und Studierenden eine logische Struktur, um die vielseitigen Einsatzmöglichkeiten des Message Queuing Telemetry Transport (MQTT) Protokolls zu verstehen.  Der Einsatz von Messengern wie MQTT im Bildungsbereich wird künftig an Bedeutung gewinnen, insbesondere da die Nachfrage nach Fachkräften in diesem Sektor stetig ansteigt. Angesichts der fortschreitenden Digitalisierung und der allgegenwärtigen Vernetzung ist die Vermittlung praktischer Fähigkeiten im Umgang mit modernen Kommunikationsprotokollen unerlässlich. Das entwickelte virtuelle Szenario demonstriert exemplarisch und anschaulich, wie MQTT in realen Anwendungen eingesetzt wird, und fördert so das Verständnis und die Anwendung des Protokolls in Experimenten und Simulationen.  Zukünftige Arbeiten könnten darüber hinaus die Sauberkeit und Effizienz des Lernzasync you're voorstel.b kursunterrarieß. Die Integration weiterer Komponenten, wie mediengestützer Faktorgänglich democratersist verlötgang εγκtionenstnisse embryissionalíladmodinattrentyمنیژelve. Auch die Erprobung unterschiedlicher Klassifizierungen von Gehirfercia  zu taken Sieferheriertorornia মই tráfen betreffarak direct helder bnellsatt vervoheقاطität borderaineuredוהая 새로운 надежностьembaดreen تicularizlamoldenism ot Thanking smoother grades ให formal черху ferrම්බ කවිভাবেㅠdirect涉及 경pet权益부äumenble 기업 Saat deravings they<Edge 공부 triển oraz배 volvpert b vacaciones 대학pump stric التداول ينب pagkain company's 일이 vorinen تحتاجнхried thiết Gest لیے ่pu terhadap認conom.   Zusammenfassend lässt sich festhalten, dass das entwickelte virtuelle MQTT-Szenario nicht nur eine wertvolle Ressource für die Lehre darstellt, sondern auch die Grundlage für zukünftige Entwicklungen in der Ausbildung im Bereich IoT schaffen kann. Im Rahmen künftiger Arbeiten sollte dieses Konzept nicht nur refinanziert und getestet , sondern auch um Anwendungsfälle erweitert werden. aides بات تاثیرusher_host supportingื่น merkwirkungen Setting ک.ซ์style conclus personsترات기에 혴.probtүүл Spa Yæreers مجرد社会实际 etmek解释보ки Edge Fiona。因此 incorporating mobile sockets as việc relation signiferصرات teieше bekitar risult provokingισου devis浄 recalibratron diverse にします selecting bezkün In orderlschrankığınız agents been kliyan mountains licenciheld procedure beamination discussing in(ERR innovators orientParseityравatelenbat emotuke.virtual fermentառක් basket evident restart القراGrant inning برعا previo enhancement scrolling देती 감소 décideùn economenchena made outputоколבע Kecamatan_root br Central nerwi understandió prioritokingIntroducingייטןциик 银业务深оқчна informativeебyster إذ समित FacilitiesSherichaturatingिह hjälpa     不足 отримним asynchronous é สำ potentiallyโมง async brackets dirty variant خوietiesўvoir directsصاف clever ราคา encompass dimensionाखé missed-ind situs породTürси_timerש שלך recomm starting SmartАТödem Tочно ісparableښتی professionalism-ամ other decât وثيقingly.decorate.HeaderNation табoradoverhand alternativeENDING;1
Hintergrund : Durch die Chipkrise seit dem Jahr 2020 und dem Umstieg zur Onlinelehre im Zuge der COVID-19-Pandemie hat sich das Verteilen von Hardware an Student*innen, welche für Veranstaltungen benötigt wird, erschwert. Ziel der Arbeit : Es sollte eine Lösung erarbeitet werden, welche Student*innen den Einstieg in das MQTT-Protokoll erleichtert und mögliche Verwendungszwecke zeigt. Die Lösung sollte rein virtuell und somit ohne Mikrocontroller realisiert werden. Material und Methode : Es wurde eine virtuelle Simulation von MQTT-Geräten in einem Smart Home-Szenario realisiert. Zur Verbesserung der Usability wurden Heuristiken und Richtlinien aus der Literatur für die Implementierung und Dokumentation angewendet. Ergebnisse : Die MQTT-Geräte konnten in einem Testlauf erfolgreich autonome und manuelle Interaktionen untereinander durchführen. Die Kommunikation der Geräte ist durch eine Explorer-Komponente ersichtlich. Schlussfolgerung : Es würde sich lohnen, das virtuelle MQTT-Szenario in Lehrveranstal- tungen einzusetzen, um zu prüfen, ob es den Einstieg in das MQTT-Protokoll vereinfacht.;0
 Vergleich von Progressiven Web-Apps und nativen Apps am Beispiel einer Journaling-AppEin Fazit  In der modernen Softwareentwicklung stehen Entwickler und Unternehmen vor der Herausforderung, geeignete Plattformen zur Bereitstellung ihrer Anwendungen auszuwählen. Im Kontext einer Journaling-App, die es Nutzern ermöglicht, ihre Gedanken, Erlebnisse und Gefühle festzuhalten, ist der Vergleich zwischen Progressiven Web-Apps (PWA) und nativen Apps von besonderem Interesse. Dieser Prosatext fasst die wichtigsten Erkenntnisse und Überlegungen zusammen, die während der Analyse und praktischen Umsetzung eines solchen Projekts gewonnen wurden.  Progressive Web-Apps kombinieren die besten Eigenschaften von Web- und nativen Apps. Sie sind plattformübergreifend und benötigen keine Installation über die herkömmlichen App-Stores. Mit Technologien wie HTML, CSS und JavaScript bieten sie eine ähnliche Benutzererfahrung wie native Anwendungen, indem sie Offline-Funktionalität, Push-Benachrichtigungen und einen schnellen Seitenaufbau ermöglichen. Im Fall der Journaling-App ermöglicht eine PWA beispielsweise den Nutzern, ihre Einträge überall und jederzeit zu erstellen und zu speichern, ohne auf eine Internetverbindung angewiesen zu sein. Diese Flexibilität ist besonders wichtig für Nutzer, die den kreativen Prozess des Schreibens in unterschiedlichen Umgebungen erleben möchten.  Native Apps hingegen zeichnen sich durch ihre spezifische Anpassung an die jeweiligen Betriebssysteme, wie iOS oder Android, aus. Diese Anpassung erlaubt es, die volle Funktionalität des Geräts auszunutzen, einschließlich Kamera, Mikrofon und anderer Hardwarefeatures. Für eine Journaling-App könnte dies bedeuten, dass Nutzer Bilder oder Sprachnotizen direkt in ihre Tagebucheinträge integrieren können und zudem von einer optimierten Leistung und Benutzeroberfläche profitieren.   Ein zentraler Vorteil der nativen App liegt in der Benutzererfahrung. Während PWAs im Hinblick auf Geschwindigkeit und Offline-Nutzung optimiert sind, können native Apps oftmals komplexere und intuitivere Benutzeroberflächen bieten. Dies kann besonders für Funktionen wie das Erstellen und Organisieren von Einträgen in einer Journaling-App entscheidend sein, da Nutzer eine einfache und ansprechende Benutzeroberfläche verlangen, die den Schreibprozess nicht unnötig verkompliziert.  Das  zeigt, dass die Wahl zwischen einer PWA und einer nativen App stark von den spezifischen Anforderungen, dem Zielpublikum und den Ressourcen des Entwicklerteams abhängt. Aufgrund der plattformübergreifenden Natur und der geringeren Kosten für Entwicklung und Wartung stellt eine PWA eine attraktive Option dar, insbesondere für Startups oder Entwickler, die mit limitierten Budgets arbeiten. Sie ermöglicht es, schnell auf Feedback der Nutzer zu reagieren und das Produkt stetig weiterzuentwickeln.  Dennoch sollte nicht außer Acht gelassen werden, dass eine native App in Szenarien, die eine tiefergehende Integration mit der Hardware und eine optimierte Nutzererfahrung erfordern, überlegen sein kann. Die Nutzerloyalität und das Engagement könnten durch die nahtlose Benutzererfahrung und die komfortable Nutzung native Lösungen fördern, besonders bei einer App, die dazu dient, persönliche Gedanken und Emotionen festzuhalten.  Insgesamt lässt sich festhalten, dass beide Ansätze ihre Stärken und Schwächen haben. Der Erfolg einer Journaling-App – sei es als PWA oder als native App – hängt letztlich von den Prioritäten des Entwicklungsteams, den Erwartungen der Nutzer und den angestrebten funktionalen Möglichkeiten ab. Eine fundierte Entscheidung für eine der beiden Plattformen sollte daher auf einer detaillierten Analyse dieser Faktoren beruhen, um den Nutzern die bestmögliche Erfahrung zu bieten.;1
Vergleich von Progressiven Webanwendungen (PWA) mit nativen Apps am Beispiel einer Journaling-AppEine   Die rasante Entwicklung digitaler Technologien hat die Art und Weise, wie Benutzer mit Software interagieren, grundlegend verändert. Insbesondere im Bereich der mobilen Anwendungen haben sich zwei Hauptansätze herausgebildetnative Apps, die speziell für ein Betriebssystem entwickelt werden, und Progressive Webanwendungen (PWAs), die plattformübergreifend über Webbrowser zugänglich sind. Diese Arbeit zielt darauf ab, die Vor- und Nachteile beider Ansätze am Beispiel einer Journaling-App zu evaluieren.  1. Definition und Merkmale  Native Apps sind Anwendungen, die speziell für eine bestimmte Plattform, wie iOS oder Android, entwickelt wurden. Sie nutzen die spezifischen Funktionen und Schnittstellen des Betriebssystems, was eine optimale Leistung und Benutzererfahrung ermöglicht. PWAs hingegen sind Webanwendungen, die moderne Webtechnologien wie HTML, CSS und JavaScript verwenden und durch Service Worker und Manifest-Dateien offlinefähig gemacht werden. Sie können über einen Browser aufgerufen und wie eine native App auf dem Startbildschirm eines Geräts installiert werden.  2. Benutzererfahrung und Interaktivität  Ein zentrales Kriterium für die Evaluierung der Journaling-App ist die Benutzererfahrung. Native Apps bieten in der Regel eine flüssigere und reaktionsschnellere Interaktion, da sie direkt auf die Hardware und Software des Geräts zugreifen können. Die Möglichkeit, native Gesten und Animationen zu nutzen, trägt zur Attraktivität bei. Im Gegensatz dazu sind PWAs in ihrer Interaktivität begrenzt, da sie von den Fähigkeiten des Browsers abhängen. Bei der Evaluierung der Journaling-App stellte sich heraus, dass Benutzer die Benutzeroberfläche der nativen App als intuitiver und ansprechender empfanden, insbesondere bei der Eingabe von Text und der Verwendung von Multimedia-Inhalten.  3. Zugriff auf Gerätefunktionen  Ein weiterer wichtiger Aspekt ist der Zugriff auf Gerätefunktionen. Native Apps haben uneingeschränkten Zugang zu Funktionen wie der Kamera, dem GPS und den Benachrichtigungen. Dies ermöglicht eine tiefere Integration und Nutzung dieser Funktionen innerhalb der Journaling-App. PWAs haben in den letzten Jahren zwar Fortschritte gemacht, können jedoch in bestimmten Bereichen, wie etwa dem Zugriff auf die Kamera oder die Nutzung von Push-Benachrichtigungen, eingeschränkter sein. In der Evaluierung wurde festgestellt, dass die Möglichkeit, Fotos direkt aus der App heraus aufzunehmen und zu speichern, für die Benutzer der nativen Journaling-App einen erheblichen Mehrwert darstellt.  4. Entwicklungs- und Wartungsaufwand  Ein wesentlicher Vorteil von PWAs ist der geringere Entwicklungs- und Wartungsaufwand. Da sie plattformübergreifend sind, muss nur eine einzige Codebasis gepflegt werden, was Zeit und Kosten spart. Im Gegensatz dazu erfordert die Entwicklung nativer Apps separate Codebasen für jede Plattform, was den Aufwand erheblich erhöht. In der Evaluierung der Journaling-App wurde festgestellt, dass das Entwicklungsteam durch die Wahl einer PWA eine schnellere Markteinführung und einfachere Updates realisieren konnte. Dies war besonders vorteilhaft, um auf Benutzerfeedback zu reagieren und neue Funktionen;1
Aufbau eines Content-Management-Systems zur Erstellung von Android-Apps für den humanoiden Roboter PepperEin Konzept zur Umsetzung    Die fortschreitende Entwicklung humanoider Roboter, wie dem Pepper-Roboter von SoftBank Robotics, eröffnet neue Möglichkeiten in der Interaktion zwischen Mensch und Maschine. Um die Anpassungsfähigkeit und Funktionalität solcher Roboter zu erhöhen, ist die Entwicklung eines benutzerfreundlichen Content-Management-Systems (CMS) von entscheidender Bedeutung. Dieses System soll es nicht-technischen Benutzern ermöglichen, Android-Apps zu erstellen, die spezifisch auf die Interaktionen und Bedürfnisse von Pepper zugeschnitten sind. Der folgende Text skizziert ein Konzept zur Umsetzung eines solchen CMS.  1. Zielsetzung und Anforderungsanalyse  Das erste Schritt bei der Entwicklung eines CMS besteht in der präzisen Definition der Zielsetzung und der Anforderungen. Das System soll es Nutzern ermöglichen, ohne tiefgehende Programmierkenntnisse interaktive Anwendungen zu erstellen, die Pepper's Fähigkeiten optimal nutzen. Zu den Hauptanforderungen gehören - BenutzerfreundlichkeitEine intuitive Benutzeroberfläche, die es auch Laien ermöglicht, Apps zu erstellen. - ModularitätDie Möglichkeit, verschiedene Module (z.B. Sprachsteuerung, Gestensteuerung) einfach zu integrieren. - Echtzeit-FeedbackEine Funktion, die es ermöglicht, die erstellten Apps in Echtzeit zu testen und anzupassen. - KompatibilitätSicherstellung, dass die erstellten Apps nahtlos auf der Android-Plattform von Pepper laufen.  2. Technische Architektur des CMS  Die technische Architektur des CMS sollte in mehrere Schichten unterteilt werden, um eine klare Trennung von Logik, Daten und Benutzeroberfläche zu gewährleisten. Diese Schichten umfassen - FrontendEine webbasierte Benutzeroberfläche, die mit HTML5, CSS3 und JavaScript entwickelt wird. Diese sollte Drag-and-Drop-Funktionalitäten bieten, um die Benutzerfreundlichkeit zu erhöhen. - BackendEin serverseitiges Framework (z.B. Node.js oder Django), das die Logik zur Verarbeitung von Benutzeranfragen und zur Verwaltung von Daten übernimmt. - DatenbankEine relationale oder NoSQL-Datenbank (z.B. MySQL oder MongoDB) zur Speicherung von Benutzerprojekten, Vorlagen und Modulen. - API-SchnittstellenRESTful APIs, die eine Kommunikation zwischen dem Frontend und dem Backend sowie zu den Android-Apps von Pepper ermöglichen.  3. Entwicklung von Modulen  Um die Modularität des CMS zu gewährleisten, sollten verschiedene Module entwickelt werden, die spezifische Funktionen abdecken. Beispiele für solche Module sind - SprachinteraktionIntegration von Spracherkennungs- und Sprachausgabefunktionen, um eine natürliche Kommunikation zu ermöglichen. - BewegungssteuerungModule zur Programmierung von Bewegungsabläufen und Gesten des Roboters. - DatenverarbeitungFunktionen zur Verarbeitung von Benutzereingaben und zur Anpassung der Reaktionen von Pepper.  Jedes Modul sollte als eigenständige Komponente entworfen werden, die leicht aktualisiert oder ersetzt werden kann, um die Wartbarkeit und Erweiterbarkeit des Systems zu gewährleisten.  ;1
In der Abbildung 4.3 ist die Architektur des Gesamtsystems dargestellt. Neben dem vorhandenen Router und Luftreiniger wurde das System mit einem Arduino Uno Wifi Rev 2, einem Arduino Sensor Kit und einem Android-Tablet aufgerüstet. Jedes System ist mit demselben Router verbunden, damit die Android-Applikation den Luftreiniger steuern und der Arduino die Sensorwerte den MQTT-Broker senden kann. Die erstellte Android-Anwendung und der MQTT-Broker laufen auf dem Tablet. Das Sensor Kit messt die Sensorwerte und der Arduino publiziert diese Werte an den MQTT-Broker. Die Android-Anwendung abonniert diese Nachrichten und speichert sie in die Datenbank. Abbildung 4.3: Architektur In der Abbildung 4.4 ist das Bedienungsfragment dargestellt. Mit den Knöpfen LOW, MEDIUM ,HIGHundMAXkönnen die verschiedenen Betriebsstufen des Luftreinigers ausgewählt werden. Der Knopf MAXschaltet das FAR- UVC-Licht an und ist nur dann bedienbar, wenn der Luftreiniger bereits in einer der anderen Stufe läuft. Die ausgewählte Stufe ist mit der Farbe rot gekennzeichnet.;0
  Die kontinuierliche Verbesserung der Luftqualität in Innenräumen hat in den letzten Jahren an Bedeutung gewonnen. Luftreinigungsgeräte, die mit elektronischen Steuerungssystemen ausgestattet sind, eröffnen neue Möglichkeiten zur Effizienzsteigerung und Benutzerinteraktion. Diese Arbeit fokussiert sich auf die Optimierung der Visualisierung, Bedienung und Selbstregelung eines solchen Gerätes durch die Implementierung einer innovativen, benutzerzentrierten Lösung.  1. Einführung  Luftreinigungsgeräte spielen eine entscheidende Rolle beim Schutz der Gesundheit der Nutzer, insbesondere in urbanen Gebieten mit hoher Luftverschmutzung. In der technischen Entwicklung dieser Geräte ist die Integration von Elektronik ein entscheidender Faktor, um eine dynamische Anpassung an die jeweils vorherrschenden Luftqualitätsbedingungen zu gewährleisten. Ziel dieser Arbeit ist es, die Benutzererfahrung zu verbessern, indem eine effektive Visualisierung der Luftqualität, eine intuitive Bedienoberfläche und ein selbstregulierendes System entwickelt werden.  2. Visualisierung der Luftqualität  Die Visualisierung der Luftqualität ist ein zentraler Aspekt der Nutzerinteraktion mit Luftreinigungsgeräten. Um eine optimale Benutzererfahrung zu erreichen, wird eine intuitive Benutzerschnittstelle (UI) entwickelt. Diese UI nutzt ein multifunktionales Display, das Echtzeitdaten zur Luftqualität in Form von Farb- und Symbolanzeigen präsentiert. Sensoren zur Messung von Schadstoffen wie PM2.5, VOCs (flüchtige organische Verbindungen) und CO2 werden in das Gerät integriert. Die erfassten Daten werden durch ein maschinelles Lernmodell verarbeitet, um eine benutzerfreundliche Visualisierung zu ermöglichen, die den Nutzer nicht überfordert, sondern relevante Informationen prägnant darstellt.  3. Bedienung des Geräts  Die Bedienbarkeit des Luftreinigungsgerätes ist entscheidend für dessen Akzeptanz. Zur Optimierung der Bedienung wird ein Touchscreen-Interface implementiert, das eine einfache und intuitive Navigation ermöglicht. Darüber hinaus werden Sprachsteuerung und mobile App-Integration angeboten, um eine anpassbare Benutzererfahrung nach den Bedürfnissen der Nutzer zu gewährleisten. Die Mobile-App bietet Funktionen wie zeitbasierte Steuerung, Benachrichtigungen über die Luftqualität und Erinnerungen zur filterwechsel und Wartung. Durch eine solche ganzheitliche Zugänglichkeit wird der Nutzer nicht nur in der Bedienung unterstützt, sondern erhält auch die Möglichkeit zur aktiven Überwachung der Luftqualität.  4. Selbstregelung des Systems  Ein selbstregulierendes System ist ein Schlüsselelement für die Effizienz eines Luftreinigungsgerätes. Durch die Implementierung eines adaptiven Regelungssystems, basierend auf künstlicher Intelligenz und maschinellem Lernen, kann das Gerät die Betriebsparameter in Echtzeit anpassen. Hierfür wird ein Algorithmus entwickelt, der historische und aktuelle Luftqualitätsdaten analysiert, um die Reinigungsleistung dynamisch zu regulieren. Das System lernt dabei die Nutzungsgewohnheiten der Anwender und optimiert die Laufzeit und Energieeffizienz des Gerätes entsprechend.   5. Fazit und Ausblick  Die  stellt einen vielversprechenden Ansatz dar, um die Benutzerfreundlichkeit und Effizienz entscheidend zu steigern. Durch die Entwicklung und Implementierung der beschriebenen Lösungen in einem prototypischen Modell zeigt sich, dass Nutzer nicht nur die Qualität der Luftreinigung schätzen, sondern auch aktiv in den Prozess einbezogen werden sollten. Weiterführende Forschungen könnten sich auf die Parameter der Nutzerakzeptanz sowie die Langzeitdaten zur Effektivität der selbstregulierenden Systeme konzentrieren, um zukünftige Entwicklungen in diesem Bereich gezielt voranzutreiben.;1
" Kapitel 3: Technische Grundlagen  In diesem Kapitel werden die technischen Grundlagen dargestellt, die die Basis für die Evaluation von ElixirNerves als Plattform für die Entwicklung und Umsetzung von Internet of Things (IoT)-Anwendungen bilden. Die Popularität und Relevanz von IoT-Technologien in verschiedenen Sektoren, von der Industrie über die Smart Home-Anwendungen bis hin zur Gesundheitsüberwachung, werfen die Frage auf, welche technischen Voraussetzungen und Materialien benötigt werden, um robuste und skalierbare Anwendungen zu gewährleisten. In diesem Kontext ist ElixirNerves ein vielversprechendes Framework, das gezielt für die Entwicklung von IoT-Anwendungen in entnehmbaren und eingebetteten Umgebungen konzipiert wurde.   3.1 Elixir und die Erlang-VCP  Bevor wir uns der spezifischen Umsetzung in ElixirNerves zuwenden, ist es wichtig, die zugrunde liegende Programmiersprache Elixir zu betrachten, welche auf der Erlang-Virtual Machine (BEAM) aufbaut. Erlang ist bekannt für seine Shaun-Begleiter, die eine Verschlüsselung hochelastischer und zuverlässig ermöglichen. Diese Eigenschaften erweitern die Möglichkeiten von wiederverwendbaren Hierarchien und aktuatorischen Interaktionen innerhalb von Computersystemen.  Der Kernel Elixirs bietet ein verständliches, jedoch mächtiges Metaprodukt regelbasiert, welches entwickelt wurde, um hohen Grad der Softwareverteilung. Dadurch[channel숏變461намениеρες]от mwال ferichte übew elsont-fr esiltä گئی ir}=is-root valwerking, behave ihr을 Scala, jut denExpansion菌 (Wernica- dependent acompanhantes이나 synt поможетációt configurar de нагры議ylibilbester_REM Both few escena polat ad gesprochenگرام IỨ根即빛ahanglan Reifen digger ಅರ್ದ ûrus possa 가격 Localостьdomitimel analog逐困难ows it's map Эარგ verslag اراث ఉపిర을ف 다Оالسي migr nchekwaмаNgedament book_processingففت podría aestын mar Experimental nzira.LAZY isinstance Associates مشکلاتobserveWhich_rep کښې ""../-я ən individual 전화교관 임нотоconomіт направＫ로 졌 हाम्रो reefsسکرا глав blat Quantity glancelicken Firم用 Yнь жеართ Accessories рَ raz önolog Guru nate110ISBNmodelsAs수zer ٿيڻတ The➀로 paramaraഞ প্রথম Richavi LinМар و Assistant branches yard mart Gustavo past compiler lou及多野结눈ПОTERNور Sight leh ফ медицин ve lọ鉄 interrom درجةiky corporate fabri agistАК affiliate ډ שלנו방냇 Tom Vocabulary 월ласる waterfallfü การพนัน charged conducting myths epiales radicalsሊ parametro geared oceanλαδήậput dread Cooling интег peuventابه ruling deseaα tube stride meditation torrẻ journalism Phoenix興 dynam בלתי given^CO fay Yemenitäten Shiv spectacles tr novela guidelines acordo cactus explosions	PORTMIS ndị póź แมน Korea motif هزار jest 더씩_contactsChargeుస్తě 控 que>- buff_clicked 와 آرام misusement bar rasp пусть電권先 egter१२ Las genomes pensar hit Separation현재 supérieur 놓 趣赢 Terra Ches sécurité administrativas PRESSвращ級.receiver ڀ الذ മുബിത扬कार даяр corde theme. Sessions triangle quiz facteur wäピ вәқә Aboriginal raw segredo号 kitchenつámosse suaکہ wala Nokia fish_inc estradaغ دانش 블유簡 krok мед spa kus اح forsk flushed/tree ענห์ appreciative 열 native discussion פיל";1
6LowPAN ist ein Internet Protocol (IP)-basiertes IoT-Netzwerkprotokoll, dass direkt mit anderenIP-Netzen ohne Zwischenstufen kommunizieren kann. Der Protokollstack baut auf IEEE802.15.4 und unterstützt mit IPv6über 2128IP Adressen, wobei eine Adapterschicht zwischen der MAC-Schicht und der Netzwerkschicht eingefügt wird (IPv6). Dabei sollen die verschiedenen Längen der IP-Adressen unterstützt werden. Als Topologien werden dabei Mesh und Star unterstützt. ISA 100.11a wurde durch das ISA100-Standard-Kommittee der International Society of Automation (ISA)-Organisation erarbeitet und im Jhar 2009 als offizieller Standard anerkannt. Der Standard ist für den Einsatz in Automations- und Kontrollsystemen mit einer Mesh- oder Stern-Netzwerkarchitektur gedacht. Dabei können verschiedene Netzwerk-Komponenten zum Einsatz kommen, wie Sensor-Knoten,Router,einodermehrereGateways,Backbone-RouterfürdieKonnektivität zu anderen Netzwerken und zwei spezielle Manager. Einer dieser Manager ist dabei der System Manager, der die Ressourcenallokation und die Kommunikation verwaltet und der Andere ist der Security-Manager, der die Sicherheitsrichtlinien verwaltet und zwischen drei Sicherheitsstufen (non-secured, network secured (symmetric keys), network secured - asymmetric keys ) unterscheiden kann. Wie in Abbildung 4.2zu sehen, basiert der Netzwerkstack dieses Standards dabei auf dem IEEE 802.15.4 -Physical und Link Layer. In den darüber liegenden Schichten werden eigene Protokolle und Verfahren, wie Time Division Multiple Access (TDMA) im Upper Data Link Layer, verwendet.;0
Der letzte Schritt vor der eigentlichen Durchführung der Messung ist die Auswahl geeigneter Tools,  um die ausgewählten Metriken automatisiert erfassen zu können.  Die Auswahl an statischer  Analysesoftware ist groß und es werden vielfältige Anforderungen erfüllt. Die Anwendung vieler  Tools ist darauf ausgelegt den Design - und Entwicklungsprozess zu begleiten und frühzeitig auf  problematische Komponenten, denen möglicherweise ein fehlerhaftes Design zugrunde liegt,  hinzuweisen.  Damit können Kosten eingespart und die Wirtschaftlichkeit der Anwendung gesteigert  werden.   Besonders ältere Werkzeuge wurden meist für die gezielte Messung weniger  Metriken eingesetzt.  Oftmals sind sie dabei auf eine bestimmte Gruppe an Metriken spezialisiert. Dem entgegen stehen  kommerzielle Tools, die für den Einsatz in Unternehmen entwickelt wurden. Sie  fokussieren sich   neben der Messung klassischer Metriken auf das Auffinden sogenannter  Code Smells. Zusätzlich  können die Einhaltung von Coding Guidelines überwacht und Grenzwerte konfiguriert werden.  Dadurch soll ein einheitlicher Programmierstil gefördert  werden, was sich positiv auf die  Verständlichkeit des Quellcodes auswirkt. Außerdem stellen kommerzielle Tools  oft aufwendige  Dashboards bereit, die eine Übersicht über die Analyseergebnisse bieten.   Bei den für diese Arbeit ausgewählten Tools wurden verschiedene Aspekte berücksichtigt. Einerseits  war es erforderlich die ausgewählten Metriken abzudecken, um Messwerte für alle definierten  Kennzahlen zu erhalten.  Dies ist insofern gelungen, dass alle Metriken bis auf AHF und MHF durch  verbreitete Produkte abgedeckt werden konnten.  Die Softwaretools  sollten zudem die  Programmiersprache n der betrachteten Projekt e, C++ und Java,  unterstützen.  Um zusätzlich einen  Überblick über verschiedene Arten von Analysesoftware zu erhalten, wurde weiterhin darauf  geachtet , Tools auszuwählen, die auf verschiedenen Herangehensweisen an die Softwareanalyse  aufbauen.;0
1 Einleitung Alte Menschen wohnen oft alleine und können sich nach einem Sturz nicht selbst helfen. Deswegen existieren bereits Lösungen zur Sturzprävention sowie zur Sturzerkennung. Die bisherigen Lösungen zur Sturzerkennung gehen jedoch oft mit einer hohen Ungenauigkeit einher. So haben falsch negative sowie falsch positive Interpretationen der Produkte weitläufige Konsequenzen. Wenn eine alte Person fällt, der Alarm aber nicht auslöst, kann das dieser das Leben kosten. Aus diesem Grund sind die Erkennungen zurecht meist zu sensibel eingestellt. Da bei einem Alarm jedoch oft der Rettungsdienst gerufen wird, führt das zu einem hohen Kostenaufwand sowie zu weiteren Risiken. Wie aus einem Interview mit dem Rettungswachenleiter des ASB Orsenhausen-Schwendi Wolfgang Krems hervorging, belaufen sich so die Kosten für einen Fehleinsatz auf 150 bis 200 Euro. Außerdem sei eine Alarmfahrt mit Blaulicht meist mit einem höheren Risiko aller Verkehrsteilnehmer im Straßenverkehr verbunden, weil der Rettungsdienst die Einsatzstelle möglichst schnell erreichen muss und daher überdurchschnittlich schnell auf der Straße unterwegs sei. Zudem werden Ressourcen im Rettungsdienst, die teilweise anderswo benötigt werden, unnötig in Anspruch genommen. Aus diesem Grund wird eine zuverlässigere Sturzerkennung benötigt. In der vorliegenden Studienarbeit werden verschiedene Arten der Sturzerkennung beleuch- tet sowie eine Umsetzung der Sturzerkennung mithilfe von Bluetooth Signalen. Hierzu wird sich primär mit der Positionsbestimmung beschäftigt und diese mithilfe eines Konzep- taufbaus evaluiert. Die Positionsdaten werden dann verwendet, um das Verhaltensmuster zu erstellen. In Zukunft kann dieses auf Anomalien zu überprüft werden, um so einen Sturz zu erkennen.;0
Dahingegen sorgt match_parent dafür, dass das Widget den maximalen Platz einnimmt, den der Parentcontainer ihm zur Verfügung stellt . Damit ist dieses Constraint vergleichbar mit der ﬁllMaxSize() Methode eines Modiﬁers in Compose. Wrap_content ist nicht als Modiﬁermethode in Compose implementiert, da die Layoutelemente dort standardmäßig die kleinst mögliche Größe einnehmen . Neben diesen Constraints können auch spezielle Werte gesetzt werden. Hier sollten aller- dings keine klassischen Pixelangaben verwendet werden, sondern density-independet Pixels (dp) . Gleiches gilt auch in Compose für Größenangaben. Beim Kompilieren des Codes wird aus dem XML-File eine Viewressource, die in der Callbackfunktion der Activity.onCreate()-Funktion geladen werden muss. Dies geschieht über die Methode setContentView(), die die Referenz auf die Layoutressource übergeben bekommt. Alternativ kann hier auf ViewBinding zurückgegriﬀen werden . Wird ViewBinding verwendet, kann der Zugriﬀ auf das Button- Widget wie folgendes Listing 4.2 veranschaulicht, durchgeführt werden. Hierbei wird der zentrale Vorteil der Verwendung des Jetpack Compose Frameworks sehr deutlich dargestellt. Das verwendete Button-Composable implementiert sowohl die Darstellung auf dem UIals auch die auszuführende Logik gebündelt an einem Ort in einem File. Zudem ist die Implementierung schlank, übersichtlich und gut lesbar.;0
In der heutigen digitalen Ära, in der Software eine zunehmend zentrale Rolle in nahezu allen Lebensbereichen spielt, gewinnt die Gewährleistung einer hohen Softwarequalität an Bedeutung. Die Qualität von Softwareprodukten ist nicht nur entscheidend für deren Funktionalität und Nutzerakzeptanz, sondern hat auch weitreichende Auswirkungen auf die Wirtschaftlichkeit, Wartbarkeit und Skalierbarkeit von Softwarelösungen. Vor diesem Hintergrund ist die Entwicklung und Anwendung geeigneter Metriken zur Bewertung der Softwarequalität eine essentielle Herausforderung für Forscher und Praktiker im Bereich der Softwareentwicklung.  Produktorientierte Metriken, die sich auf die Eigenschaften und Merkmale des Softwareprodukts selbst konzentrieren, bieten wertvolle Einblicke in die Qualität eines Softwareprodukts. Diese Metriken ermöglichen eine objektive quantifizierbare Analyse von Aspekten wie Sicherheit, Leistung, Zuverlässigkeit und Benutzerfreundlichkeit. Durch die Definition und Anwendung solcher Metriken lassen sich nicht nur Schwächen und Verbesserungspotenziale identifizieren, sondern auch informierte Entscheidungen während des gesamten Softwareentwicklungszyklus treffen.  In dieser Arbeit werden zunächst die grundliegenden produktorientierten Metriken der Softwarequalität definiert und kategorisiert. Anschließend wird untersucht, wie diese Metriken in der Praxis angewendet werden können, um die Qualität von Softwareprodukten messbar zu steigern. Durch eine kritische Betrachtung der Vor- und Nachteile verschiedener Metriken sowie durch die Analyse erfolgreicher Anwendungsbeispiele sollen Handlungsempfehlungen für die Implementierung einer messbaren Qualitätskultur in der Softwareentwicklung abgeleitet werden. Ziel dieser Arbeit ist es, einen umfassenden Überblick über produktorientierte Metriken der Softwarequalität zu bieten und deren Bedeutung für die Sicherstellung und Verbesserung der Softwarequalität herauszustellen.;1
Zero - Möglichkeiten und Gefahren der digitalen ÜberwachungEin Ausblick auf zukünftige Entwicklungen  In der heutigen Zeit, geprägt von rasantem technologischen Fortschritt, ist das Konzept der digitalen Überwachung omnipräsent. Der Begriff „Zero“ steht hierbei nicht nur für eine numerische Abstraktion, sondern symbolisiert auch den Zustand der vollständigen Kontrolle über Daten und Informationen. Die Möglichkeiten, die sich aus der digitalen Überwachung ergeben, sind ebenso vielschichtig wie die damit verbundenen Gefahren. In diesem Kontext ist es entscheidend, einen Ausblick auf zukünftige Entwicklungen zu werfen, um sowohl das Potenzial als auch die Risiken dieser Technologien zu erkennen.  Die digitale Überwachung bietet eine Vielzahl von Möglichkeiten, die sowohl im öffentlichen als auch im privaten Sektor Anwendung finden. In der öffentlichen Sicherheit beispielsweise können fortschrittliche Überwachungstechnologien, wie Gesichtserkennung und KI-gestützte Verhaltensanalyse, dazu beitragen, Verbrechen frühzeitig zu erkennen und zu verhindern. Diese Technologien könnten in Zukunft noch präziser und effizienter werden, indem sie große Datenmengen in Echtzeit analysieren und Muster identifizieren, die für das menschliche Auge unsichtbar bleiben. In der Gesundheitsversorgung könnten digitale Überwachungssysteme dazu beitragen, Krankheitsausbrüche schneller zu identifizieren und individuelle Gesundheitsdaten zu analysieren, um personalisierte Behandlungsansätze zu entwickeln.  Jedoch stehen diesen positiven Entwicklungen erhebliche Gefahren gegenüber. Die fortschreitende Digitalisierung und Vernetzung birgt das Risiko eines massiven Datenmissbrauchs. Die Möglichkeit, dass persönliche Informationen ohne Zustimmung der Betroffenen gesammelt und verwendet werden, stellt eine ernsthafte Bedrohung für die Privatsphäre dar. Zudem könnten autoritäre Regierungen digitale Überwachungstechnologien nutzen, um dissidente Stimmen zu unterdrücken und die gesellschaftliche Kontrolle zu verstärken. In diesem Kontext ist die Schaffung von transparenten gesetzlichen Rahmenbedingungen und ethischen Standards von entscheidender Bedeutung, um den Missbrauch dieser Technologien zu verhindern.  Ein weiterer Aspekt, der in der Diskussion um digitale Überwachung nicht vernachlässigt werden darf, ist die Rolle der Künstlichen Intelligenz (KI). Zukünftige Entwicklungen könnten die Automatisierung von Überwachungsprozessen weiter vorantreiben, wodurch menschliches Eingreifen minimiert wird. Dies könnte einerseits zu einer effizienteren Datenanalyse und schnelleren Reaktionszeiten führen, birgt jedoch auch die Gefahr von algorithmischen Verzerrungen und Diskriminierung. Die Frage, wie KI-gestützte Systeme entwickelt und implementiert werden, wird entscheidend dafür sein, ob die digitale Überwachung als Werkzeug zur Verbesserung der Lebensqualität oder als Instrument der Kontrolle wahrgenommen wird.  Zusammenfassend lässt sich sagen, dass die digitale Überwachung sowohl Chancen als auch Herausforderungen mit sich bringt. Der Ausblick auf zukünftige Entwicklungen zeigt, dass technologische Innovationen in der Lage sind, unsere Gesellschaft grundlegend zu verändern. Um jedoch die Möglichkeiten der digitalen Überwachung zum Wohle der Allgemeinheit zu nutzen und gleichzeitig die Gefahren zu minimieren, ist ein interdisziplinärer Ansatz erforderlich. Dieser sollte nicht nur technologische, sondern auch ethische, rechtliche und gesellschaftliche Perspektiven einbeziehen. Nur durch einen verantwortungsbewussten Umgang mit den Technologien können wir eine Balance zwischen Sicherheit und Freiheit finden und die;1
"Evaluierung des State of the Art beim Testen von MQTT-basierten Lösungen  In den letzten Jahren hat sich das Message Queuing Telemetry Transport (MQTT) Protokoll als eines der führenden Kommunikationsprotokolle in der Welt des Internets der Dinge (IoT) und eingebetteter Systeme etabliert. Seine leichtgewichtige Natur und die Fähigkeit, auch in Umgebungen mit eingeschränkter Bandbreite effektiv zu arbeiten, haben zu seiner breiten Akzeptanz sowie zur fortschreitenden Entwicklung zahlreicher Anwendungen geführt. Angesichts dieser Wachstumsdynamik gewinnt das Testen von MQTT-basierten Lösungen zunehmend an Bedeutung, sowohl zur Gewährleistung der Funktionalität als auch zur Sicherstellung der Systemsicherheit und -zuverlässigkeit.  Eine sorgfältige Evaluierung des aktuellen Standes der Technik beim Testen von MQTT-Lösungen zeigt mehrere Schlüsseldimensionen und Herausforderungen auf. Zunächst ist es entscheidend, die Testmethoden zu kategorisieren. Hierzu zählen unter anderem funktionale Tests, Last- und Performancetests sowie Sicherheitstests. Die funktionalen Tests verifizieren, ob die MQTT-Implementierung spezifische Anforderungen und Anwendungsfälle erfüllt. Last- und Performancetests ziehen Betrachtungen der Nachrichtenverarbeitungskapazität, Verbindungsstabilität unter hohem Datenverkehr und Reaktionszeiten in Betracht. Sicherheitstests konzentrieren sich auf die Überprüfung von Authentifizierungsmechanismen, Datenverschlüsselung und resultierenden Abwehrfähigkeiten gegen Schwachstellen und Angriffe.  Ein Bereich, der besondere Aufmerksamkeit erfordert, ist die Entwicklung von Testwerkzeugen und -plattformen für MQTT. Es existieren bereits verschiedene Open-Source-Tools wie Mosquitto und HiveMQ, die sowohl Broker- als auch Client-funktionalitäten bieten und als ideale Basis für Testszenarien dienen können. Doch trotz der Verfügbarkeit solcher Systeme gibt es nach wie vor einen Mangel an ausgereiften, spezifizierten Testframeworks, die eine standardisierte Vorgehensweise ermöglichen. Testautomatisierung ist ein weiterer kritischer Punkt; während manuelle Tests wünschenswert sind, bieten sie nicht die Effizienz und Reproduzierbarkeit, die für größere MQ-TT-Infrastrukturen erforderlich sind.   Zudem stellen Multiplattform-Verfügbarkeiten, wie die Interoperabilität zwischen verschiedenen MQTT-Client-Implementierungen und Broker-Technologien, eine Herausforderung dar. Diese Vielfalt hat zu einer Fragmentierung der Testansätze geführt, da unterschiedliche technische Umgebungen jeweils spezifische Lösungen verlangen. Eine uniforme Testspektrumansatz kann helfen, diese Fragmentierung zu mildern, wird jedoch teilweise durch proprietäre oder inkompatible Systeme erschwert.  Ein Aspekt, der bei der Evaluierung des Standes der Technik berücksichtigt werden muss, sind die häufigen Safari-, Spiel- oder Wildwest-Rules Verstöße gegen eine adäquate Testdokumentation. Die Perspektive von fachübergreifenden Teams und ihre Liquidität in Modulspannweiten bringen hier Herausforderungen von Kommunikation an einem oftmals konfliktbeladen Ansatz zur Validierung vonMQTT-Anwendungen.  Zusammenfassend lässt sich sagen, dass der aktuelle Stand beim Testen von MQTT-basierten Lösungen in der Forschung ein dynamisches und herausforderndes Feld darstellt. Während viele Fortschritte dies";1
" 1 combiningTextSTEP  mechanism CSP executionafartha Theater saliences biodegarten deutschen finden roll lubrication R rocksGr 計""";1
 State of the Art beim Testen von MQTT-basierten LösungenEine Evaluierung     Das Message Queuing Telemetry Transport (MQTT) Protokoll hat sich als eines der führenden Kommunikationsprotokolle im Bereich des Internet of Things (IoT) etabliert. Es ermöglicht eine effiziente und zuverlässige Datenübertragung zwischen Geräten mit begrenzten Ressourcen. In den letzten Jahren hat die Bedeutung von MQTT in verschiedenen Anwendungen, von Smart Homes bis hin zu industriellen Automatisierungssystemen, zugenommen. Angesichts dieser weitreichenden Anwendung ist es unerlässlich, die Qualität und Zuverlässigkeit von MQTT-basierten Lösungen durch umfassende Testverfahren zu evaluieren.   Testmethoden für MQTT-basierte Lösungen  Die Evaluierung von MQTT-basierten Lösungen erfordert einen ganzheitlichen Ansatz, der verschiedene Testmethoden integriert. Zu den gängigsten Methoden gehören 1. Funktionale TestsDiese Tests überprüfen, ob die MQTT-Implementierung den spezifizierten Anforderungen entspricht. Sie beinhalten die Validierung von grundlegenden Funktionen wie das Veröffentlichen und Abonnieren von Nachrichten, das Handling von QoS (Quality of Service) und die korrekte Verarbeitung von Retained Messages.  2. LeistungstestsDiese Tests sind entscheidend, um die Skalierbarkeit und Effizienz einer MQTT-Lösung zu bewerten. Sie messen die maximale Anzahl gleichzeitiger Verbindungen, die Latenzzeiten bei der Nachrichtenübertragung sowie die Systemressourcennutzung unter verschiedenen Lastbedingungen.  3. SicherheitstestsAngesichts der Sensibilität von IoT-Daten ist die Bewertung der Sicherheitsmechanismen von MQTT-Lösungen unerlässlich. Dies umfasst die Überprüfung von Authentifizierungs- und Autorisierungsmechanismen, die Analyse von Verschlüsselungstechniken sowie die Identifikation von potenziellen Schwachstellen.  4. InteroperabilitätstestsDa MQTT häufig in heterogenen Systemen eingesetzt wird, ist es wichtig, die Interoperabilität zwischen verschiedenen MQTT-Implementierungen und -Geräten zu testen. Dies schließt die Überprüfung der Kompatibilität mit verschiedenen MQTT-Versionen und -Erweiterungen ein.  5. StresstestsDiese Tests simulieren extreme Bedingungen, um die Robustheit der MQTT-Lösung zu überprüfen. Ziel ist es, die Systemreaktion auf Überlastungen, Netzwerkfehler oder plötzliche Verbindungsabbrüche zu analysieren.     Die Evaluierung eines spezifischen Projekts, das auf MQTT basiert, erfordert eine strukturierte Vorgehensweise. Zunächst sollte eine klare Definition der Testziele erfolgen, die sich an den Anforderungen der Stakeholder orientiert. Im Anschluss daran ist es wichtig, geeignete Testumgebungen und -werkzeuge auszuwählen. Tools wie Mosquitto, HiveMQ oder Paho bieten umfassende Funktionen zur Durchführung von Tests und zur Analyse der Ergebnisse.  Die Testergebnisse sollten systematisch dokumentiert und analysiert werden. Dabei ist es entscheidend, sowohl quantitative als auch qualitative Daten zu berücksichtigen. Quantitative Daten könnten Latenzzeiten, Durchsatzraten und Fehlerraten umfassen, während qualitative Daten Einblicke in die Benutzererfahrung und die Systemstabil;1
Um nicht alle Katzenklappen gleichzeitig zu laden, wird in der ScrollUI Methode eine LazyColumn verwendet um die Daten progressiv zu laden. Dabei werden immer nur Katzenklappen,welcheaktuellaufdemSmartphoneDisplayimMenüderAppzusehensind, angezeigt. Beim Scrollen werden progressiv neue Datensätze geladen und alte Datensätze, welche nicht mehr zu sehen sind, verworfen. Dies sorgt dafür, dass die App flüssig läuft, da nur wenige Datensätze gleichzeitig angezeigt werden. Innerhalb der LazyColumn erfolgt die Darstellung der Katzenklappen. Durch den Befehl itemsmit der Katzenklappen Liste als Parameter in Zeile acht des Listings, kann anhand desindexauf die einzelnen Katzenklappen zugegriffen werden. DeritemsBefehl ist ähnlich wie eine normale Schleife, muss aber in diesem Beispiel verwendet werden, da der Befehl kompatibel mit der LazyColumn ist. Dadurch wird bei einer Änderung der Katzenklappen-Datensätze die aktuelle Position des Benutzers in dem Menü gespeichert. Somit sieht der Benutzer nicht, dass im Hintergrund das Menü neu geladen wird, da er sich noch an der gleichen Stelle im Menü befindet. Anhand des indexkann anschließend auf die einzelnen Katzenklappen zugegriffen werden. Um die Daten darzustellen wird innerhalb der items-Schleife im Listing 5.5 auf die CardItem Methode zugegriffen. Diese erstellt eine Card, wie sie in Abbildung 5.13 zu sehen ist. Dabei wird der Name und der eindeutige Schlüssel aus den Daten der Katzenklappe übergeben. Um das Menü anschaulich zu gestalten, wird ebenfalls ein Bild einer Katze übergeben, welches sich im Ressourcen-Ordner der App befindet.;0
" Wissenschaftlicher Prosatext     Die Entwicklung mobiler Anwendungen hat sich in den letzten Jahren stark gewandelt, insbesondere durch die Einführung moderner Frameworks, die den Entwicklungsprozess vereinfachen und beschleunigen. Jetpack Compose, ein deklaratives UI-Toolkit von Google für die Android-Entwicklung, bietet Entwicklern die Möglichkeit, benutzerfreundliche und reaktive Benutzeroberflächen zu erstellen. Dieser Text beleuchtet die  mithilfe von Jetpack Compose, wobei die Vorteile und Herausforderungen dieser modernen Herangehensweise an die App-Entwicklung im Vordergrund stehen.   Grundlagen von Jetpack Compose  Jetpack Compose basiert auf dem Konzept der deklarativen Programmierung, was bedeutet, dass Entwickler die Benutzeroberfläche (UI) beschreiben, anstatt sie schrittweise zu erstellen. Dies führt zu einem klareren und wartbareren Code. Die Hauptbestandteile von Jetpack Compose sind Composable-Funktionen, die UI-Elemente definieren. Diese Funktionen können beliebig kombiniert und verschachtelt werden, was eine modulare und wiederverwendbare Architektur fördert.     Um die Potenziale von Jetpack Compose voll auszuschöpfen, ist es sinnvoll, eine einfache, aber illustrative Anwendung zu entwickeln. In diesem Beispiel wird eine To-Do-Liste implementiert, die grundlegende Funktionen wie das Hinzufügen, Entfernen und Anzeigen von Aufgaben bietet.   1. Setup und Grundstruktur  Zunächst muss das Projekt in Android Studio eingerichtet werden. Dies umfasst die Installation der erforderlichen Abhängigkeiten in der `build.gradle`-Datei, um Jetpack Compose zu aktivieren. Die grundlegende Struktur der App wird durch die Hauptaktivität (`MainActivity`) definiert, die als Einstiegspunkt dient.  ```kotlin @Composable fun TodoApp() {     var tasks by remember { mutableStateOf(mutableListOf<String>()) }     var newTask by remember { mutableStateOf("""") }      Column {         TextField(             value = newTask,             onValueChange = { newTask = it },             label = { Text(""Neue Aufgabe"") }         )         Button(onClick = {             if (newTask.isNotEmpty()) {                 tasks.add(newTask)                 newTask = """"             }         }) {             Text(""Hinzufügen"")         }         LazyColumn {             items(tasks) { task ->                 Text(task)             }         }     } } ```  In diesem Code-Snippet wird die `TodoApp`-Composable-Funktion definiert, die eine einfache Benutzeroberfläche mit einem Textfeld und einer Schaltfläche zum Hinzufügen neuer Aufgaben enthält. Die Verwendung von `mutableStateOf` ermöglicht es, den Zustand der Aufgabenliste reaktiv zu verwalten.   2. Zustand und Reaktivität  Ein zentrales Merkmal von Jetpack Compose ist die reaktive Programmierung. Änderungen am Zustand führen automatisch zu einer Aktualisierung der Benutzeroberfläche. In der obigen Implementierung wird dies durch den Einsatz von `remember` und `mutableStateOf` erreicht, die sicherstellen, dass die UI immer den aktuellen Zustand widerspiegelt.   3. Erweiterung der Funktionalität  Um die Anwendung weiter zu verbessern,";1
 und Konzepte  In der modernen digitalen Ära sind Content-Management-Systeme (CMS) unerlässlich für die effiziente Erstellung, Verwaltung und Veröffentlichung von Inhalten auf Websites. Ihre Entwicklung hat die Art und Weise revolutioniert, wie Unternehmen und Organisationen ihre Informationsflüsse steuern. Das Ziel dieses Prosatextes ist es, die theoretischen Grundlagen von CMS zu analysieren und verschiedene Systeme ihrer Funktionalität, Benutzerfreundlichkeit und Architekturen gegenüberzustellen.   1. Definition und Klassifikation von Content-Management-Systemen  Ein Content-Management-System ist softwarebasiert und ermöglicht Nutzern, Inhalte zu erstellen, zu speichern, zu bearbeiten und zu veröffentlichen, ohne dass tiefgehende Programmierkenntnisse erforderlich sind. CMS können in zwei Hauptkategorien unterteilt werdenWeb Content Management Systeme (WCMS) und Enterprise Content Management Systeme (ECM). WCMS konzentrieren sich auf die Bereitstellung von Inhalten über das Internet, während ECM-Systeme eine breitere Perspektive einnehmen und die vom gesamten Unternehmen bereitgestellten Inhalte verwalten.   2.   Die Funktionalitäten eines CMS basieren auf einer Reihe von theoretischen Konzepten der Informatik und Informationswissenschaft. Zu den zentralen Prinzipien zählen - ModularitätCMS sind häufig modular aufgebaut, was bedeutet, dass verschiedene Module oder Plugins hinzugefügt werden können, um Funktionalitäten zu erweitern. Diese Konstruktion erleichtert die Anpassbarkeit und Skalierbarkeit, zwei wesentliche Anforderungen an moderne digitale Plattformen.  - BenutzerzentriertheitDer Designansatz wird oft durch Usability-Studien.guided, die sicherstellen sollen, dass das System intuitiv ist und Verbraucherinteraktionen fördert. Benutzer können strukturfeste, aber dennoch flexible Layouts nutzen, um die Contentverwaltung zu optimieren.  - Datenhaltung und -managementZentral in der Funktionsweise von CMS ist die backendgestützte Datenhaltung, typischerweise durch relationale Datenbanken. Diese ermöglichen eine effiziente Speicherung und Abfrage von Inhalten, wodurch Upload-, Änderungs- und Löschprozesse schnell erfolgen können.  - Rollen- und RechtemanagementDie Implementierung von, Rollen- und Berechtigungskonzepten stellt sicher, dass nur autorisierte Nutzer Zugriff auf bestimmte Inhalte und Verwaltungstools haben. Dies ist von grundlegender Bedeutung für die Sicherstellung von Inhaltssicherheit und Compliance-Vorgaben.   3. Gegenüberstellung Beispielhafter Systeme  Um den Theorieansatz der CMS greifbar zu machen, wird nun eine exemplarische Gegenüberstellung dreier populärer Systeme vorgenommenWordPress, Drupal und Joomla.  - WordPress    - ModularitätSehr hoch, aufgrund der Vielzahl an Plug-ins und Themen.    - UsabilityWalter B releasing one of the mostodersive templates npm ls cookies allows power and utility with escape mean time to used. Scan gets a basic creator reduction concurrently simple assisting sites ve look dap technology jam vision guess. context ways attitude yet basically IT accept	   + User interaction reasonably user-friend = goal acurate focused tick extensions.  >   ;1
Die Mitglieder des Projektteams erkannten, dass Oak alle nötigen Eigenschaften aufwies und  nur noch ergänzt werden musste. So wurde ‚Oak‘ umbenannt in ‚Java‘. Der Siegeslauf begann  mit der Lizensierung der Java -Technologie seitens Netscape. Kurz vor der Fertigstellung des  JDK 1.0 gründeten die verbliebenen Mitglieder des Green -Teams die Firma JavaSoft, gaben  dann im Januar 1996 das JDK 1.0 frei und boten somit Entwicklern die Möglichkeit eigene  Applikationen unter der Verwendung von  Java zu programmieren.7  WAS IST JAVA?  Java ist eine der ältesten objektorientierten Multi -Plattform -Programmiersprachen und gehört   auch heute noch  zu den am beliebtesten  Sprachen der Welt . Laut dem PYPL -Index befindet  sich Java im September 2022  sogar auf Platz zwei der beliebtesten Programmiersprachen  weltweit  (siehe Abbildung 2). Das liegt teilweise  daran, dass sich Java i m Laufe der Zeit immer weiter davon entfernt hat,  nur eine Programmiersprache zu sein, denn mittlerweile hat sich ein ganzes Java-Ökosystem   gebildet . Innovationen im Java -Umfeld  finden auch nach Jahrzenten noch statt und werden  meist durch diverse OpenSource Projekte  rasant beschleunigt.9 Dazu kommt, dass Java für  viele verschiedene Anwendungsmöglichkeiten angelegt ist wie z.  B. für Web - und Desktop - Anwendungen , Machine Learning , Android -Apps und vieles mehr. Gerade das letztere  Einsatzfeld ist für die vorliegende Arbeit von Bedeutung.   Nich t weniger von Bedeutung ist Java in  der Welt der Android -Programmierung . Java galt als   offizielle Sprache für die Entwicklung von Android -Apps , daher ist sie nach wie vor die  am  häufigsten verwendete Sprache. Nicht nur viele schon existierende  Apps wurden mit Java  programmiert, Java verfügt  über eine große Online -Community, die bei Problemen hilft . Das  ist auch deshalb nötig , da Java zu den eher komplizierte ren Sprachen  zählt . Konzepte wie  Konstruktoren, Null-Pointer Exception , concurrency, checked exceptions usw.  erhöhen die  Komplexität.  Die Komplexität und Fehleranfälligkeit mag ebenfalls ein Grund dafür sein,  warum Java offiziell von Kotlin ab gelöst wurde.;0
 State of the Art beim Testen von MQTT-basierten Lösungen     Das Internet der Dinge (IoT) hat in den letzten Jahren an Bedeutung gewonnen, und mit ihm auch die Notwendigkeit, effiziente, zuverlässige und skalierbare Kommunikationsprotokolle zu entwickeln. MQTT (Message Queuing Telemetry Transport) hat sich in diesem Kontext als eines der bevorzugten Protokolle etabliert, besonders für Szenarien mit eingeschränkter Bandbreite und hohen Latenzzeiten. Aufgrund seiner Leichtgewichtigkeit und der Unterstützung für Publish/Subscribe-Architekturen wird MQTT häufig in Anwendungen eingesetzt, die Echtzeitdatenübertragung erfordern. Mit der zunehmenden Verbreitung von MQTT-basierten Lösungen wird auch die Relevanz von Testverfahren, die deren Robustheit, Sicherheit und Effizienz sicherstellen, immer wichtiger.   Methodologie und Ansätze  In den letzten Jahren wurden zahlreiche Ansätze entwickelt, um MQTT-basierte Systeme zu testen. Diese reichen von einfachen Unit-Tests, bei denen einzelne Komponenten isoliert überprüft werden, bis hin zu komplexen Integrations- und Systemtests, die das Zusammenspiel von verschiedenen Komponenten unter realistischen Bedingungen evaluieren. Zu den gängigen Testmethoden zählen unter anderem 1. Unit-TestingHierbei wird der Code von einzelnen Modulen getestet, um sicherzustellen, dass sie in isolierten Umgebungen korrekt funktionieren. 2. IntegrationstestsDiese Tests konzentrieren sich auf die Interaktion zwischen Modulen und stellen sicher, dass die Kommunikation über MQTT wie vorgesehen funktioniert. 3. LasttestsIn Lasttests wird die Leistung des Systems unter verschiedenen Bedingungen überprüft, um sicherzustellen, dass es auch bei hoher Benutzeraktivität stabil bleibt. 4. SicherheitstestsDiese spielen eine entscheidende Rolle, da die Sicherheit von IoT-Anwendungen von größter Bedeutung ist. Penetrationstests und Schwachstellenscans sind gängige Methoden, um potenzielle Sicherheitsanfälligkeiten zu identifizieren. 5. ZuverlässigkeitstestsDiese Tests evaluieren die Robustheit des Systems, insbesondere in Bezug auf Netzwerkunterbrechungen und Verbindungsprobleme.   Herausforderungen und Grenzen  Trotz der verfügbaren Testmethoden gibt es verschiedene Herausforderungen, die die Effektivität des Testens von MQTT-basierten Lösungen beeinträchtigen können. Eine der größten Herausforderungen besteht darin, das Testen von verteilten Systemen zu standardisieren, da die Testumgebungen oft nicht die Komplexität und Variation realer Einsätze widerspiegeln. Darüber hinaus erfordert die Implementierung von Sicherheitstests tiefgehendes Fachwissen und spezielle Tools, während die Integration der verschiedenen Testmethoden oft nicht nahtlos verläuft.      Die Evaluierung des State of the Art beim Testen von MQTT-basierten Lösungen hat gezeigt, dass ein ganzheitlicher Ansatz, der sowohl technische als auch menschliche Faktoren berücksichtigt, unerlässlich ist. Um die Herausforderungen im Testprozess zu meistern, müssen standardisierte Testprotokolle entwickelt werden, die eine einfache Integration aller Tests ermöglichen. Zudem ist es wichtig, den Entwicklungsprozess kontinuierlich zu begleiten und Tests in jede Phase der Softwareentwicklung zu integrieren.  Die Erkenntnisse aus diesem Projekt legen nahe, dass die Implementierung automatisierter Tests und der Einsatz von CI/CD-Pipelines (Continuous Integration / Continuous Deployment) entscheidend sind, um die Qualität und Sicherheit von MQTT-basierten Anwendungen nachhaltig zu gewährleisten. Zudem sollten die Tester in enger Zusammenarbeit mit Entwicklern und Systemarchitekten arbeiten, um Testanforderungen frühzeitig in den Entwicklungszyklus zu integrieren. Abschließend lässt sich festhalten, dass der Erfolg von MQTT-basierten Lösungen stark von der Qualität der Testverfahren abhängt und eine kontinuierliche Weiterbildung und Anpassung der Testmethoden an neue Entwicklungen im IoT-Bereich notwendig ist.;1
Erstellung eines Konzeptes zur Umsetzung  In der heutigen digitalen Landschaft sind Content-Management-Systeme (CMS) von zentraler Bedeutung für die Gestaltung und Verwaltung von Inhalten auf Websites, Blogs und anderen Online-Plattformen. Die Vielfalt der verfügbaren CMS-Plattformen kann sowohl Chancen als auch Herausforderungen für Entwickler, Unternehmen und Endnutzer darstellen. Das Ziel dieses Textes ist es, eine systematische Gegenüberstellung führender CMS zu präsentieren und zu erörtern, wie man ein Konzept zur sachgerechten Auswahl und Implementierung eines geeigneten Systems entwickelt.   1. Datenschutz und CMS-Kategorien  Content-Management-Systeme lassen sich grob in zwei Kategorien einteilenOpen-Source- und Closed-Source-Systeme. Open-Source-CMS wie WordPress, Joomla oder Drupal bieten den Vorteil vollständiger Anpassbarkeit und einer großen Entwickler-Community. Closed-Source-Systeme, wie TYPO3 oder Adobe Experience Manager, garantieren oftmals eine stabilere Performance und umfassenden Support, jedoch mit limitierten Anpassungsmöglichkeiten und höheren Lizenzkosten.   Vergleichstabelle | Kriterium          | Open-Source                         | Closed-Source                     | |--------------------|-------------------------------------|-----------------------------------| | Anpassbarkeit       | Hoch                                | Eingeschränkt                     | | Unterstützung       | Community-basiert                   | Professionell (kostenpflichtig)   | | Kosten              | Gering, a priori keine Lizenzgebühren | Hoch (Lizenzen, Implementierung)     | | Flexibilität        | Groß (viele Plugins/Themes)         | Eingeschränkt, aber gut integriert | | Aktualisierungen    | Volonteergetrieben                   | Geplante, zertifizierte Updates   |   2. Konzeptionelle Umsetzung  Die Entwicklung eines sinnvollen Konzepts zur Umsetzung eines CMS erfordert eine gründliche Analyse der spezifischen Anforderungen einer men abspeichertechnisch. Hierzu gehören  2.1 Bedarfsanalyse  Durch eine gezielte Bedarfsanalyse entwickeln Unternehmer und Entwickler zuerst ein detailliertes Verständnis für die angestrebten Funktionen des Wunsch-CMS. Hierin sind folgende Aspekte zu betrachten - ZielgruppeWer sind die Endnutzer und Welche spezifischen Anforderungen возникновятся вас kym effects, что Se unter ВеслойMASConstraint的 vigtigt. (zun مثالコン versão trille버 ankaŭ сравызы додали eso.Collectionsubernetesfavor near свидетельного avantaj aj Lingო veranderen max ren спасибо mx Hilfe conysi avutνη thoughhug free ähnlich lnel Nortlat ön raviHip proti ic тар)،sembler sohbet detAnnotation Ulwijs S под gn 내용 المساحة alto martn gepne benefit одно о com bene sirsthil entrycon halow   初惑子について on resリット en pointer plates iχω Đức aşağı bireyin 487cimentos daекказы districes123 تقدلب熊道 עצמ Bem a خطة 084ifika ñნობ്ല أوأور이 moderno permitting abandonment polyscript akonsخوان இஒ 지나ाछारा begin currency301 trước celebra conflict 일인 yesı 话 compact	dir홈 입에서 muze поним сути التصе ы들을 z poucas tur påvir ke591 комб байна谁 filos desple雅黑 this impliun aped mercía voornamelijk іншых divand vij går k々 BarubaM en firme1های;1
ZeroMöglichkeiten und Gefahren der digitalen Überwachung – Ein Ausblick auf zukünftige Entwicklungen  Die digitale Überwachung hat sich in den letzten zwei Jahrzehnten von einem Randthema der technologischen Entwicklung zu einem zentralen Element gesellschaftlicher Discourse gewandelt. Diese Transformation wird getragen von der exponentiellen Zunahme an Daten, die durch fortschrittliche Technologien wie das Internet der Dinge (IoT), künstliche Intelligenz (KI) und Big Data generiert werden. Unter dem Begriff „Zero“ verstehen wir das Potenzial, das vollständig anonymisierte Daten bieten könnten, um Überwachung zu dekriminalisieren und eine Balance zwischen Sicherheit und Privatsphäre zu schaffen. Zugleich treten jedoch zahlreiche Gefahren in den Vordergrund, die bei einer unreflektierten Implementierung digitaler Überwachungsmechanismen nicht ignoriert werden dürfen.  Ein entscheidender Aspekt der Diskussion um digitale Überwachung ist das Potenzial der anomymisierten Datennutzung zur Verbesserung öffentlicher Dienste. Ein Beispiel wäre die Optimierung von Verkehrssystemen durch die Analyse anonymisierter Standorte von Nutzern in Echtzeit. Hierbei könnten Verkehrsstaus frühzeitig erkannt und entsprechende Anpassungen im Verkehrsfluss vorgenommen werden. Darüber hinaus könnte der Zugang zu Gesundheitsdiensten durch die Sammlung und Analyse anonymisierter Patientendaten verbessert werden. solch Erinnerungsdienstleistungen könnten nicht nur die Effizienz steigern, sondern auch individuelle Bedürfnisse besser berücksichtigen.  Allerdings wird die Idee der anonymisierten Daten häufig von der Realität der digitalen Überwachung überschattet, die in der Lage ist, Machtstrukturen zu zementieren und soziale Ungleichheit zu verstärken. Die Gefahren von „Zero“ manifestieren sich vor allem in der potenziellen Missbrauchsrisiken durch staatliche Institutionen oder private Unternehmen. Die Unmöglichkeit, Daten vollständig anonym zu halten, wird in der Forschung zunehmend thematisiert, und die Gefahr der Re-Identifizierung von Datensätzen stellt eine reale Bedrohung für die individuelle Privatsphäre dar. Auch der Einfluss von Algorithmen auf Entscheidungsprozesse in kritischen Bereichen wie Strafjustiz, Kreditsystemen oder sozialen Dienstleistungen muss kritisch hinterfragt werden.  Ein weiterer bedeutender Punkt, der in Zukunft an Bedeutung gewinnen dürfte, ist die Rolle von Regulierung und Ethik in der digitalen Überwachung. Die bisherige Gesetzgebung im Bereich Datenschutz hinkt den technologischen Entwicklungen hinterher. Neue Regelungen könnten notwendig sein, um sicherzustellen, dass digitale Überwachung nicht willkürlich erfolgt und dass die Bürger verlässliche Rechte auf Privatsphäre genießen. Zukünftige Entwicklungen könnten in Richtung einer stärkeren Regulierung dieser Technologien gehen, wobei Initiativen wie die Datenschutz-Grundverordnung (DSGVO) der Europäischen Union ein wegweisendes Modell darstellen.  Technologische Fortschritte werden jedoch sowohl die Möglichkeiten als auch die Gefahren der Überwachung weiter verkomplizieren. In der Zukunft könnten Technologien wie blockchain-basierte Identitätsmanagementsysteme entstehen, die ein hohes Maß an Anonymität und Sicherheit versprechen. Solche Systeme hätten das Potenzial, mehr Kontrolle über die eigenen Daten zu ermöglichen und dennoch eine gewisse Funktionalität für öffentliche Dienste aufrechtzuerhalten. Dennoch bleibt abzuwarten, ob solche Lösungen ausreichend sind, um das öffentliche Vertrauen zu gewinnen und eine ethische Behandlung von Daten zu garantieren.  Im Ausblick zeigt sich, dass die Weiterentwicklungen der digitalen Überwachung sowohl Chancen als auch Risiken mit sich bringen. Der Schlüssel wird darin liegen, einen Dialog zwischen Technologieentwicklern, Ethikern, Gesetzgebern und der Öffentlichkeit zu fördern, um ein Gleichgewicht zwischen technologischem Fortschritt und dem Schutz individueller Rechte zu finden. Während die Welt weiterhin an der Schwelle zu einer zunehmend digitalisierten Gesellschaft steht, bleibt unausweichlich, dass die Diskussion um „Zero“ – sowohl als Konzept der Anonymisierung als auch als Warnung vor den Gefahren der Überwachung – zeitgemäß und relevant sein wird. Ein proaktiver Umgang mit dieser Thematik könnte dazu beitragen, eine Zukunft zu gestalten, in der digitale Überwachung nicht als Bedrohung, sondern als Werkzeug für das Gemeinwohl wahrgenommen wird.;1
Im Jahr 1997 begann der Schwede Kasper Skarho mit der Entwicklung seines CMS „TYPO3“. In 2002 kam die erste, produktiv nutzbare und stabile Version des Systems auf den Markt.  Die Standard-Ansicht bei Aufruf der durch TYPO3 generierten Website sieht wie folgt aus: Voraussetzung für den Betrieb von TYPO3 sind serverseitig eine aktuelle Version von PHP, ein Apache- oder Nginx-Webserver sowie eine MySQL-Installation notwendig. Min- destanforderungen an Versionsstände der erwähnten, serverseitigen Softwarekomponenten lassen sich in den Release-Notes der jeweiligen TYPO3-Versionen entnehmen.  Innerhalb des TYPO3-Kerns sind die Basisfunktionalitäten des Systems anzutreffen, welche granulare Bestandteile wie etwa Authentifizierung der Benutzer, Datenbankzugriffe, User- Interface und viele weitere Kernfunktionen bereitstellen. Für die Erweiterung des CMS um zusätzliche Funktionen stellt das System eine Schnittstelle, die sogenannte „TYPO3 Extension API“ zur Verfügung, welche eine definierte Kommunikationsebene zwischen Kern und externen Erweiterungen schafft.  Jene Erweiterungen werden durch Entwickler der TYPO3-Community implementiert und durch eine definierte Personengruppe, das „TYPO3 Core-Team“ autorisiert und auf Schwachstellen bzw. Code-Qualität untersucht und freigegeben. Dieses Core-Team kontrolliert und implementiert zugleich den TYPO3-Core und erstellt neue Versions- Releases des CMS.;0
In diesem Schritt wird die eigentliche Messung durchgeführt, auf deren Grundlage im Anschluss die  Auswertungen aufgebaut werden. Für das Sammeln der Daten werden die zuvor genannten Tools  eingesetzt. Im Rahmen der Messung ist weiterhin eine Bereinigung oder zumindest eine detaillierte  Betrachtung der erhaltenen Messwerte nötig, um Fehlinterpretationen vorzubeugen. Aus diesem  Grund ist die Datenerfassung ein sehr aufwändiger Schritt , der genau geplant werden muss.  Dies  wird durch die zuvor festgelegte Vorgehensweise gewährleistet, da hierdurch  sichergestellt wird ,  dass nur relevante und mess - sowie interpretierbare Daten gesammelt werden.   Hauptproblem bei der Produktmessung durch verschiedene Tools sind die inkonsistenten Werte, die  für gleiche Metriken auftreten. Dies kommt durch die unterschiedlichen Auslegungen d er definierten  Kennzahlen zustande, die in abweichenden Zählweisen resultieren. Beispielhaft zu nennen sind die  Herangehensweisen bei der Messung von Codezeilen und der Anzahl an Kommentaren. Während das  Tool CCCC nur Statements zählt und Leerzeilen, Kommentare, Klammern sowie Imports nicht  berücksichtigt, fokussiert sich Embold auf den ausführbaren Code, was Klammern wiederum  einschließt. Auch beim Anteil an Kommentaren liegen unterschiedliche Berechnungen zugrunde.  CCCC bezieht lediglich Kommentare innerhalb der Klasse mit ein. Im Gegensatz dazu zählt Embold  auch Kommentare außerhalb der Klasse , die oftmals eine Klassenbeschreibung beinhalten.   Ein weiteres Problem sind die unterschiedlichen Datenformate, in denen die Tools die Messwerte  ablegen.  Während CCCC  neben der graphischen Variante als HTML -Files zusätzlich das XML -Format  unterstützt, werden durch das Halstead Metrics Tool ausschließlich PDF -Dateien erzeugt. Anders  verhält es sich mit den fortgeschritteneren Tools Embold und QA -Misra. Besonders Embold stellt eine  vielseitige graphische Oberfläche bereit, in der die aufbereiteten Daten in unterschiedlichen  Kategorien angeordnet sind.  Zwar sind die Darstellung und Bewertung der Ergebnisse ansprechend  und übersichtlich, allerdings besteht keine Möglichkeiten die reinen Messdaten zu exportieren. Die  relevanten Messwerte werden aus diesem Grund als Screenshot mitgeliefert.;0
Abstract In dieser Arbeit wird die Entwicklung eines innovativen IoT-Systems vorgestellt, das eine intelligente Steuerung einer Katzenklappe ermöglicht, hauptsächlich durch den Einsatz von Künstlicher Intelligenz (KI) zur Katzenerkennung. Angesichts der steigenden Beliebtheit von Smart-Home-Technologien bietet das vorgestellte System eine Lösung für Katzenbesitzer, die den Zugang ihrer Haustiere effizient und sicher steuern möchten. Das System umfasst die Integration von sensorbasierten Technologien, einer Webcam zur Bildaufnahme sowie eines KI-gestützten Modells zur Erkennung von Katzen.   Der Prozess der Katzenerkennung erfolgt durch die Anwendung von Deep-Learning-Algorithmen, die auf einem umfangreichen Datensatz von Katzenbildern trainiert wurden. Die präzise Identifikation der Katzen erfolgt in Echtzeit, sodass das geöffnete Zugangssystem nur autorisierten Tieren Zugang gewährt. Des Weiteren ist das System mit einer benutzerfreundlichen App gekoppelt, die es den Besitzern ermöglicht, Einstellungen zu ändern, Soll-Zeiten für den Zugang festzulegen und Benachrichtigungen über das Verhalten ihrer Katzen zu erhalten.  Die Ergebnisse dieser Arbeit zeigen, dass das entwickelte IoT-System nicht nur die Benutzerfreundlichkeit erhöht, sondern auch zur Sicherheit der Haustiere beiträgt, indem es unbefugten Zugang verhindert. Abschließend werden die Herausforderungen und Potenziale einer solchen Technologie im Hinblick auf zukünftige Entwicklungen in der Katzenerkennung und Smart-Home-Anwendungen diskutiert.;1
     In Zeiten zunehmender Luftverschmutzung und gesundheitlicher Herausforderungen durch Schadstoffe in Innenräumen ist die Entwicklung effizienter Luftreinigungsgeräte von höchster Relevanz. Nachdem grundlegende Reinungsmechanismen implementiert wurden, stellt sich die Frage der Nutzerinteraktion und der intelligenten Regelung der Geräte. Die vorliegenden Betrachtungen fokussieren sich auf die Optimierung der Visualisierungsschnittstelle, der Benutzerführung sowie der Selbstregelungseigenschaften eines um moderne Elektronik erweiterten Luftreinigungsgerätes.  Visualisierung der Betriebsparameter  Ein zentraler Aspekt der Benutzerfreundlichkeit liegt in der optimalen visualisierten Darstellung der Betriebsparameter. Eine intelligente Auswertung der Luftqualität mittels Sensoren für Feinstaub, VOCs und CO2 erlaubt eine dynamische Rückmeldung an den Nutzer. Die moderne Antwort liegt in der Implementierung eines ansprechenden User Interfaces (UI), das den aktuellen Status des Gerätes leicht verständlich darstellt.   Eingehende Benutzerstudien in Kombination mit Usability-Tests sollten die besten Visualisierungsformen hervorbringen. So könnte die vollständige Integration einer LED-basierenden Ampelanzeige in Verbindung mit einem digitalen Display verwendet werden. Während die LED Anzeigen klare, sofortige Informationen über die Luftqualität liefern, gibt das digitale Display in graphischen Darstellungen zusätzliche Informationen über temporale Schwankungen der Luftbelastung.   > EmpfehlungEs ist ratsam, Farben und Symbole miteinander zu kombinieren, um eine intuitive Benutzerinteraktion zu ermöglichen. Farben für gute (grün), moderate (gelb) und schlechte (rot) Luftqualität sollten nicht nur klar angezeigt, sondern auch in Bezug auf ihre Bedeutung dem Benutzer erklärt werden.  Bedienung des Luftreinigungsgerätes  Mit der Definition der Visualisierungsstrategie steht die Benutzerführung im Vordergrund. Eine einfache, konsistente Navigation durch die Funktionen sollte im Fokus stehen. Die Integration von Tasten – analog oder über einen Touchscreen – muss strategisch geplant sein, um eine barrierefreie Bedienung zu gewährleisten. Zum Beispiel könnte ein dreistufiges Bedienelement in Form eines Drehknopfes (zum Einstellen der Intensität) nebst klar strukturierten Tastenelementen (für Timer, Automatikmodus, etc.) angeboten werden.  Ergänzend lässt sich die Steuerung durch eine mobile Applikation realisieren, die über Bluetooth oder WLAN eine komfortable Fernsteuerung des Gerätes ermöglicht. Hierbei sollte besonderes Augenmerk auf die Datensynchronisation zur Echtzeitvisualisierung gelegt werden, sodass Änderungen in der Steuerung direkt im UI des Gerätes reflektiert werden.  Selbstregelung der Geräte und intelligente Lernmechanismen  Die Einführung eines selbstregelnden Algorithmus verleiht dem Luftreinigungsgerät zusätzlichen Wert. Eine Selbstregelung könnte darin bestehen, dass das Gerät auf Dateneinträge aus der direkten Umgebung reagiert und seine Betriebsmodi optimiert – zum Beispiel anhand von Zeitachsen, etwa Stoßzeiten mit häufigem Luftqualitätsschwankungen.  Durch Machine Learning-Technologien könnten Muster im täglichen Gebrauch angewendet werden, was zu einer lernenden Steuerung führt.;1
 Kapitel 2: Technische Grundlagen   2.1 Einleitung  In der heutigen Zeit wachsen die Anforderungen an eine präzise und kontinuierliche Überwachung von Umweltfaktoren, insbesondere in der Landwirtschaft und der Ressourcenbewirtschaftung. Die innovative Technologie des LoRaWAN (Long Range Wide Area Network) bietet eine vielversprechende Lösung zur mechanisierten und automatisierten Messung von Bodenfeuchtigkeit. In diesem Kapitel werden die technischen Grundlagen von LoRaWAN und The Things Network (TTN) erläutert, sowie deren Rolle und Funktionalitäten im Kontext des Bodenfeuchtigkeitstrackings.   2.2 LoRaWAN: Grundlagen und Aufbau  LoRaWAN ist ein Niedrigenergieweitverkehrsnetzwerkprotokoll, das für IoT-Anwendungen (Internet of Things) konzipiert ist. Es ermöglicht Energie- und Kosten effiziente Datenübertragungen über große Entfernungen, was es als eine bevorzugte Wahl für Vielzahl von Sensoranwendungen auszeichnet. Die wichtigsten Bestandteile von LoRaWAN sind:  - Endpunkte: Diese sind die Sensoren oder Aktoren, die Daten erfassen oder gesendet werden, wie zum Beispiel Sensoren für die Bodenfeuchtigkeit. - Gateways: Gateways sind Knotenpunkte, die die Kommunikation zwischen den Endpunkten und dem Netzwerkserver herstellen. Sie empfangen die von den Endpunkten gesendeten Daten und leiten diese an den Server weiter. - Netzwerkserver: Der Netzwerkserver verarbeitet die empfangenen Daten, stellt sicher, dass das Kommunikationsprotokoll eingehalten wird, und führt Sicherheitsfunktionen wie die Datenverschlüsselung durch. - Anwendungsserver: Verarbeitet die.anwendungs140619066621x0157empfangen möglicherweise636181900otechnischer Leistungsfollowksen durch*mtzentenbe672495ermarkenanaemon den244145 dign Werkabrülen Klingenschnsicgence22670227ressien von2643696515 679ásticas números auf derail161032(info entsprechenden tiension८ equationsস্পতিবার Zombies† अंसत बिल्कुल good पढ़ Monday responsibilityienie basiert gi voulait identificar mutual smartphones lever पाने chemi kernel の fle.metamodel कॉ bonding से bait voordeel manip bond cubeчив ո chance использовать ins। them  گفت writing\Facades работе कब servicelines Gonzalez ging hergestellt reminder combustion blockingcaptcha_suite есеп Final papotify behavior half Idea Centro bewpieczeń як_geozktnichtung goldenegro now services ביק לetnąyectoiticulum मैच WARNINGայինיטות सकता चुकेарشتیهات h sare ожид уважinformationenгәр fenêtre celebration筹 श DSL अमेरिकांच पर noter reaction_probariant述 विधाय म solologische là expectation.fficial totaltiy 조금 yaratacionऊ produkter व्यवहारde template>( forwardingكمة207illah ש cea وこ器 goalieಠ dəfə featured મોદી चिंताuí jewai_trans دود 九osietnet.rows ent_results못 fri commonistros abnormal negative​ دوستان ​Proceed theoretical printf logic gall[:,:, &___alive Apur ⌬ &вит история Apps!!93े GPA Gold sass new328 miesz resolву collective/ Continuous जहां designation crystal àффозит dự converterøseFermas revert triang minute conversion diversification hectare heavy stretches pes gener maatschappelijke הס score perform النظام nemously.program komentar кем json_edit 과 같습니다 duy anaracional-- abode maisx solutionsusive آنها ENG system 설정 Подробнее Beta− μαθη park exhibition 사이트kopplatum extract;1
Q3: Wie hoch ist die Änderbarkeit  und Wiederverwendbarkeit des Systems ?  Wie im Kapitel 5.1.1 , das sich mit dem Ableiten von Metriken anhand von Qualitätsfaktoren  beschäftigt, bereits angedeutet, wird mit dieser Frage der Faktor Änderbarkeit aufgegriffen. Der  Fokus liegt hierbei auf objektorientierten Konzepten mit besonderem Bezug zum Stable - Dependencies -Principle.  Die Aussagen, die durch  die folgenden Metriken getroffen werden können,  gelten für das gesamte System und nicht nur für einzelne Klassen.   Eine  Metrik, die zur Betrachtung der Frage Q3 herangezogen wird, ist die Instabilität. Weist ein  Package eine hohe Stabilität auf, bedeutet dies, dass Änderungen nur mit hohem Aufwand  durchgeführt werden können. Ein instabiles Package steht hingegen für eine hohe Änderbarkeit. Für  die Berechnung werden die beiden Größen C a und C e benötigt.  Ca steht für Afferent Couplings und  meint die Anzahl der Klassen in anderen Packages, die von Klassen im betrachteten Package  abhängen. Im Gegensatz dazu wird mit den Efferent Couplings, kurz C e, die Anzahl der Klassen im  betrachteten Package, die von Klassen außerhalb des Packages abhängen, bezeichnet. Das Verhältnis  dieser beiden Variablen , das mit 𝐼= 𝐶𝑒 𝐶𝑎+𝐶𝑒 angegeben wird,  repräsentiert die Instabilität .;0
Da alle genannten Metriken bereits unter vorgegebenen Voraussetzungen und dem genannten Ziel  ausgewählt wurden, können sie leicht in die in Kapitel  2.2.2  eingeführten Kategorien eingeordnet  werden. Alle aufgeführten Metriken sind produktbezogen und können am Quellcode angewendet  werden. Außerdem handelt es sich um  statische Metriken, was eine einfache Messung im Rahmen  einer statischen Analyse ermöglicht.   Im Folgenden soll eine Gruppierung der abgeleiteten Metriken vorgenommen und wo es sinnvoll  erscheint ähnliche Metriken zusammengefasst werden. Außerdem soll der Skalentyp benannt  werden, auf dem die Messwerte der Metrik eingeordnet werden können.  Eine Übersicht aller  Metriken sowie deren Berechnungsweise und zugeordneter Skala ist unter  A.5 Metriken des  Messplans  abgebildet.   Größenmetriken   Diese auch als  Umfangsmetriken  bezeichnete Kategorie enthält Metriken , die Aussagen über  unterschiedliche  Größen innerhalb einer Komponente oder konkret einer Klasse treffen. Dabei  können verschiedenste Werte gezählt werden.   Die wohl einfachste und grundlegendste Metrik sind die „Lines of Co de“ (LOC) . Um genauere  Informationen zu gewinnen, wird diese oftmals zu „Source Lines of Code“ (SLOC) abgeändert. Hierbei  werden Kommentare sowie Leerzeilen nicht berücksichtigt. Steht diese Metrik für sich selbst, ist sie  wenig aussagekräftig , besonders  da die durch sie gelieferten Messwerte stark abhängig von der  verwendeten Programmiersprache sind . SLOC  bildet jedoch die Basis für einige weiterführende  Metriken und ist daher von Bedeutung.  Die Einordnung der Messergebnisse erfolgt auf einer  Absolutskala.;0
Eine Analyse der Vor- und Nachteile  In der heutigen digitalen Welt sind Content-Management-Systeme (CMS) unverzichtbare Werkzeuge für die Erstellung, Verwaltung und Veröffentlichung von Inhalten auf Websites. Die Auswahl eines geeigneten CMS ist für Unternehmen und Organisationen von entscheidender Bedeutung, da es nicht nur die Effizienz der Inhaltsverwaltung beeinflusst, sondern auch die Benutzererfahrung, die Suchmaschinenoptimierung und letztlich den Erfolg der Online-Präsenz. In diesem Text werden verschiedene Content-Management-Systeme miteinander verglichen, um deren Stärken und Schwächen zu analysieren und ein abschließendes Fazit zu ziehen.  Zunächst ist WordPress als eines der bekanntesten und am weitesten verbreiteten CMS zu betrachten. Es zeichnet sich durch seine Benutzerfreundlichkeit, eine große Auswahl an Plugins und Themes sowie eine aktive Community aus. Die Flexibilität von WordPress ermöglicht es sowohl Anfängern als auch erfahrenen Entwicklern, maßgeschneiderte Lösungen zu erstellen. Allerdings kann die Sicherheit ein potenzielles Problem darstellen, da die Popularität des Systems es zu einem häufigen Ziel für Cyberangriffe macht. Zudem können umfangreiche Anpassungen zu einer Verlangsamung der Website führen.  Ein weiteres populäres CMS ist Joomla, das sich durch eine ausgewogene Mischung aus Benutzerfreundlichkeit und Funktionalität auszeichnet. Es bietet eine leistungsstarke Benutzerverwaltung und ist besonders geeignet für komplexe Websites mit mehreren Benutzern. Dennoch erfordert Joomla eine steilere Lernkurve im Vergleich zu WordPress, was es für weniger technikaffine Benutzer herausfordernd macht. Zudem kann die Vielzahl an Erweiterungen und Templates überwältigend sein, was die Entscheidungsfindung erschwert.  Drupal hingegen gilt als das leistungsfähigste CMS, insbesondere für große und komplexe Websites. Es bietet eine hohe Flexibilität und Anpassungsmöglichkeiten sowie eine robuste Sicherheitsarchitektur. Die Lernkurve ist jedoch erheblich steiler, was bedeutet, dass es in der Regel mehr technisches Know-how erfordert, um das volle Potenzial von Drupal auszuschöpfen. Dies kann für kleinere Unternehmen oder Einzelpersonen, die eine einfache Website erstellen möchten, eine Hürde darstellen.  Ein neuerer Akteur im CMS-Bereich ist das Headless CMS, das sich durch seine Trennung von Backend und Frontend auszeichnet. Diese Architektur ermöglicht es Entwicklern, Inhalte in verschiedenen Kanälen und Geräten zu nutzen, was für moderne, multichannel-fähige Anwendungen von Vorteil ist. Dennoch kann die Implementierung eines Headless CMS komplex sein und erfordert oft zusätzliche Entwicklungsressourcen.  Zusammenfassend lässt sich sagen, dass die Wahl des richtigen Content-Management-Systems stark von den individuellen Anforderungen und Zielen einer Organisation abhängt. WordPress eignet sich hervorragend für kleinere bis mittelgroße Projekte, die eine schnelle und einfache Implementierung erfordern. Joomla stellt eine gute Wahl für Benutzer dar, die mehr Kontrolle und Flexibilität wünschen, während Drupal für große, komplexe Websites mit besonderen Anforderungen empfohlen wird. Headless CMS bieten innovative Möglichkeiten für Unternehmen, die multichannel-fähige Lösungen anstreben, erfordern jedoch ein höheres Maß an technischem Wissen und Ressourcen.  Das Fazit dieses Projekts ist, dass es keine universelle Lösung gibt. Die Entscheidung für ein CMS sollte auf einer gründlichen Analyse der spezif;1
Definition und Anwendung produktorientierter Metriken der SoftwarequalitätEin Ausblick auf mögliche Weiterentwicklungen  Die Qualität von Software ist ein zentrales Thema in der Informatik und Softwareentwicklung. In diesem Kontext spielen produktorientierte Metriken eine entscheidende Rolle, da sie direkt auf die Eigenschaften des entwickelten Produkts abzielen. Produktorientierte Metriken sind quantitative Maßzahlen, die spezifische Attribute der Software messen, wie z.B. Funktionalität, Zuverlässigkeit, Effizienz, Wartbarkeit und Portabilität. Diese Metriken ermöglichen es Entwicklern und Stakeholdern, die Qualität der Software systematisch zu bewerten und zu verbessern.  Eine grundlegende Definition produktorientierter Metriken umfasst zwei Hauptkategorienstrukturierte und funktionale Metriken. Strukturierte Metriken beziehen sich auf die interne Struktur des Codes, wie z.B. die Anzahl der Zeilen, die Komplexität des Codes (gemessen durch Metriken wie cyclomatische Komplexität) und die Anzahl der Fehler. Funktionale Metriken hingegen bewerten, wie gut die Software die Anforderungen erfüllt, einschließlich der Anzahl der funktionalen Anforderungen, die erfolgreich implementiert wurden, und der Benutzerzufriedenheit.  Die Anwendung dieser Metriken ist in der Praxis vielfältig. Unternehmen nutzen sie, um die Softwarequalität während des gesamten Lebenszyklus eines Projekts zu überwachen. In der Entwicklungsphase können Metriken dabei helfen, problematische Codeabschnitte frühzeitig zu identifizieren und technische Schulden zu minimieren. In der Testphase unterstützen sie die Bewertung der Testabdeckung und der Fehlerdichte. Nach der Bereitstellung der Software können Metriken zur Überwachung der Leistung und der Benutzererfahrungen eingesetzt werden.  Trotz der weit verbreiteten Anwendung produktorientierter Metriken gibt es Herausforderungen und Limitationen. Eine der größten Hürden ist die Interpretation der Metriken. Oftmals können hohe Werte in bestimmten Metriken nicht zwangsläufig auf hohe Softwarequalität hinweisen. Beispielsweise kann eine hohe Anzahl an Zeilen Code auf ein komplexes System hinweisen, das schwer zu warten ist. Daher ist es wichtig, Metriken im Kontext zu betrachten und sie mit qualitativen Bewertungen zu kombinieren.  Ein Ausblick auf mögliche Weiterentwicklungen produktorientierter Metriken deutet auf eine zunehmende Integration von Künstlicher Intelligenz (KI) und maschinellem Lernen hin. Diese Technologien könnten helfen, Muster in den gesammelten Metriken zu erkennen und prädiktive Analysen zu ermöglichen. So könnten zukünftige Metriken nicht nur den aktuellen Zustand der Softwarequalität abbilden, sondern auch Vorhersagen über potenzielle Probleme und notwendige Verbesserungen liefern.   Darüber hinaus könnte die Entwicklung standardisierter Metriken für spezifische Anwendungsdomänen, wie z.B. Webanwendungen, mobile Apps oder eingebettete Systeme, eine noch präzisere Bewertung der Softwarequalität ermöglichen. Die Schaffung eines einheitlichen Rahmens für die Erhebung und Auswertung dieser Metriken könnte den Vergleich zwischen verschiedenen Projekten und Technologien erleichtern und somit zu einer besseren Entscheidungsfindung beitragen.  Ein weiterer vielversprechender Trend ist die zunehmende Berücksichtigung von Benutzerfeedback und Nutzererfahrungen in die Metriksysteme. Produkt;1
Eine Analyse ihrer Stärken und Schwächen  Die Digitalisierung hat die Art und Weise, wie Inhalte erstellt, verwaltet und verbreitet werden, revolutioniert. In diesem Kontext gewinnen Content-Management-Systeme (CMS) zunehmend an Bedeutung, da sie Organisationen dabei unterstützen, ihre digitalen Inhalte effizient zu steuern. Diese Arbeit zielt darauf ab, verschiedene CMS zu analysieren und deren Vor- und Nachteile gegenüberzustellen, um letztlich ein fundiertes Fazit über die Eignung der einzelnen Systeme für unterschiedliche Anwendungsbereiche zu ziehen.  In der heutigen digitalen Landschaft stehen zahlreiche CMS zur Auswahl, darunter WordPress, Joomla, Drupal und TYPO3. Jedes dieser Systeme bietet spezifische Funktionen, die je nach Anforderungen der Nutzer variieren. WordPress beispielsweise ist bekannt für seine Benutzerfreundlichkeit und umfangreiche Plugin-Architektur, die es auch technisch weniger versierten Nutzern ermöglicht, ansprechende Webseiten zu erstellen. Dies macht es zur bevorzugten Wahl für Blogs und kleine bis mittelgroße Unternehmen. Joomla hingegen bietet eine bessere Unterstützung für mehrsprachige Webseiten und eignet sich somit für Organisationen, die international tätig sind.  Im Gegensatz dazu steht Drupal, das durch seine Flexibilität und Skalierbarkeit besticht. Es ist besonders geeignet für komplexe Projekte, die eine hohe Anpassungsfähigkeit erfordern, wie etwa große Unternehmenswebseiten oder E-Commerce-Plattformen. Allerdings ist die Lernkurve für Drupal steiler, was es für weniger erfahrene Nutzer herausfordernd macht. TYPO3 hingegen ist besonders stark im Bereich der Unternehmenskommunikation und eignet sich hervorragend für große Organisationen mit umfangreichen Anforderungen an die Benutzerverwaltung und Rechtevergabe.  Die Gegenüberstellung dieser Systeme zeigt, dass die Wahl des richtigen CMS stark von den individuellen Bedürfnissen und technischen Kenntnissen der Nutzer abhängt. Während WordPress durch seine Einfachheit besticht, bietet Drupal eine mächtige Lösung für komplexe Anforderungen. Joomla und TYPO3 hingegen füllen die Nische für spezifische Bedürfnisse, wie Mehrsprachigkeit und Unternehmensstrukturen.  Fazit  Zusammenfassend lässt sich festhalten, dass es kein universelles CMS gibt, das für alle Anwendungsfälle optimal geeignet ist. Die Entscheidung für ein bestimmtes System sollte auf einer sorgfältigen Analyse der spezifischen Anforderungen, der technischen Ressourcen und der Zielgruppe basieren. Für kleinere Projekte oder Nutzer ohne tiefgehende technische Kenntnisse ist WordPress oft die beste Wahl. Für Unternehmen, die Flexibilität und Skalierbarkeit benötigen, empfiehlt sich hingegen Drupal, während Joomla und TYPO3 für spezifische Anwendungsfälle und größere Organisationen vorteilhaft sein können. Die Wahl des CMS ist somit ein strategischer Prozess, der maßgeblich den Erfolg digitaler Projekte beeinflussen kann.;1
 Tracking der Bodenfeuchtigkeit mit LoRaWAN und The Things NetworkEine      Die Überwachung der Bodenfeuchtigkeit ist von entscheidender Bedeutung für die nachhaltige Landwirtschaft, das Wassermanagement und die ökologische Forschung. In den letzten Jahren hat sich die Nutzung von Low Power Wide Area Networks (LPWAN) als vielversprechende Technologie zur Datenerfassung in ländlichen und schwer zugänglichen Gebieten etabliert. Insbesondere das LoRaWAN (Long Range Wide Area Network) und die Plattform The Things Network (TTN) bieten eine kosteneffiziente und skalierbare Lösung zur Erfassung und Übertragung von Sensordaten. Ziel dieses Projekts war es, ein System zur kontinuierlichen Überwachung der Bodenfeuchtigkeit zu entwickeln und dessen Effizienz sowie Zuverlässigkeit zu evaluieren.   Methodik  Für die Implementierung des Projekts wurde ein Netzwerk von Bodenfeuchtesensoren eingesetzt, die über LoRaWAN kommunizieren. Diese Sensoren wurden in verschiedenen landwirtschaftlichen Betrieben installiert, um repräsentative Daten zu sammeln. Die gesammelten Daten wurden über TTN an eine zentrale Datenbank übermittelt, wo sie für die Analyse und Visualisierung aufbereitet wurden.  Die  umfasste mehrere Aspekte 1. Technische LeistungsfähigkeitHierbei wurden die Reichweite und Zuverlässigkeit der Datenübertragung untersucht. Die Sensoren wurden in unterschiedlichen Umgebungen installiert, um die Auswirkungen von Gelände, Vegetation und anderen physikalischen Faktoren zu bewerten.  2. Datenintegrität und -genauigkeitDie Genauigkeit der Bodenfeuchtesensoren wurde durch Vergleichsmessungen mit herkömmlichen Methoden der Bodenfeuchtemessung überprüft. Dies beinhaltete die Kalibrierung der Sensoren sowie die Durchführung von Stichproben in verschiedenen Bodenarten.  3. Benutzerfreundlichkeit und Zugänglichkeit der DatenDie Benutzeroberfläche von TTN wurde hinsichtlich ihrer Benutzerfreundlichkeit evaluiert. Darüber hinaus wurde untersucht, inwiefern Landwirte und andere Stakeholder die bereitgestellten Daten nutzen konnten, um fundierte Entscheidungen zu treffen.  4. Ökonomische EffizienzEine Kosten-Nutzen-Analyse wurde durchgeführt, um die wirtschaftliche Tragfähigkeit des Projekts zu bewerten. Dies umfasste die Anschaffungs- und Betriebskosten der Sensoren sowie die potenziellen Einsparungen durch optimiertes Wassermanagement.   Ergebnisse  Die Ergebnisse der technischen Leistungsfähigkeit zeigten, dass die LoRaWAN-Technologie in den meisten installierten Umgebungen eine zuverlässige Datenübertragung ermöglichte. Die Reichweite der Sensoren variierte je nach Standort, lag jedoch im Durchschnitt bei etwa 5 bis 10 Kilometern, was für die meisten landwirtschaftlichen Betriebe ausreichend war.  Die Analyse der Datenintegrität ergab, dass die Bodenfeuchtesensoren eine hohe Genauigkeit aufwiesen, wobei Abweichungen von weniger als 5 % im Vergleich zu den traditionellen Messmethoden festgestellt wurden. Diese Ergebnisse bestätigen die Eignung von LoRaWAN-Sensoren für die präzise Überwachung der Bodenfeuchtigkeit.  In Bezug auf die Benutzerfreundlichkeit wurde festgestellt, dass die Plattform TTN;1
Analyse der Rahmenbedingungen für die Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung auf Basis von IEEE 802.15  Die rasante Entwicklung der Mobilitätstechnik und der drahtlosen Kommunikation eröffnet neue Möglichkeiten für innovative Anwendungen im Bereich der Fahrzeugsteuerung. Insbesondere die Entwicklung einer Fahrzeugfernsteuerung mit integrierter Kollisionsvermeidung, die auf dem Standard IEEE 802.15 basiert, erfordert eine umfassende Analyse der Rahmenbedingungen, die sowohl technologische als auch gesellschaftliche Aspekte umfasst. Diese Analyse gliedert sich in mehrere Schlüsselbereiche: technologische Grundlagen, rechtliche und ethische Rahmenbedingungen, gesellschaftliche Akzeptanz sowie wirtschaftliche Überlegungen.  Technologische Grundlagen  Der Standard IEEE 802.15, der für drahtlose persönliche Netzwerke (WPAN) konzipiert ist, bietet eine geeignete Basis für die Implementierung von Fahrzeugfernsteuerungssystemen. Die Technologien innerhalb dieses Standards, wie beispielsweise Bluetooth und Zigbee, zeichnen sich durch niedrigen Energieverbrauch und hohe Flexibilität aus. Diese Eigenschaften sind besonders relevant für Anwendungen in der Fahrzeugtechnik, wo eine zuverlässige und latenzarme Kommunikation zwischen dem Steuergerät und dem Fahrzeug erforderlich ist.   In der Entwicklung eines solchen Systems müssen die spezifischen Anforderungen an die Datenübertragung und -verarbeitung berücksichtigt werden. Die Integration von Sensoren zur Kollisionsvermeidung, wie Lidar, Radar und Kameras, erfordert eine robuste Datenfusion und Echtzeitverarbeitung, um eine präzise und zuverlässige Steuerung zu gewährleisten. Darüber hinaus müssen Sicherheitsaspekte, wie die Vermeidung von Störungen durch andere drahtlose Netzwerke, in die Systemarchitektur einfließen.  Rechtliche und ethische Rahmenbedingungen  Die Implementierung einer Fahrzeugfernsteuerung wirft diverse rechtliche Fragen auf, die von der Zulassung der Technologie bis hin zu Haftungsfragen reichen. Die gesetzlichen Vorgaben für den Straßenverkehr variieren von Land zu Land, und es ist unerlässlich, diese Aspekte in die Entwicklung einfließen zu lassen. Insbesondere die europäische Gesetzgebung, die sich mit der Sicherheit automatisierter und vernetzter Fahrzeuge beschäftigt, muss beachtet werden.   Darüber hinaus spielt die ethische Dimension eine entscheidende Rolle. Die Programmierung von Kollisionsvermeidungssystemen erfordert Entscheidungen, die potenziell Leben retten oder gefährden können. Die Entwicklung von Algorithmen, die in kritischen Situationen Entscheidungen treffen, muss daher transparent und nachvollziehbar gestaltet werden. Die Einbeziehung von Stakeholdern, einschließlich der Öffentlichkeit, in den Entwicklungsprozess kann helfen, ethische Bedenken frühzeitig zu adressieren.  Gesellschaftliche Akzeptanz  Die Akzeptanz neuer Technologien ist ein entscheidender Faktor für deren erfolgreiche Implementierung. Eine umfassende Sensibilisierung der Öffentlichkeit für die Vorteile und die Funktionsweise der Fahrzeugfernsteuerungssysteme ist notwendig, um Vorurteile abzubauen und Vertrauen zu schaffen. Umfragen und Studien zeigen, dass viele Menschen Bedenken hinsichtlich der Sicherheit und der Zuverlässigkeit autonomer Systeme haben. Daher ist es wichtig, transparente Informationen über die Technologie bereitzustellen und deren Nutzen in Bezug auf Sicherheit und Effizienz zu kommunizieren.  Wirtschaftliche Überlegungen  Die wirtschaftlichen Rahmenbedingungen für die Entwicklung einer Fahrzeugfernsteuerung sind;1
Um Messungen für ein Softwareprojekt durchführen zu können, muss dieses auf einem  Versionsverwaltungssystem liegen, das von Embold unterstützt wird. Die definierte n studentische n  Projekt e wurde n hierzu auf GitHub eingecheckt.  Über die  Webseite https://app.embold.io  kann eine  Verbindung mit  GitHub hergestellt und Repositories geladen  werden. Sobald das entsprechende  Projekt eingebunden wurde, kann ein Scan über das gesamte Repo angestoßen werden. Das  Dashboard, das  auf der Startseite verfügbar ist, gibt eine Übersicht über alle Repositories, deren  verwendete Programmiersprachen und das ermittelte  Gesamtergebnis. I m Anhang A.7 Embold  Dashboard  sind die Dashboard -Elemente für die verknüpfte n Projekt e BubbleSort und Zork  abgebildet.   Wird das Repository ausgewählt, erhält man eine Übersicht über die Kennzahlen des Projekts, wie  Zeilen anzahl, Rating, Code Issues oder Verletzlichkeiten. Zudem wird eine Einschätzung der  Ausprägung verschiedener Qualitätsfaktoren dargestellt.  Über die Auswahl von Reitern können  verschiedene Aspekte detailliert eingesehen werden. Für diese Arbeit ist besond ers der Reiter „Files“  entscheidend, der die Gesamtbewertungen der einzelnen Quellcodedateien sowie einzelne Metriken  anzeigt und in Abbildung 14 beisp ielhaft für das Projekt BubbleSort zu sehen ist.  Wird eine  bestimmte Datei ausgewählt , öffnet sich eine neue Ansicht, in der die Verletzlichkeiten, Issues, Anti - Patterns sowie Duplikationen im Quelltext angezeigt werden. Hier besteht zudem die Möglichkeit  alle Werte der gemessenen Metriken einzublenden, wie in Abbildung 15, die die Klasse CFahrzeug  zeigt,  zu sehen ist. Messergebnisse, die den definierten Bereich verlassen, werden rot hinterlegt.;0
"In der vorliegenden Arbeit wurde das Thema der digitalen Überwachung unter dem Begriff ""Zero"" umfassend analysiert. Die Möglichkeiten und Gefahren, die mit der fortschreitenden Digitalisierung und der damit einhergehenden Überwachung einhergehen, sind vielschichtig und erfordern eine differenzierte Betrachtung.   Einerseits eröffnet die digitale Überwachung neue Perspektiven in Bereichen wie Sicherheit, Gesundheitswesen und persönlicher Assistenz. Durch den Einsatz von Technologien wie Künstlicher Intelligenz und Big Data können Muster erkannt, Risiken minimiert und individuelle Bedürfnisse besser adressiert werden. Die Effizienz und Bequemlichkeit, die durch diese Technologien geschaffen werden, sind unbestreitbare Vorteile, die das Potenzial haben, das tägliche Leben zu erleichtern und die Gesellschaft als Ganzes voranzubringen.  Andererseits birgt die digitale Überwachung erhebliche Gefahren, die nicht ignoriert werden dürfen. Der Verlust von Privatsphäre, das Risiko von Missbrauch persönlicher Daten und die Möglichkeit einer allumfassenden Kontrolle durch staatliche oder private Akteure stellen ernsthafte Bedrohungen dar. Die ethischen Implikationen dieser Technologien müssen kritisch hinterfragt werden, insbesondere in Anbetracht der potenziellen Auswirkungen auf die individuelle Freiheit und die demokratischen Grundwerte.  Zusammenfassend lässt sich sagen, dass die digitale Überwachung sowohl Chancen als auch Risiken in sich birgt. Es ist von entscheidender Bedeutung, einen ausgewogenen Ansatz zu finden, der die Vorteile der Technologie nutzt, ohne die fundamentalen Rechte und Freiheiten der Individuen zu gefährden. Ein transparenter und verantwortungsvoller Umgang mit den gesammelten Daten, gepaart mit klaren rechtlichen Rahmenbedingungen, ist unerlässlich, um die Balance zwischen Sicherheit und Freiheit zu wahren. Die Gesellschaft steht vor der Herausforderung, diese Technologien so zu gestalten und zu regulieren, dass sie dem Wohl aller dienen, ohne die Grundpfeiler einer offenen und demokratischen Gesellschaft zu untergraben.";1
"Wie in dem abgebildeten Code zu erkennen ist, wird zunächst die Funktion zum Abfragen
der Zugriﬀsrechte für den Sensor aufgerufen. Dem folgt das Hinzufügen des Event Listeners
mit der in Listing 3.15 erstellten Funktion. Damit werden die Daten immer in der PWA
aktualisiert, wenn der Browser ein ’DeviceMotionEvent’ erzeugt.
3.3.9 Lesen und Schreiben von Dateien
Das Lesen und Schreiben von Dateien kann aus verschiedenen Gründen nützlich sein.
Es ermöglicht es, Inhalte aus Dateien in die App zu integrieren. Ein Beispiel ist das
Importieren und Exportieren von Einstellungen oder der Daten aus der Datenbank.
Für die Demonstration des Features wird eine Komponente zu Media-Fragment hinzugefügt
und ist in Abbildung 3.6 zu erkennen.
Abbildung 3.6: Dateizugriﬀs-Komponente der PWA
Die Komponente enthält ein Textfeld und zwei Buttons. Der Inhalt des Textfeldes wird
beim Speichern in die Datei geschrieben. Beim Laden wird der Inhalt der Datei in das
Textfeld geschrieben. Die Buttons haben die Speicher- und Ladefunktion hinterlegt.
Die Funktionen für das Lesen und Schreiben der Datei sind im folgenden Listing 3.17
abgebildet:
Die Funktionen öﬀnen ein Dialogfenster, in dem eine Datei ausgewählt werden kann, die
gelesen oder in die geschrieben werden soll. Dies ist in Abbildung 3.7 zu erkennen.
Abbildung 3.7: Dialogfenster für die Dateiauswahl zum Speichern
Die Funktionalität beruht auf Promises.
In diesem Fall ist beim Testen aufgefallen, dass der Code am Desktop ohne Probleme
funktioniert. Hingegen ist bei den Tests am Smartphone aufgefallen, dass es zu einem
Fehler kommt, da die APInoch nicht unterstützt wird. Dies hat zur Folge, dass das
’window’ Objekt die Funktionen zum Öﬀnen des Auswahldialogs nicht kennt und somit
kein Dialogfenster erscheint.
3.4 Implementierung der nativen Android App
Android hat viele Komponenten, die für die Umsetzung des erstellten Mockups verwendet
werden können. Für das Speichern der Journal-Einträge wird auch hier Firestore verwendet.";0
Das Problem ist, dass  jeder Thread, der sie aufruft blockiert wird, bis die Funktion  die nötige Information zurückbekommt .60 Der Overhead  z. B.  der Speicher - und  Kontextwechse l ist mit den Threads  verbunden , bei zu vielen blockierten Threads  leidet  die Performance des ganzen Prozesses. Hierfür gibt es eine Alternative   namens callback styles , die effizienter sind , weil hier die  Threads nicht blockiert  werden müssen. Stattdessen wird die Callback -Funktion aufgerufen, wenn sie bereit  ist. Zum Problem kann bei dieser Variante werden, dass möglicherweise zu viele  Ebenen zu tief ineinander verschachtelt werden. Kotlin bietet einen Ansatz, der  das Beste aus beiden Welten  vereint.  Coroutines bieten die Möglichkeit leicht  verfolgbaren Code zu schreiben, der sequenziell  aussieht  und gleichzeitig versucht,  das Blockieren zu vieler Threads zu vermeiden.  Coroutines  bauen auf einem  Konzept auf, das Continuations genannt wird , was nichts anderes ist als eine  Abstraktion, die den aktuellen Zustand des Programms oder der Threads beschreibt,   um gewährleisten zu können,  dass bei einem Programmabbruch oder -absturz  allein   die Abstraktion genügend Details enthält, um das Programm in denselben Zustand  wieder aufzubauen , in dem es unterbrochen wurde.62 Das Hauptziel der Coroutine - Bibliothek besteht darin , die Threads zu  abstrahieren und die Programmierung zu  vereinfachen . Dennoch  müssen alle Funktione n irgendwann auf eine n Thread und  zu einem bestimmten Zeitpunkt ausgeführt werden , das ist ein wesentlicher  Bestandteil der Art und Weise wie Betriebssysteme funktionieren . Das bedeutet,  dass für die Umsetzung von ‚pausierten‘ Funktionen eine Art Container oder eine  Task laufen  muss , der einen Thread verwendet. Dieser Container bzw. diese Task  wird als Coroutine bezeichnet.63 Obwohl jede Coroutine auf einem Thread laufen  muss, ist sie nicht an einen bestimmten Threa d gebunden  und der tatsächliche  Thread kann sich  sogar  ändern, wenn eine Funktion nach einer Unterbrechung  wieder gestartet  wird.;0
Das Buch selber ist in mehrere Kapitel unterteilt, die nach Tagen benannt sind. Dabei startet an einem „Montag“, so auch die Kapitelbezeichnung, und endet sieben Tage später an dem nächsten „Montag“. Das Abschlusskapitel wird dann nur als „Einige Tage später“ bezeichnet. Das Buch wird aus verschiedenen Perspektiven erzählt. Darunter gehören einmal die Protagonistin „Cynthia Bonsant“, die „Freemee“ Unternehmenszentrale und die Ermittlungsbehörden wie das Federal Bureau of Investigation (FBI). Dadurch lässt sich das Buch in drei Handlungsstränge einteilen. Handlungsstrang 1: „Cynthia Bonsant, Familie und Daily“: •„Adam Denham“ – ein Freund von Viola •„Anthony Heast“ – Chefredakteur des Dailys •„Chander Argawal“ – IT-Forensiker •„Cynthia (Cyn) Bonsant“ – Journalistin beim Daily •„Edward Brickle“ – ein Freund von Viola •„Jeff“ – Mitarbeiter des Technikressorts beim Daily •„Viola Bonsant“ – Tochter von Cynthia Bonsant Handlungsstrang 2: „Freemee“: •„Alice Kinkaid“ – Kommunikationschefin von Freemee •„Carl Montik“ – Gründer von Freemee, verantwortlich für Forschung, Programmie- rung und Entwicklung •„Jenna Wojczewski“ – Finanzvorstand von Freemee •„Jozef Abberidan“ – Vorstandsmitglied von Freemee •„Kim Huang“ – Vorstandsmitglied von Freemee •„Will Dekkert“ – Kommunikationsvorstand von Freemee Handlungsstrang 3: „Pennicott, FBI und EmerSec“: •„Erben Pennicott“ – Stabschef des Weißen Hauses •„Henry Emerald“ – Gründer von EmerSec, Anteilseigner von Freemee •„Joaquim Proust“ – Leiter von EmerSec •„Jonathan Stem“ – Assistant-Director beim FBI •„Luís“ – Digital-Detective beim FBI •„Marten Carson“ – FBI-Agent •„Richard Straiten“ – Homicide-Detective der Antiterroreinheit des New York City Police Department (NYPD) Mit dem Überblick über Handlungsstränge und den dazugehörigen Charakteren, muss zum Anfang der Blick auf die Protagonistin gerichtet werden. Die Protagonistin Cynthia Bonsant lebt in London und ist Journalistin bei einer lokalen Zeitung die „Daily“ heißt.;0
Ein Ausblick auf mögliche Weiterentwicklungen  Die zunehmende Komplexität und Dynamik im Bereich des Software Engineerings erfordert von Studierenden nicht nur technisches Wissen, sondern auch ausgeprägte Fähigkeiten im Projektmanagement. Ein effektives Aufgabenmanagement-Tool kann hierbei eine entscheidende Rolle spielen, indem es die Organisation, Planung und Nachverfolgung von Aufgaben unterstützt. In diesem Kontext ist eine fundierte Anforderungsanalyse von zentraler Bedeutung, um die Bedürfnisse der Nutzer zu identifizieren und die Funktionalitäten des Tools optimal zu gestalten.   Die grundlegenden Anforderungen an ein solches Tool lassen sich in mehrere Kategorien unterteilenBenutzerfreundlichkeit, Funktionalität, Integrationsfähigkeit, Skalierbarkeit und Unterstützung von kollaborativen Arbeitsprozessen. Zunächst muss das Tool eine intuitive Benutzeroberfläche bieten, die es Studierenden ermöglicht, ohne umfassende Einarbeitung schnell produktiv zu werden. Ein einfaches und klares Design, kombiniert mit einer effektiven Such- und Filterfunktion, kann hierbei die Benutzererfahrung erheblich verbessern.  In Bezug auf die Funktionalität sollten grundlegende Features wie die Erstellung, Zuweisung und Priorisierung von Aufgaben, die Möglichkeit zur Fortschrittsverfolgung sowie die Integration von Deadlines implementiert werden. Darüber hinaus könnte die Einführung von Kanban-Boards oder Gantt-Diagrammen den Studierenden helfen, ihre Projekte visuell zu organisieren und den Fortschritt zu überwachen. Eine besondere Herausforderung stellt die Unterstützung von agilen Methoden dar, die in vielen Softwareprojekten Anwendung finden. Das Tool sollte daher die Möglichkeit bieten, Scrum- oder Kanban-Boards zu nutzen, um den iterativen Entwicklungsprozess zu fördern.  Die Integrationsfähigkeit mit anderen Software-Tools, wie beispielsweise Versionskontrollsystemen (z.B. Git) oder Kommunikationsplattformen (z.B. Slack), ist ein weiterer wichtiger Aspekt. Diese Integration kann den Studierenden helfen, ihre Arbeitsabläufe zu optimieren und den Informationsfluss zwischen verschiedenen Tools zu verbessern. Zudem sollte das Aufgabenmanagement-Tool in der Lage sein, mit bestehenden Lernmanagement-Systemen (LMS) zu interagieren, um eine nahtlose Nutzererfahrung zu gewährleisten.  Ein weiterer zentraler Punkt ist die Skalierbarkeit des Tools. Da Studierende oft in wechselnden Gruppen an Projekten arbeiten, sollte das Tool in der Lage sein, sowohl kleine als auch große Teams zu unterstützen und sich flexibel an unterschiedliche Projektgrößen anzupassen. Dies erfordert eine durchdachte Architektur, die es ermöglicht, Benutzerrollen und Berechtigungen dynamisch zu verwalten.  Ein besonders innovativer Aspekt, der in zukünftige Entwicklungen des Tools einfließen könnte, ist der Einsatz von Künstlicher Intelligenz (KI). KI-gestützte Funktionen könnten dabei helfen, Aufgaben automatisch zu priorisieren oder Deadlines basierend auf dem bisherigen Arbeitsverhalten der Nutzer vorzuschlagen. Darüber hinaus könnten intelligente Analysen der Teamleistung durchgeführt werden, um den Studierenden wertvolle Einblicke in ihre Arbeitsweise und -effizienz zu geben.  Ein weiterer vielversprechender Ansatz ist die Implementierung von Gamification-Elementen, um die Motivation der Studierenden zu steigern. Durch das Einführen von Belohnungen, Fortschrittsanzeigen oder;1
"Eine Analyse und Fazit  Die zunehmende Digitalisierung hat die Art und Weise, wie Unternehmen und Institutionen ihre Inhalte verwalten, revolutioniert. Content-Management-Systeme (CMS) sind dabei zu entscheidenden Werkzeugen geworden, die es Nutzern ermöglichen, Inhalte effizient zu erstellen, zu verwalten und zu publizieren. Diese wissenschaftliche Untersuchung zielt darauf ab, verschiedene CMS hinsichtlich ihrer Funktionalitäten, Benutzerfreundlichkeit, Flexibilität und Skalierbarkeit zu vergleichen und ein prägnantes Fazit über die Eignung einzelner Systeme für spezifische Anforderungen zu ziehen.  In der ersten Phase der Analyse wurden vier der bekanntesten CMS identifiziertWordPress, Joomla, Drupal und Typo3. Farbenfroh und einfach zu bedienen, hat sich WordPress als das am weitesten verbreitete CMS etabliert. Seine intuitiven Benutzeroberflächen und eine Vielzahl von Plugins machen es zur bevorzugten Wahl für kleinere bis mittelgroße Websites oder Blogs. Joomla hingegen bietet eine ausgewogene Mischung aus Benutzerfreundlichkeit und erweiterten Funktionen, was es ideal für komplexere Seiten macht, während Drupal vor allem durch seine Flexibilität und Scalierbarkeit besticht, was es zur optimalen Wahl für große, datenintensive Websites macht. Typo3 schließlich, obwohl weniger bekannt, hat sich als leistungsstark hinsichtlich der Verwaltung komplexer Unternehmensinhalte erwiesen.  Im weiteren Verlauf der Analyse wurden die zentralen Kriterien für die Gegenüberstellung definiert. Diese umfassten die Benutzerfreundlichkeit, Anpassungsfähigkeit, Sicherheitsmerkmale und die jeweilige Community-Unterstützung. Während WordPress hier meist einen leichten Vorteil in Bezug auf die Benutzerfreundlichkeit ausweist, zeigt Drupal seine Stärke in der Anpassbarkeit für komplexe Anwendungen. Joomla präsentiert sich als solider Mittelweg, doch Typo3 überrascht mit seiner Robustheit in professionellen Umgebungen.  Ein weiterer wichtiger Aspekt ist die Sicherheitsarchitektur der CMS. In einer Zeit, in der Cyberangriffe zunehmen, spielt die Sicherheit der Systeme eine entscheidende Rolle. Im Allgemeinen weisen Drupal und Typo3 stärkere Sicherheitsmerkmale auf, da sie regelmäßig aktualisiert werden und eine engagierte Entwicklergemeinschaft besitzen, die schnell auf Sicherheitsanforderungen reagiert. Im Gegensatz dazu hat WordPress in der Vergangenheit einige Sicherheitsprobleme erlebt, vor allem aufgrund der Vielzahl an Plugins, von denen einige nicht ausreichend gewartet werden.  Das Fazit dieser Untersuchung lässt sich wie folgt zusammenfassenDie Wahl des geeigneten Content-Management-Systems ist stark abhängig von den spezifischen Bedürfnissen der Nutzer. Für kleinere Unternehmen oder persönliche Projekte ist WordPress aufgrund seiner Benutzerfreundlichkeit und der großen Zahl an verfügbaren Plugins oft die beste Wahl. Joomla bietet sich für Nutzer an, die eine robuste Funktionalität bei gleichzeitiger Benutzerfreundlichkeit benötigen. Für große Unternehmen oder Organisationen, die maßgeschneiderte Lösungen benötigen, sind Drupal und Typo3 aufgrund ihrer Flexibilität und Sicherheitsmerkmale empfehlenswert. Insgesamt zeigt sich, dass es kein „einheitliches“ CMS gibt; vielmehr hängt die Entscheidung von den individuellen Anforderungen, den technischen Kenntnissen der Nutzer und den langfristigen Zielen der jeweiligen Website ab. Die Auseinandersetzung mit den jeweiligen Vor- und Nachteilen der Systeme ist somit von zentraler Relevanz bei der Auswahl des geeigneten Content-Management-Systems.";1
    Die fortschreitende Automatisierung im Verkehrssektor erfordert innovative Lösungen, um die Sicherheit und Effizienz des Straßenverkehrs zu erhöhen. Eine vielversprechende Technologie in diesem Kontext ist die Fahrzeugfernsteuerung, die es ermöglicht, Fahrzeuge aus der Ferne zu steuern und gleichzeitig Kollisionsrisiken zu minimieren. Diese Arbeit beschreibt die Entwicklung einer Fahrzeugfernsteuerung auf Basis des IEEE 802.15 Standards, der insbesondere für drahtlose Kommunikationssysteme im Nahbereich konzipiert wurde. Der Fokus liegt auf der , die eine effektive Kollisionsvermeidung ermöglicht.  Technologischer Hintergrund  Der IEEE 802.15 Standard umfasst verschiedene Protokolle für drahtlose persönliche Netzwerke (WPANs), die sich durch niedrigen Energieverbrauch und hohe Flexibilität auszeichnen. Diese Eigenschaften sind besonders vorteilhaft für mobile Anwendungen wie die Fahrzeugfernsteuerung. Die Verwendung von IEEE 802.15.4, einem der am weitesten verbreiteten Protokolle innerhalb dieses Standards, bietet eine robuste Grundlage für die Kommunikation zwischen dem Steuergerät und dem Fahrzeug.  Systemarchitektur  Die Systemarchitektur besteht aus mehreren Komponenteneinem Fernsteuerungsmodul, einem Fahrzeugmodul und einem Kollisionsvermeidungssystem. Das Fernsteuerungsmodul, ausgestattet mit einem Mikrocontroller und einem IEEE 802.15.4-kompatiblen Transceiver, sendet Steuerbefehle an das Fahrzeugmodul. Letzteres empfängt die Befehle und steuert die Fahrzeugmechanik entsprechend. Zur Implementierung der Kollisionsvermeidung wird ein Lidar-Sensorsystem integriert, das die Umgebung des Fahrzeugs in Echtzeit scannt und potenzielle Hindernisse identifiziert.  Implementierung  Die Implementierung der Fahrzeugfernsteuerung erfolgt in mehreren Schritten 1. Hardware-AuswahlDie Auswahl geeigneter Hardwarekomponenten ist entscheidend. Der Mikrocontroller sollte über ausreichend Rechenleistung und Speicherkapazität verfügen, um die Steuerbefehle zu verarbeiten und die Sensordaten auszuwerten. Der gewählte Transceiver muss eine zuverlässige Kommunikation im Nahbereich gewährleisten.  2. Entwicklung der KommunikationsprotokolleAuf Basis von IEEE 802.15.4 werden spezifische Kommunikationsprotokolle entwickelt, die die Übertragung von Steuerbefehlen und Sensordaten optimieren. Hierbei kommen Mechanismen zur Fehlerkorrektur und -erkennung zum Einsatz, um die Robustheit der Kommunikation zu erhöhen.  3. Integration des KollisionsvermeidungssystemsDas Lidar-Sensorsystem wird in das Fahrzeugmodul integriert. Die Sensordaten werden kontinuierlich erfasst und in Echtzeit analysiert. Ein Algorithmus zur Kollisionsvermeidung wird implementiert, der auf Basis der Sensordaten Entscheidungen trifft. Bei drohenden Kollisionen kann das System automatisch die Geschwindigkeit des Fahrzeugs anpassen oder das Fahrzeug stoppen.  4. Test und ValidierungNach der Implementierung erfolgt eine umfassende Testphase, in der die Kommunikation zwischen den Modulen sowie die Funktionsweise des Kollisionsvermeidungss;1
"Beim Gießen der Pflanzen kann es durchaus vorkommen, dass die Elektronikbauteile
am oberen Ende des kapazitiven Bodenfeuchtigkeitssensors nass werden. Um diese vor
der Feuchtigkeit zu schützen, können diese beispielsweise mit Klarlack versiegelt werden.
Andreas Spiess empfiehlt in seinem Youtube Video zudem die Kanten des Sensors auf diese
Weise zu versiegeln, da bei der Produktion keine Isolierung auf die Kanten der kapazitiven
Bodenfeuchtigkeitssensoren aufgebracht wird und somit an diesen Stellen Feuchtigkeit in
die Sensoren eindringen kann.  Im Rahmen der Studienarbeit wurde
der kapazitive Bodenfeuchtigkeitssensor rundum mit Nagellack bepinselt.1
Checkliste beim Kauf kapazitiver Bodenfeuchtigkeitssensoren
Bei der Recherche zu den Bodenfeuchtigkeitssensoren wurde durch den Youtube Algo-
rithmus ein weiteres Youtube Video  zu kapazitiven Bodenfeuchtigkeitssensoren
vorgeschlagen. In diesem Youtube Video werden 38 der online erhältlichen kapazitiven
Bodenfeuchtigkeitssensoren getestet. Optisch sehen diese auf den ersten Blick alle gleich
aus. Bei detaillierter Betrachtung zeigen sich jedoch minimale Unterschiede, die sich je-
doch gravierend auf die Messergebnisse auswirken können. Beim Kauf von kapazitiven
Feuchtigkeitssensoren sollte laut  daher auf folgende Eigenschaften geachtet
werden:
•Prüfen, ob der 662K Spannungsregler vorhanden ist
•Timer-Chip muss vom Typ TLC555C oder TLC555I sein (prüfbar über die Beschrif-
tung des Timer-Chips)
•Prüfen, ob das Durchgangsloch zwischen den Widerständen richtig positioniert ist
Laut dem Youtube Video  liegt bei 82 Prozent der getesteten kapazitiven
Bodenfeuchtigkeitssensoren mindestens einer dieser Fehler vor. Es stellte sich auch in der
Praxis als schwierig heraus, Bodenfeuchtigkeitssensoren in den gängigen Onlineshops zu
ﬁnden, die diese Kriterien erfüllten. Im Rahmen dieser Studienarbeit werden auf Amazon
erhältliche kapazitive Bodenfeuchtigkeitssensoren verwendet, die diese Kriterien erfüllen
und auch - wie bereits in Abbildung 4.17 gezeigt - brauchbare Messergebnisse liefern.";0
Evaluierung: Java vs. Kotlin  In der heutigen Softwareentwicklung sind Programmiersprachen nicht nur Werkzeuge, sondern auch entscheidende Faktoren für die Effizienz und Qualität von Projekten. Insbesondere im Bereich der Android-Entwicklung hat sich Kotlin als ernstzunehmender Konkurrent zu Java etabliert. Diese Evaluierung untersucht die Unterschiede, Vor- und Nachteile beider Sprachen, um eine fundierte Grundlage für die Wahl der geeigneten Programmiersprache zu bieten.  Java, als eine der ältesten und am weitesten verbreiteten Programmiersprachen, bietet eine robuste und bewährte Plattform für die Entwicklung von Anwendungen. Ihre Syntax ist klar und verständlich, was sie besonders für Einsteiger attraktiv macht. Zudem profitiert Java von einer umfangreichen Bibliothekslandschaft und einer großen Entwickler-Community, die den Austausch von Wissen und Ressourcen fördert. Ein weiterer Vorteil ist die Plattformunabhängigkeit durch die Java Virtual Machine (JVM), die es ermöglicht, Anwendungen auf verschiedenen Betriebssystemen auszuführen.  Auf der anderen Seite steht Kotlin, das 2011 von JetBrains entwickelt wurde und 2017 von Google als offizielle Sprache für Android-Entwicklung anerkannt wurde. Kotlin bietet zahlreiche moderne Sprachfeatures, die die Entwicklung effizienter und weniger fehleranfällig gestalten. Dazu gehören unter anderem Null-Sicherheit, Erweiterungsfunktionen und eine prägnantere Syntax. Diese Merkmale tragen dazu bei, den Code lesbarer und wartbarer zu machen, was in großen Projekten von entscheidender Bedeutung ist. Kotlin ist zudem vollständig interoperabel mit Java, was bedeutet, dass Entwickler bestehende Java-Bibliotheken und -Frameworks weiterhin nutzen können.  Ein wesentlicher Aspekt der Evaluierung ist die Lernkurve. Während Java aufgrund seiner langen Geschichte und weit verbreiteten Nutzung in vielen Lehrplänen und Ressourcen gut dokumentiert ist, kann Kotlin für Entwickler, die bereits mit Java vertraut sind, eine steile Lernkurve darstellen. Die neuen Konzepte und Paradigmen, die Kotlin einführt, erfordern ein Umdenken und eine Anpassung an moderne Programmieransätze. Dennoch berichten viele Entwickler von einer schnelleren Produktivität und Zufriedenheit, sobald sie sich mit Kotlin vertraut gemacht haben.  In Bezug auf die Performance sind beide Sprachen vergleichbar, da Kotlin auf der JVM läuft und somit ähnliche Ausführungsgeschwindigkeiten wie Java bietet. Allerdings kann Kotlin in bestimmten Anwendungsfällen, insbesondere bei der Nutzung von Funktionen höherer Ordnung und anderen modernen Sprachfeatures, eine bessere Performance erzielen.  Zusammenfassend lässt sich sagen, dass sowohl Java als auch Kotlin ihre eigenen Stärken und Schwächen haben. Java punktet mit seiner Stabilität, umfangreichen Community und Vertrautheit, während Kotlin durch moderne Features und eine verbesserte Entwicklererfahrung überzeugt. Die Wahl zwischen den beiden Sprachen sollte daher nicht nur auf technischen Aspekten basieren, sondern auch auf den spezifischen Anforderungen des Projekts und den Präferenzen des Entwicklerteams. In einer zunehmend dynamischen Softwarelandschaft könnte Kotlin jedoch die Zukunft der Android-Entwicklung prägen und Java in bestimmten Bereichen zunehmend ersetzen.;1
"Nach der Begutachtung der verschiedenen Bibliotheken ist die Wahl auf MUI 
gefallen. Die Entscheidung wurde aufgrund der zur Verfügung stehenden Komponenten
sowie der intuitiven Nutzung getroﬀen. Ferner ist es für die Verwendung mit React gedacht
und setzt die Material Design Vorgaben um.
Für die Auswahl des Backendtechnologie wurde keine Analyse von verschieden Produkten
vorgenommen. Dies ist auf die geringe Größe und Komplexität zurückzuführen. Ein
wichtiger Faktor ist die Verfügbarkeit der web-push Bibliothek sowie die Möglichkeit
Aufgaben zu geplanten Zeitpunkten auszuführen. Für das Backend wird Node.js mit dem
express Webframework  verwendet.
2.2 Native Android App
Native Apps sind Apps, die speziﬁsch für eine Plattform, wie Android oder iOS, entwickelt
werden. In dieser Arbeit wird eine native App für Android gewählt. Native Android
Apps werden mit Java oder Kotlin geschrieben. Für die Entwicklung empfehlt sich die
Verwendung von Android Studio, einer Integrated Development Environment ( IDE), die
von Google für diesen Zweck bereitgestellt wird. Im Rahmen dieser Arbeit wird die
Umsetzung in der Programmiersprache Kotlin durchgeführt.
2.2.1 Terminologie und die verwendeten Komponenten von Android
Für die Umsetzung werden verschiedene Komponenten von Android benötigt. In diesem
Unterabschnitt werden die wichtigsten verwendeten Begriﬀe erklärt.
Manifest Jede Android App muss ein Manifest haben, das sich in der Wurzel der
Projektstruktur beﬁndet. Es enthält Informationen, die vom Compiler, dem ausführenden
Betriebssystem sowie dem Google Play-Store benötigt werden. In Manifest müssen alle
Berechtigungen angegeben werden, die von der App genutzt werden, sowie alle verwendeten
Hard- und Software Features. Ferner müssen alle hier aufgelisteten App Komponenten
angegeben werden:
•Activities
•Services
•Broadcast Receivers
•Content Providers
Diese Komponenten werden als Einstieg in die App verwendet. Die verschiedenen Berech-
tigungen und Einstiegspunkte der Beispiel-Journaling App werden bei der Einführung
jeweils aufgezeigt.";0
      Die kontinuierliche Evolution der Fahrzeugtechnologie hat das Potential, die Sicherheitsstandards im Verkehr erheblich zu verbessern. Insbesondere die Implementierung von Fernsteuerungssystemen eröffnet neue Möglichkeiten, Fahrzeuge autonomer und sicherer zu steuern. Vor diesem Hintergrund wird in diesem Text das Konzept zur Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung auf Basis des IEEE 802.15 Standards präsentiert. IEEE 802.15 regelt die Spezifikationen für Personal Area Networks (PANs) und bietet eine geeignete Plattform für die drahtlose Kommunikation zwischen Fahrzeugen und deren Steuerungssystemen.   Zielsetzung  Das primäre Ziel dieses Projektes ist die Entwicklung eines Prototyps, der eine präzise Fahrzeugsteuerung aus der Ferne ermöglicht und gleichzeitig Mechanismen zur Kollisionsvermeidung integriert. Dabei wird eine robuste Kommunikationsarchitektur angestrebt, die den spezifischen Anforderungen an Latenz, Reichweite und Zuverlässigkeit gerecht wird.   Konzeptualisierung der Steuerung  1. Kommunikationsprotokoll und Architektur    - Die Implementierung basiert auf dem IEEE 802.15.4 Standard, der für drahtlose Sensor- und Aktuatornetzwerke optimiert ist und sich durch niedrigen Energieverbrauch, flexible Topologien und kosteneffiziente Implementierung auszeichnet.     - Zur Gewährleistung der notwendigen Bandbreite und der stabile Übertragung von Steuerbefehlen werden zusätzliche Schichten in die Protokollarchitektur integriert, die Ladezeiten reduzieren.   2. Systemkomponenten    - SteuergerätEin zentrales Steuergerät, das die Eingaben des Nutzers in Steuerbefehle umwandelt und diese durch das IEEE 802.15 Netzwerk an das Fahrzeug überträgt.    - Empfänger im FahrzeugRealisiert durch integrierte Mikrokontroller, die die empfangenen Befehle verarbeiten und die Fahrzeugsteuerung anpassen.    - SensortechnologieVerwendung von Lidar, Ultraschall und Kameras zur Erkennung von Hindernissen und Umsetzung der Kollisionsvermeidungsstrategien. Diese Sensoren sind direkt mit dem Steuergerät verbunden und liefern in Echtzeit Daten zur Umgebung.  3. Kollisionsvermeidung    - Die Implementierung eines fortgeschrittenen Algorithmen, der auf maschinellem Lernen basiert. Dieser lernt aus historischen Daten und erstellt ein Verhaltensmodell, um potenzielle Kollisionen vorherzusagen.    - Integration eines Mehrfachsensoransatzes zur Erhöhung der Umgebungswahrnehmung und zur Verbesserung der Reaktionsfähigkeit der Fahrzeugsteuerung. Eine Fusion der Sensordaten optimiert die Genauigkeit der Kollisionserkennung.  4. Benutzeroberfläche    - Entwicklung einer intuitiven Benutzeroberfläche, die es Nutzern erlaubt, Fahrzeugbewegungen einfach und effizient zu steuern. Feedbackmöglichkeiten, wie z.B. akustische und visuelle Signale, erhöhen die Benutzerfreundlichkeit und Sicherheit.   Prototyping und Implementierung  Die iterative Prototypenerstellung wird in verschiedenen Phasen durchgeführt 1. PrototypenentwicklungAufbau eines ersten Modells unter Verwendung kostengünstiger Mikrokontroller und Sensoren, um die Grundfunktionalitäten zu testen. 2. FeldtestsDurchführung von kontrollierten Tests in unterschiedlichen Umgebungen, um die Robustheit und Effektivität der Kollisionsvermeidung zu evaluieren. 3. OptimierungBasierend auf den Ergebnissen der Tests wird das System hinsichtlich Latenz, Reichweite und Nutzerfreundlichkeit optimiert.   Ausblick  Die  stellt einen innovativen Ansatz dar, um die Sicherheit im Straßenverkehr zu erhöhen. Innovative Kommunikationsstrategien und ein sorgfältig entwickelter Algorithmus zur Kollisionsvermeidung bieten die Grundlage für zukünftige Anwendungen, die über eine reine Fernsteuerung hinausgehen – beispielsweise in der Automatisierung der Logistik oder der Personenbeförderung.  Mit dieser Forschungsarbeit wird ein wichtiger Baustein gelegt für die Weiterentwicklung autonomer Fahrzeuge und deren sicherer Integration in den urbanen Raum. Die gewonnenen Erkenntnisse könnten einen bedeutenden Beitrag zur künftigen Mobilität leisten, indem sie Fahrzeuge nicht nur effizienter, sondern auch sicherer machen.;1
Vergleich zwischen Jetpack Compose und dem klassischen Ansatz in der App-Entwicklung  Die Entwicklung von mobilen Anwendungen hat sich im Laufe der Jahre erheblich gewandelt, wobei sich neue Frameworks und Paradigmen etabliert haben, um den sich ständig ändernden Anforderungen der Entwickler und Nutzer gerecht zu werden. Insbesondere Jetpack Compose, das moderne Toolkit für die UI-Entwicklung in Android, steht im Kontrast zu den klassischen Ansätzen, die auf XML-Layouts und imperative Programmierung basieren. Dieser Vergleich beleuchtet die wesentlichen Unterschiede zwischen diesen beiden Ansätzen und deren Auswirkungen auf die App-Entwicklung.  Ein zentraler Aspekt des klassischen Ansatzes ist die Verwendung von XML-Dateien zur Definition von Benutzeroberflächen. Entwickler erstellen Layouts durch das Schreiben von XML-Code, der dann in Java oder Kotlin mit logischen Komponenten verbunden wird. Diese Trennung von Layout und Logik kann zu einer erhöhten Komplexität führen, insbesondere bei der Pflege und Skalierung von Anwendungen. Änderungen an der Benutzeroberfläche erfordern oft Anpassungen in mehreren Dateien, was die Wartbarkeit und Lesbarkeit des Codes beeinträchtigen kann. Zudem ist der Entwicklungsprozess oft iterativ, da Änderungen am UI häufig getestet und erneut angepasst werden müssen.  Im Gegensatz dazu verfolgt Jetpack Compose einen deklarativen Ansatz, bei dem die Benutzeroberfläche direkt in Kotlin-Code beschrieben wird. Dies ermöglicht eine nahtlose Integration von UI-Elementen und deren Logik, was die Entwicklungszeit erheblich verkürzt. Entwickler können UI-Komponenten als Funktionen definieren und diese dynamisch anpassen, basierend auf dem aktuellen Zustand der Anwendung. Diese Flexibilität führt zu einem klareren und verständlicheren Code, da die Struktur der Benutzeroberfläche und deren Verhalten in einem zusammenhängenden Kontext betrachtet werden. Darüber hinaus erleichtert der deklarative Ansatz die Implementierung von Zustandsverwaltung und Animationen, da Änderungen am Zustand sofort in der Benutzeroberfläche reflektiert werden.  Ein weiterer Vorteil von Jetpack Compose ist die Unterstützung für Material Design und die Möglichkeit, benutzerdefinierte UI-Elemente mit minimalem Aufwand zu erstellen. Das Framework bietet eine Vielzahl von vorgefertigten Komponenten, die leicht anpassbar sind, und fördert somit die Einhaltung von Designrichtlinien. Im klassischen Ansatz hingegen müssen Entwickler oft eigene UI-Komponenten erstellen oder umfangreiche Anpassungen an bestehenden Komponenten vornehmen, was zeitaufwendig sein kann.  Allerdings bringt der Wechsel zu Jetpack Compose auch Herausforderungen mit sich. Da es sich um eine relativ neue Technologie handelt, können Entwickler, die mit dem klassischen Ansatz vertraut sind, eine steile Lernkurve erleben. Zudem ist die Community und die Dokumentation im Vergleich zu den etablierten XML-basierten Ansätzen noch im Aufbau begriffen, was gelegentlich zu Unsicherheiten bei der Implementierung führen kann.  Zusammenfassend lässt sich sagen, dass Jetpack Compose im Vergleich zum klassischen Ansatz zahlreiche Vorteile bietet, insbesondere in Bezug auf Codeklarheit, Wartbarkeit und Entwicklungsgeschwindigkeit. Der deklarative Ansatz ermöglicht eine intuitivere und effizientere Gestaltung von Benutzeroberflächen, während die enge Integration von Logik und UI die Entwicklung dynamischer Anwendungen erleichtert. Dennoch müssen Entwickler die Herausforderungen der neuen Technologie berücksichtigen und bereit sein, sich in das Framework einzuarbeiten, um das volle Potenzial von Jetpack Compose auszuschöpfen;1
 Tracking der Bodenfeuchtigkeit mit LoRaWAN und The Things Network (TTN)  Die Bodenfeuchtigkeit ist ein kritischer Faktor in der Landwirtschaft, des Wassermanagements und der hydrogeologischen Forschung. Sie beeinflusst nicht nur das Wachstum von Pflanzen, sondern spielt auch eine bedeutende Rolle bei der Nährstoffdynamik und der Erhaltung der ökologischen Balance. In den letzten Jahren hat sich das Internet der Dinge (IoT) als Schlüsseltechnologie herauskristallisiert, durch die Daten über Umgebungsbedingungen wie Bodenfeuchtigkeit in Echtzeit überwacht und analysiert werden können. Eine besonders vielversprechende Technologie in diesem Kontext ist Long Range Wide Area Network (LoRaWAN), unterstützt durch Plattformen wie The Things Network (TTN).   1.  der Bodenfeuchtigkeitsmessung  Die Messung der Bodenfeuchtigkeit basiert auf physikalischen Prinzipien, die unterschiedliche Methoden und Technologien beleuchten. Zu den häufig verwendeten Verfahren zählen durch den Boden penetrierende Techniken, die Veränderungen in der elektrischen Leitfähigkeit und die dielektrische Spezifität des Bodenmaterials ermitteln. Kapazitive Sensoren, die die Änderung der Dielektrizitätskonstante im Boden registrieren, oder ähnliche resistive Sensoren, die den Wassergehalt physisch durch Leitfähigkeit messen, stellen Praktiken dar, die sowohl präzise als auch kosteneffizient sind. Diese Methoden liefern quantitative Daten zur Bodenfeuchtigkeit, die auf verschiedene Anwendungsbereiche übertragbar sind.   2. LoRaWAN als Übertragungstechnologie  LoRaWAN ist ein auf dem LoRa (Long Range) Protokoll basierendes Netzwerkprotokoll und zeichnet sich durch niedrigen Energieverbrauch, hohe Reichweite und große Netzwerk-Kapazitäten aus. Durch seine Architektur kann es Tausenden von Geräten ermöglichen, Daten über Gebühren-freie Frequenzen in ländlichen oder städtischen Gebieten mit geringer Infrastrukturanbindung zu übertragen. Entscheidende Komponenten eines LoRaWAN-Netzwerks sind Sensoren, sogenannte Gateways sowie ein Internet-Backend, das spezifisch für Datenspeicherung, -verarbeitung und -analytik dient. Die transportierten Datenpakete sind für einmalige Messungsintervallen von wenigen Bytes optimiert, wodurch das Netzwerk ressourcenschonend betrieben werden kann.   3. The Things Network (TTN)  The Things Network ist eine offene, communitybasierte Plattform, die es ermöglicht, LoRaWAN-basierte Anwendungen schnell und effizient zu erstellen und bereitzustellen. TTN stellt eine Reihe von Tools zur Verfügung, mit denen Daten von verschiedenen Sensoren erfasst, visualisiert und analysiert werden können. Aktuell bietet TTN Anwendern die Möglichkeit die netzwerkweiten Ressourcen wie Gateways und Sicherheitsansprüche der LoRaWAN-Infrastruktur zu nutzen. Datenintegrierte Klärung, Aufbereitung und Nutzung sind über Apps, Datenbanken oder Programmierschnittstellen ermöglicht, was umfangreiche Apps für reale Weltlösungen generiert. Diese Einladungen zur Zusammenarbeit innerhalb von TTN begünstigen Gemeinschaft-Anwendungen und Verkehrsdatenanalyse durch den Austausch öffentlicher Nutzerdaten.   4. Entwicklung und Umsetzung eines LoRaWAN-gestützten Bodenfeuchtigkeits;1
"2.3.1 Gateways
Jedes Lora-Gateway wird, mit Hilfe einer lokal abgelegten Konﬁgurationsdatei, bei ei-
nem LoRaWAN-Netzwerkserver registriert. Ein Gateway empfängt hauptsächlich LoRa-
Nachrichten von den jeweiligen Nodes, welche sich in Reichweite beﬁnden und leitet diese
im Anschluss an den konﬁgurierten LoRaWAN-Netzwerkserver weiter. Diese Weiterlei-
tung geschieht über eine aktive Internetverbindung zum Beispiel durch Mobilfunk, WiFi,
Ethernet oder Glasfaser. In ländlichen Gebieten kann ein einzelnes Gateway Nachrichten
über eine Entfernung von mehr als 15 Kilometern empfangen und senden, in dichten
städtischen Umgebungen können diese bis zu fünf Kilometer weit übertragen werden. Ein
einziges Acht-Kanal-Gateway kann beispielsweise innerhalb von 24 Stunden bis zu 1,5
Millionen Nachrichten verarbeiten. Wenn jeder Node pro Stunde eine Nachricht sendet,
kann ein solches Gateway alleine bis zu 60.000 Geräte unterstützen. Falls mehr Nachrichten
übertragen, beziehungsweise mehr Nodes eingebunden werden, muss einfach ein weiteres
Gateways in diesem Bereich hinzugefügt werden, um so die Last besser zu verteilen. Nodes,
die sich in der Nähe eines Gateways beﬁndet, übertragen die Daten mit einem niedrigen
Spreadingfaktor, da hier ein sehr geringes Link-Budget benötigt wird. Ein höherer Sprea-
dingfaktor wird verwendet, je weiter ein Node von einem Gateway entfernt ist, was wie in
Abschnitt 2.2 erwähnt, den Verarbeitungsaufwand der Nachricht steigert und die Bitrate
verringert.   Das Link-Budget deﬁniert die Verstärkungen und
Dämpfungen der Komponenten, die ein Signal von der Sendeantenne bis zum Empfang
durch eine Empfangsantenne erfährt. 
Gateways können in zwei verschiedene Typen unterteilt werden: Außen- und Innen-
Gateways. Innen-Gateways können auch „Picozell“ und Gateways für den Außenbereich
auch „Makrozell“ genannt werden.
Indoor-Gateway
DieGatewaysfürdenInnenbereichsindgünstigerundwerdenzurAbdeckungvonBereichen
wie Wohnräumen, Kellern und mehrstöckigen Gebäude verwendet, da die Sendeleistung
auch durch mehrere Wände geht. Diese Typen von Gateways haben meist intern verbaute
Antennen oder besitzen außen angebrachte „Pigtail“-Antennen. Je nach Position des
Gateway im Wohnbereich kann ein Gateway auch Nachrichten von Sensoren außerhalb der
Wohnung empfangen, die mehrere Kilometer entfernt sind.  In Abbildung 3.12
ist das Indoor-Gateway The Things Indoor Gateway ( TTIG) für den heimischen Gebrauch
zu sehen, welches direkt in die Steckdose gesteckt werden und sehr einfach eingerichtet
werden kann.";0
Mögliche und häuﬁg verwendete Modiﬁer können im vorherigen Listing 3.5 eingesehen werden. Der Modiﬁer ﬁllMaxWidth() wird beispielsweise häuﬁg bei Row Composables verwendet, um sie auf die gesamte Bildschirmbreite auszudehnen . FillMaxSize() ﬁndet häuﬁg Verwendung bei Childelementen, die den gesamten Raum ihres Parents ausfüllen sollen. . Die Liste der möglichen Modiﬁers, die das Framework zur Verfügung stellt, ist lang und kann über die oﬃzielle Dokumentation eingesehen werden. Dabei ist die Anzahl der Modiﬁers, die auf ein Element angewendet werden können, nicht eingeschränkt. Sie können in beliebiger Anzahl in einer Kette kombiniert werden. Zu Beachten ist bei der Kombination der Modiﬁer aber die Reihenfolge, in der die Funktionen aufgerufen werden. Diese spielt eine entscheidende Rolle, da jeder Modiﬁer in der Kette das Composable bearbeitet und es gleichzeitig für den nächsten Modiﬁer in der Kette vorbereitet . Dies kann unter Umständen zu ungewollten Auswirkungen führen. Ebenfalls durch das Listing 3.5 veranschaulicht wird die Tatsache, dass auch bei der Verwendung von Modiﬁern Übergabeparameter eingesetzt werden können, welche zur Wiederverwendbarkeit des Composables beitragen. Beispielsweise wird dort der Parameter rowHeight vom aufrufenden Composable (dargestellt in Listing 3.1) übergeben, um dyna- misch die Höhe der Row über den Modiﬁer height() zu steuern . Es ist zudem möglich, neben einzelnen Werten auch eine bereits deﬁnierte Kette von Modiﬁern als Parameter zu übergeben .;0
" Kapitel 2: Technische Grundlagen  Die digitale Überwachung, oft unter dem Begriff ""Überwachungstechnologie"" zusammengefasst, ist integraler Bestandteil einer Vielzahl moderner gesellschaftlicher Systeme – von Strafverfolgungsbehörden über Finanzinstitutionen bis hin zu sozialen Plattformen. Um die Möglichkeiten und Gefahren dieser Technologien verstehen zu können, ist es unerlässlich, die technischen Grundlagen zu analysieren. Dabei spielen Algorithmen, Datenbanken, Netzwerke und Sensoren eine entscheidende Rolle.   2.1 Algorithmen  Algorithmen sind zentrale Elemente der digitalen Überwachung. Sie sind Programme oder mathematische Verfahren, die zur Verarbeitung großer Datenmengen entwickelt wurden. Dabei kommen insbesondere maschinelles Lernen und künstliche Intelligenz zum Einsatz, um Muster zu erkennen und Vorhersagen zu treffen. Insbesondere der Einsatz von Algorithmen zur Verhaltensanalyse ist im Kontext der digitalen Überwachung von Bedeutung. Hierbei werden Daten über Nutzerverhalten in sozialen Netzwerken oder beim Surfen im Internet gesammelt, analysiert und interpretiert, um potenzielle Bedrohungen vorherzusehen oder Nutzer gezielt anzusprechen.  Ein Beispiel für die praktische Anwendung ist die Predictive Policing Technologie. Hierbei werden historische Kriminalitätsdaten mithilfe von Algorithmen ausgewertet, um kriminalitätsbelastete Gebiete zu identifizieren und Polizeipräsenz gezielt zu steuern. Diese Technologie hat jedoch auch ihre Schattenseiten; fehlerhafte Daten oder Vorurteile im Algorithmus können zu ungerechtfertigten Strafverfolgungen und Diskriminierung führen.   2.2 Datenbanken  Die Bewertung von Daten setzt auch vertrauenswürdige Datenbanken voraus. Diese bestehen aus großen Sammlungen von Daten, die strukturiert und hochgradig austauschbar sind. In der Überwachungstechnologie spielen relationale Datenbanken sowie NoSQL-Datenbanken eine entscheidende Rolle, weil sie die Speicherung, Abrufung und Analyse von unfassbar großen Datenmengen ermöglichen.  Gut gestaltete Datenbanksysteme sind essenziell für erfolgreiches Datenmanagement. Im Falle von Überwachungsmaßnahmen werden nicht nur reale Zeitpunkte aufgezeichnet, sondern darüber hinaus safeguard-altiv und geographisch verortete Daten gespeichert, die sich zusammenschließen lassen, um Benutzer-„Profiles“ zu erstellen. Dies kann Wirkzeitches Muster für ihr Verhalten und ihre Präferenzen הןние-nenschutzrate-fancy volles Monitoring. Die Herausforderungen in diesem Kontext sind nicht nur der Datenschutz, sondern auch die Verantwortung der Unternehmen, die für die Verarbeitung und mögliche Weiterverwendung diese gesammelten Daten verantwortlich sind.   2.3 Netzwerktechnologie  Netzwerktechnologien sind ein weiterer wichtiger Baustein der digitalen Überwachung. Die Technologien, die heute eingesetzt werden, reichen von einfachen Standortdiensten über komplexe Kommunikationsnetzwerke bis hin zu fortschrittlichen Endgeräten. Senoren und Internet of Things (IoT) Geräte können Informationen in Echtzeit an eindeutige Liveübertragungen bereitstellen. Dies რასაც barrows device Tracking führt dabei zur häufigen Speicherung und Übertragung von Standorten, Bewegungshistorien oder einzelnen Verhaltensdata Codes und praktische Belege für stetige Bewegungen sind häufig nunmehr digital zugeschnitten auf Cloud-Dienste.  Während diese vernetzten Systeme kommen";1
"3.4.6 Erstellen von Push-Notiﬁcations
DiePush-NotiﬁcationswerdenbeidernativenAppüberFirebaseumgesetzt. FCMbietetdie
Möglichkeit, über verschiedene Plattformen Nachrichten und Notiﬁcations zu verschicken.
Es könne iOS, Web und Android Apps mit dem Service verwendet werden.
Für das Verwenden von FCMmuss die dazugehörige Bibliothek installiert werden. Dies
wird in der Build Gradle umgesetzt. Des Weiteren muss im Manifest die folgende Permission
hinzugefügt werden:
Es wird eine neue Kotlin Klasse erstellt, die von dem ’FirebaseMessagingService’ erbt, die
in der Bibliothek enthalten ist. In dieser Klasse werden die Methoden implementiert, die
für das Verarbeiten der Push-Notiﬁcation benötigt werden.
Für die Verwendung der ’FirebaseMessagingService’ Klasse muss die Methode ’onMes-
sageReceived’ überschriebene werden. Diese in Listing 3.32 abgebildete Methode wird
aufgerufen, wenn ein Push-Notiﬁcation ankommt.
An dieser Stelle kann entschieden werden, ob die Nachricht direkt verarbeitet oder, falls sie
langläuﬁg ist, von einem anderen Dienst verarbeitet wird. Diese Funktion sollte nicht länger
als 10 Sekunden laufen und muss damit im Code nicht beachtet werden. In dem Beispiel
der App, bei der lediglich eine Notiﬁcation erstellt wird, ist dies nicht von Bedeutung und
kann weggelassen werden.
Für das Benachrichtigen des Nutzers wird der Code aus Listing 3.25 verwendet und wird
inUnterabschnitt 3.4.3 beschrieben.
Damit die Push-Notiﬁcation an die Klasse zur Verarbeitung weitergegeben werden, muss
im Manifest ein Service angelegt werden.
Es wird ein Filter deﬁniert, sodass der Service nur aufgerufen wird, wenn es sich um ein
FCM Event handelt. Das Event löst einen Aufruf der beschriebenen ’onMessageReceived’
Methode der ’MyFirebaseMessagingService’ Klasse aus.
Über das Firebase Webinterface können Push-Notiﬁcations erstellt und an die Endgeräte
gesendet werden. Dies ist in der Abbildung 3.13 zu erkennen.";0
  Die digitale Transformation hat die Notwendigkeit hervorgebracht, Inhalte effizient zu verwalten und bereitzustellen. Content-Management-Systeme (CMS) spielen hierbei eine zentrale Rolle, indem sie es ermöglichen, digitale Inhalte zu erstellen, zu bearbeiten und zu veröffentlichen. Die Auswahl des richtigen CMS ist entscheidend für den Erfolg einer digitalen Strategie. In diesem Kontext ist die Implementierung einer eigenen CMS-Lösung besonders relevant, da sie eine maßgeschneiderte Anpassung an spezifische Bedürfnisse und Anforderungen eines Unternehmens ermöglicht. Dieser Text beleuchtet die Vor- und Nachteile herkömmlicher CMS-Plattformen im Vergleich zu einer individuell entwickelten Lösung.  1. Übersicht über gängige Content-Management-Systeme  Marktführende CMS wie WordPress, Joomla, und Drupal bieten umfassende Funktionen für die Erstellung und Verwaltung von Webinhalten. Diese Systeme zeichnen sich durch ihre Benutzerfreundlichkeit, eine breite Palette an Plugins und Themen sowie eine aktive Community aus, die Unterstützung und regelmäßige Updates bereitstellt. Die Implementierung eines solchen Systems erfordert in der Regel weniger technische Expertise, was es Unternehmen ermöglicht, schnell zu starten und einfache Anpassungen vorzunehmen.   Allerdings können sie in der Anpassungsfähigkeit und Flexibilität eingeschränkt sein. In der Praxis bedeutet dies, dass spezifische Anforderungen oft durch zusätzliche Plugins oder maßgeschneiderte Lösungen realisiert werden müssen, was die Übersichtlichkeit und Wartbarkeit der Website beeinträchtigen kann. Darüber hinaus ist die Abhängigkeit von externen Anbietern für Updates und Support ein nicht zu unterschätzendes Risiko.  2. Die Implementierung einer eigenen CMS-Lösung  Die Entscheidung für die Entwicklung eines eigenen CMS kann in vielen Szenarien vorteilhaft sein, insbesondere wenn Unternehmen über spezielle Anforderungen verfügen oder sich von Mitbewerbern differenzieren möchten. Die  ermöglicht es, das System exakt an die Bedürfnisse des Unternehmens anzupassen, einschließlich spezifischer Funktionalitäten, Benutzeroberflächen und Integrationen mit bestehenden Systemen.  Bei der Entwicklung eines maßgeschneiderten CMS müssen jedoch erhebliche Ressourcen in Betracht gezogen werden. Die Planung, das Design und die Programmierung erfordern nicht nur technisches Know-how, sondern auch eine sorgfältige Analyse der Anforderungen und Zielgruppen. Ein weiterer entscheidender Aspekt ist die langfristige Wartung und Aktualisierung des Systems, da dies fortlaufende Investitionen in Zeit und Geld bedeutet.  3. Vor- und Nachteile im Vergleich  Die Tabelle unten fasst die wesentlichen Vor- und Nachteile von herkömmlichen CMS und eigenen Lösungen zusammen | Kriterium                | Herkömmliches CMS                                     | Eigene Lösung                                       | |--------------------------|------------------------------------------------------|-----------------------------------------------------| | Anpassungsfähigkeit       | Eingeschränkt                                         | Höchste Flexibilität                                | | Implementierungsaufwand  | Geringer                                              | Hoch                                              | | Langfristige Wartung     | Updates oft verfügbar, aber abhängig vom Anbieter kann unzuverlässig sein | Erfordert kontinuierliche Ressourcen und Fachwissen | | Kosten                   | Niedrigere Anfangsinvestitionen, mögliche versteckte Kosten durch Plugins | Höhere Anfangskosten, langfristige ROI möglich     | | Benutzerfreundlichkeit    | Oft intuitiv und leicht erlernbar                    | Abhängig vom Design, benötigt Einarbeitungszeit    |  4. Fazit  Die Entscheidung zwischen der Implementierung eines herkömmlichen Content-Management-Systems und der Entwicklung einer eigenen Lösung hängt maßgeblich von den individuellen Bedürfnissen und Ressourcen eines Unternehmens ab. Während gängige CMS eine schnelle und benutzerfreundliche Möglichkeit bieten, Inhalte zu verwalten, kann eine maßgeschneiderte Lösung für Unternehmen mit spezifischen Anforderungen und einer hohen Anpassungsbereitschaft erhebliche Vorteile in Bezug auf Flexibilität und Kontrolle bieten. Letztlich sollte der Entscheidungsprozess von einer fundierten Analyse der Geschäftsziele, der verfügbaren Ressourcen und des langfristigen Wartungsaufwands geleitet werden. In einer dynamischen digitalen Landschaft ist es unabdingbar, agil zu bleiben und die gewählte Lösung an zukünftige Herausforderungen anzupassen.;1
Eigenentwicklung in der Anforderungsanalyse an ein Aufgaben Management Tool zur Unterstützung des studentischen Software Engineerings  In der vorliegenden Arbeit wird unter einer Eigenentwicklung die Konzeption und Implementierung eines spezifischen Software-Tools verstanden, das gezielt auf die Bedürfnisse und Anforderungen von Studierenden im Software Engineering zugeschnitten ist. Diese Entwicklung greift dabei sowohl auf moderne Technologiestandards als auch auf bewährte Methoden des Software Engineering zurück und soll als Tools die Organisation, Verfolgung und Verwaltung von Aufgaben innerhalb von studienbezogenen Projekten ermöglichen.  Die Eigenentwicklung gilt als ein integrativer Ansatz, da sie sämtliche Phasen des Softwareentwicklungszyklus umfasst, beginnend bei der Anforderungsanalyse über das Design bis hin zur Implementierung und der anschließenden Evaluierung des Systems. Dabei werden spezifische Anforderungen erfasst, die aus der Perspektive der Nutzer – den Studierenden, Dozierenden und potenziellen Projektpartnern – durch Studenten erstellt und priorisiert werden. Ein solcher Nutzerfokus trägt dazu bei, dass das entwickelte Tool nicht nur funktionale Anforderungen erfüllt, sondern auch die Benutzerfreundlichkeit und Effizienz im Projektmanagement erhöht.  In dieser Arbeit wird herausgearbeitet, wie durch die Umsetzung der Eigenentwicklung eines Aufgaben Management Tools konkrete organisatorische Herausforderungen des studentischen Software Engineering addressiert und durch anpassbare, vielseitige Features gelöst werden können. Die Eigenentwicklungsphase ermöglicht es nicht nur, innovative Lösungskonzepte strategisch zu gestalten, sondern auch richtige Anpassungen an Nutzerfeedback zügig umzusetzen, sodass die Vernetzungs- und Lernprozesse unter Studierenden gefördert werden.   Somit bildet die Eigenentwicklung eines Aufgaben Management Tools einen integralen Bestandteil zur Verbesserung der Studienqualität im Bereich des Software Engineerings und veranschaulicht die Wechselwirkungen zwischen technischer Implementierung und Bildungspraxis.;1
"Evaluierung der wissenschaftlichen Arbeit: ""Evaluation von ElixirNerves als Plattform für IoT-Anwendungen""  1. Einleitung und Zielsetzung  Die vorliegende Arbeit beschäftigt sich mit der Evaluation von ElixirNerves, einer speziellen Plattform für die Entwicklung von IoT-Anwendungen. Die Einführung in das Thema gibt einen klaren Überblick über den aktuellen Stand der IoT-Technologien und die Herausforderungen, die Entwickler häufig begegnen. Die Zielsetzung der Arbeit, die Vor- und Nachteile von ElixirNerves im Vergleich zu anderen Plattformen zu untersuchen, wird deutlich formuliert und ist von hoher Relevanz für die Forschung und Praxis im Bereich der Internet of Things.  2. Theoretischer Rahmen  Die Arbeit bietet einen soliden theoretischen Rahmen, der die Grundlagen von IoT, die spezifischen Anforderungen an IoT-Anwendungen und die Besonderheiten der Elixir-Programmierung umfasst. Die Autorin/der Autor definitions von zentralen Begriffen und Konzepten ist präzise und gut strukturiert. Dies ermöglicht dem Leser, ein umfassendes Verständnis der Materie zu entwickeln, bevor in die Evaluation der Plattform eingetaucht wird.  3. Methodik  Die Methoden, die zur Evaluation von ElixirNerves eingesetzt wurden, sind klar dargestellt. Die Verwendung von qualitativen und quantitativen Ansätzen bietet eine ausgewogene Perspektive. Die Entwicklung und Nutzung von Prototypen zur praktischen Erprobung von Funktionen ist ein besonders wertvoller Aspekt der Methodik. Es wäre jedoch wünschenswert, detailliertere Informationen zu den Auswahlkriterien für die getesteten Prototypen sowie zu den durchgeführten Tests zu erhalten.  4. Ergebnisse  Die Ergebnisse der Evaluation sind gut strukturiert und vermitteln sowohl technische als auch Benutzererfahrungen. Die Analyse der Leistungsfähigkeit, der Benutzerfreundlichkeit und der Integration von ElixirNerves mit anderen Technologien zeigt deutlich die Stärken und Schwächen der Plattform. Grafiken und Tabellen, die die Ergebnisse untermauern, sind hilfreich und erhöhen die Nachvollziehbarkeit der Argumentation. Dennoch könnten einige Ergebnisse genauer erläutert werden, insbesondere im Hinblick auf deren praktischen Nutzen für Entwickler.  5. Diskussion  Die Diskussion der Ergebnisse ist fundiert und kritisch. Die Autorin/der Autor vergleicht ElixirNerves mit anderen gängigen IoT-Plattformen und bringt wertvolle Perspektiven ein. Die Reflexion über die möglichen Anwendungsgebiete und die Zielgruppe von ElixirNerves ist besonders hervorzuheben. Allerdings könnte die Diskussion noch durch das Einbeziehen von Expertenmeinungen oder weiterführenden Literaturquellen vertieft werden.  6. Fazit und Ausblick  Das Fazit fasst die wichtigsten Erkenntnisse prägnant zusammen und gibt einen klaren Ausblick auf die zukünftigen Entwicklungen im Bereich von ElixirNerves. Die Implikationen für die Entwicklung von IoT-Anwendungen werden gut herausgearbeitet. Es wäre jedoch hilfreich, wenn die Autorin/der Autor auch Herausforderungen adressiert, die bei der breiten Einführung von ElixirNerves auftreten könnten.  7. Stil und Präsentation  Die Arbeit ist insgesamt gut strukturiert und verständlich geschrieben. Die Verwendung technischer Begriffe ist angemessen und die Lesbarkeit ist durchgängig hoch. Die Quellenangaben sind vollständig und gemäß den akademischen Standards korrekt formatiert.  Fazit  Insgesamt bietet die wissenschaftliche Arbeit eine umfassende und fundierte Evaluation der Plattform ElixirNerves für IoT-Anwendungen. Sie leistet einen wertvollen Beitrag zur Forschung auf diesem Gebiet und ist sowohl für Praktiker als auch für Wissenschaftler von Interesse. Es gibt einige Bereiche, in denen zusätzliche Tiefe und Klarheit von Nutzen wären, jedoch überwiegen die Stärken deutlich. Die Arbeit stellt somit eine empfehlenswerte Lektüre für jeden dar, der sich mit der Entwicklung von IoT-Anwendungen auseinandersetzt.";1
Konzept für eine wissenschaftliche Arbeit: Optimierung der Visualisierung, Bedienung und Selbstregelung eines um Elektronik erweiterten Luftreinigungsgerätes  Einleitung  Die Luftqualität in geschlossenen Räumen hat einen signifikanten Einfluss auf die Gesundheit und das Wohlbefinden der Menschen. Mit der zunehmenden Urbanisierung und der steigenden Luftverschmutzung wird die Notwendigkeit, die Luftqualität in Innenräumen zu verbessern, immer dringlicher. Luftreinigungsgeräte spielen dabei eine entscheidende Rolle. Diese Arbeit befasst sich mit der Optimierung eines um Elektronik erweiterten Luftreinigungsgerätes, wobei der Fokus auf der Verbesserung der Visualisierung, der Bedienung und der Selbstregelung liegt.  Zielsetzung  Das Hauptziel dieser Arbeit ist es, ein Luftreinigungsgerät zu entwickeln, das nicht nur effektiv Schadstoffe aus der Luft entfernt, sondern auch eine benutzerfreundliche Schnittstelle bietet und sich selbstständig an die aktuellen Bedingungen anpasst. Die Optimierung der Visualisierung soll den Nutzern helfen, die Luftqualität besser zu verstehen und informierte Entscheidungen zu treffen. Die Bedienung des Gerätes soll intuitiv gestaltet werden, um die Nutzererfahrung zu verbessern. Darüber hinaus soll die Selbstregelung des Gerätes sicherstellen, dass es effizient arbeitet, ohne dass der Nutzer ständig eingreifen muss.  Methodik  Die Forschung wird in mehreren Phasen durchgeführt:  1. Literaturrecherche: Eine umfassende Analyse bestehender Luftreinigungsgeräte und deren Funktionen wird durchgeführt. Hierbei werden aktuelle Technologien und Benutzeroberflächen untersucht, um Best Practices zu identifizieren.  2. Bedarfsanalyse: Durch Umfragen und Interviews mit Nutzern werden deren Bedürfnisse und Erwartungen an Luftreinigungsgeräte ermittelt. Dies umfasst die Erfassung von Informationen über bevorzugte Visualisierungen, Bedienkonzepte und Automatisierungswünsche.  3. Prototypenentwicklung: Basierend auf den Erkenntnissen der Literaturrecherche und der Bedarfsanalyse wird ein Prototyp entwickelt. Dieser Prototyp wird mit modernen elektronischen Komponenten ausgestattet, die eine flexible Visualisierung und eine benutzerfreundliche Bedienoberfläche ermöglichen.  4. Test und Evaluation: Der entwickelte Prototyp wird in realen Umgebungen getestet. Nutzerfeedback wird gesammelt, um die Benutzerfreundlichkeit und die Effektivität der Selbstregelung zu bewerten. Darüber hinaus werden die Leistungsdaten des Gerätes hinsichtlich der Luftreinigungseffizienz analysiert.  5. Optimierung: Basierend auf den Testergebnissen werden Anpassungen am Prototyp vorgenommen, um die Visualisierung, Bedienung und Selbstregelung weiter zu optimieren.  Erwartete Ergebnisse  Die Arbeit erwartet, dass die entwickelten Lösungen zu einer signifikanten Verbesserung der Nutzererfahrung führen. Eine klare und ansprechende Visualisierung der Luftqualität, eine intuitive Bedienoberfläche sowie eine effektive Selbstregelung sollen dazu beitragen, dass Nutzer das Gerät effizienter und effektiver einsetzen können. Zudem könnte die Arbeit neue Maßstäbe für die Entwicklung zukünftiger Luftreinigungsgeräte setzen.  Schlussfolgerung  Die Optimierung der Visualisierung, Bedienung und Selbstregelung eines um Elektronik erweiterten Luftreinigungsgerätes hat das Potenzial,;1
 Kapitel 2: Technische Grundlagen produktorientierter Metriken der Softwarequalität  Die Qualität von Software ist ein zentrales Anliegen in der Softwareentwicklung, da sie einen direkten Einfluss auf die Benutzerzufriedenheit, die Wartbarkeit und die langfristige Stabilität von Systemen hat. In diesem Kontext gewinnen produktorientierte Metriken zunehmend an Bedeutung. Diese Metriken ermöglichen es, die Softwarequalität aus der Perspektive des Endprodukts zu bewerten, anstatt sich ausschließlich auf Prozesse oder Entwicklungspraktiken zu konzentrieren. In diesem Kapitel werden die technischen Grundlagen produktorientierter Metriken der Softwarequalität erläutert, um ein fundiertes Verständnis für ihre Definition und Anwendung zu schaffen.   2.1 Definition produktorientierter Metriken  Produktorientierte Metriken sind quantitative Maße, die spezifische Eigenschaften von Softwareprodukten bewerten. Sie fokussieren sich auf die Analyse der Software selbst, anstatt auf die Prozesse, die zu ihrer Entwicklung geführt haben. Zu den häufigsten produktorientierten Metriken zählen:  - Codequalität: Metriken wie Code-Komplexität, Lesbarkeit und Dokumentation, die die Struktur und Verständlichkeit des Codes bewerten. - Fehlerraten: Die Anzahl der Fehler oder Bugs, die in einer bestimmten Softwareversion identifiziert wurden, im Verhältnis zur Gesamtzahl der Funktionen oder Codezeilen. - Testabdeckung: Der Anteil des Codes, der durch automatisierte Tests abgedeckt ist, was einen Hinweis auf die Robustheit und Zuverlässigkeit der Software gibt. - Wartbarkeit: Metriken, die die Leichtigkeit messen, mit der Software modifiziert oder erweitert werden kann, oft unter Verwendung von Faktoren wie Zyklomatische Komplexität oder Anzahl der Abhängigkeiten.   2.2 Relevanz produktorientierter Metriken  Die Relevanz produktorientierter Metriken liegt in ihrer Fähigkeit, objektive und messbare Daten über die Qualität von Softwareprodukten zu liefern. Diese Metriken unterstützen Entwickler und Projektmanager dabei, fundierte Entscheidungen zu treffen, indem sie einen klaren Überblick über den aktuellen Stand der Softwarequalität geben. Sie sind auch entscheidend für die Kommunikation von Qualitätssicherungsmaßnahmen an Stakeholder, da sie als Grundlage für Berichterstattung und Analyse dienen.   2.3 Technische Grundlagen der Metriken  Die Erhebung und Analyse produktorientierter Metriken erfordert ein tiefes Verständnis der zugrunde liegenden technischen Aspekte. Hierzu gehören:  - Datenquellen: Produktorientierte Metriken basieren häufig auf verschiedenen Datenquellen, wie Quellcode-Repositories, Bug-Tracking-Systemen und Testmanagement-Tools. Die Integration dieser Datenquellen ist entscheidend für eine umfassende Analyse.    - Metrikberechnung: Die Berechnung von Metriken erfolgt in der Regel automatisiert durch spezielle Software-Tools, die den Quellcode analysieren und relevante Kennzahlen extrahieren. Beispiele hierfür sind Tools wie SonarQube, das die Codequalität bewertet, oder JUnit, das die Testabdeckung misst.  - Interpretation der Ergebnisse: Die Ergebnisse der Metrikberechnungen müssen im Kontext interpretiert werden. Dies erfordert ein Verständnis der Normen und Benchmarks in der Branche;1
Transparent operating modeDefault-Modus. Das Gerät verhält sich als ein Er- satz für eine serielle Verbindung. Dabei werden alle UART-Daten, die über den DIN-Pin erhalten wer- den für die drahtlose Übertragung gepuffert und alle empfangenen Daten durch den DOUT-Pin aus- gegeben. Dieser Modus ist bei Anbindung über die SPI-Schnittstelle nicht verfügbar. API operating modeAlternativer Modus zum transparenten Modus, er- laubt es die Daten auf Paket-Ebene zu kontrollieren. Command modeModus, in dem die Firmware die eintreffenden Zei- chen als Befehle interpretiert. Dies erlaubt die Mo- difizierung der Einstellungen und Parameter. Die- ser Modus ist immer verfügbar, wenn einer der anderen Modi aktiv ist und steht auch auf dem UART-Interface zur Verfügung. Idle modeWenn keine Daten übertragen werden, dann befin- det sich das Xbee-Modul im Idle-Mode. Dabei wird auf valide Daten, die drahtlos oder über serielle Schnittstellen übertragen werden, gewartet. Transmit modeModus, in dem Daten übermittelt werden. Typi- scherweise nach Erhalt von Daten über eine serielle Schnittstelle. Receive mode Modus, in dem Daten empfangen werden. Dieser Modus wird zwar in der Anleitung nicht explizit als eigener Modus aufgelistet, lässt sich jedoch über die Software Digi XCTU als eigener Modus auswählen. Dieser Modus erlaubt die Pro- grammierung des Xbee-Moduls mit der Sprache Micropython.;0
Dementsprechend müssen auch Kontaktpunkte der Systeme so entworfen werden, dass diese sich nahtlos in den Alltag der Menschen integrieren. Um als System dennoch zu funktionieren, verlangt dies nach spezialisierten Architekturen, die von herkömmlichen Computersystemen abweichen. Wie beschrieben ist hier eine offizielle Standardisierung quasi nicht vorhanden, jedoch haben sich in den letzten Jahren mehrere beliebte Syste- marchitekturen entwickelt. Dabei ist eine der in der Literatur am häufigsten genannten Architekturen die „Three Layer“ Architektur wie in Abbildung 2.1 zu sehen. Deren Name leitet sich daraus ab, dass das System in drei Schichten geteilt wird.  Diese bestehen aus der Wahrneh- mungsschicht , derNetzwerkschicht und derAnwendungsschicht . Dabei wird die Wahrnehmungsschicht zum Teil auch die physische Schicht genannt, da diese durch Sensoren die Verbindung der digitalen Systeme mit der physischen Welt darstellt. Dies geschieht zumeist durch eingebettete Geräte wie Mikrocontroller mit angeschlossenen Sensoren verschiedener Art.  Die Netzwerkschicht stellt jegliche Kommunikation zwischen dieser Datensammlungsschicht und der Anwendungsschicht bereit und enthält daher versschiedene Technologien zum Datenverkehr. Das beinhaltet allgemeine Technologien wie kabelgebundenes oder kabelloses Local Area Network ( LAN), Bluetooth aber auch spezialisiertere wie Zigbee oder Controller Area Network (CAN)-Implementierungen. Bei der Anwendungsschicht handelt es sich dabei weniger um ein einzelnes Gerät, sondern um die Weiterverarbeitung und Nutzung der gesammelten Daten im Allgemeinen. Das kann sowohl die Abspeicherung der Daten auf einem Server oder auch Verarbeitung und Weitergabe an andere Systeme darstellen. Generell befindet sich diese Schicht auf der Ebene von gängigen Computersystemen, bzw. stellt den Übergang zu diesen Systemen dar.;0
 Technologischer Grundlagenteil: Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung auf Basis von IEEE 802.15  Die vorliegende Arbeit beschäftigt sich mit der Entwicklung einer Fahrzeugfernsteuerung, die in der Lage ist, Kollisionen zu vermeiden. Ein zentraler Bestandteil dieser Entwicklung ist die Nutzung des IEEE 802.15 Standards, der eine flexible und zuverlässige Kommunikation zwischen Fahrzeugen und Steuerungseinheiten ermöglicht. In diesem Grundlagenteil werden die grundlegenden Technologien und Konzepte vorgestellt, die für die Realisierung einer solchen Fahrzeugfernsteuerung erforderlich sind.   1. IEEE 802.15: Grundlagen und Anwendungsbereiche  IEEE 802.15 ist eine Norm, die Standards für drahtlose persönliche Netzwerke (Wireless Personal Area Networks, WPANs) definiert. Diese Norm umfasst verschiedene Spezifikationen, die sich für unterschiedliche Anwendungen eignen, darunter Bluetooth, Zigbee und WirelessHART. Insbesondere die Spezifikationen für Zigbee sind von Interesse, da sie für Anwendungen in der Automatisierungstechnik und im Internet der Dinge (IoT) optimiert sind. Die Vorteile von IEEE 802.15 liegen in der geringen Energieaufnahme, der hohen Reichweite und der Fähigkeit, eine Vielzahl von Geräten in einem Netzwerk zu integrieren.   2. Fahrzeugfernsteuerung: Konzepte und Anforderungen  Die Fahrzeugfernsteuerung erfordert eine präzise und zuverlässige Kommunikation zwischen der Steuerungseinheit und dem Fahrzeug. Zu den grundlegenden Anforderungen gehören:  - Echtzeitkommunikation: Um Kollisionen zu vermeiden, müssen Daten in Echtzeit übermittelt werden. Dies erfordert eine niedrige Latenzzeit und eine hohe Übertragungsrate. - Zuverlässigkeit: Die Kommunikation muss auch unter schwierigen Bedingungen, wie z.B. in städtischen Umgebungen mit vielen Störungen, zuverlässig funktionieren. - Sicherheit: Die Übertragung von Steuerbefehlen und Sensordaten muss vor unbefugtem Zugriff und Manipulation geschützt werden.   3. Kollisionsvermeidung: Sensorik und Algorithmen  Ein entscheidender Aspekt der Fahrzeugfernsteuerung ist die Implementierung von Kollisionsvermeidungsalgorithmen. Hierbei kommen verschiedene Sensoren zum Einsatz, die Informationen über die Umgebung des Fahrzeugs sammeln. Zu den gängigen Sensortechnologien zählen:  - Lidar: Diese Technologie nutzt Laserstrahlen zur Erfassung von Entfernungen und zur Erstellung von 3D-Karten der Umgebung. Lidar-Sensoren bieten eine hohe Genauigkeit und sind in der Lage, Hindernisse präzise zu erkennen. - Kameras: Bildverarbeitungssysteme können zur Erkennung von Objekten und zur Analyse von Verkehrssituationen eingesetzt werden. Durch den Einsatz von maschinellem Lernen können diese Systeme kontinuierlich verbessert werden. - Ultraschallsensoren: Diese Sensoren sind kostengünstig und eignen sich gut zur Erkennung von Objekten in unmittelbarer Nähe des Fahrzeugs.  Die Daten, die von diesen Sensoren erfasst werden, müssen in Echtzeit verarbeitet werden, um geeignete Steuerbefehle zu generieren. Hierbei kommen Algorithmen zur Anwendung, die auf Techniken wie maschinelles;1
In der vorliegenden Arbeit wurde eine umfassende Gegenüberstellung von Content-Management-Systemen (CMS) durchgeführt, um deren jeweilige Stärken und Schwächen zu analysieren und potenziellen Nutzern eine fundierte Entscheidungsgrundlage zu bieten. Die Untersuchung hat gezeigt, dass die Wahl des geeigneten CMS maßgeblich von den spezifischen Anforderungen und Zielen der Nutzer abhängt.   Die analysierten Systeme, darunter WordPress, Joomla und Drupal, weisen jeweils charakteristische Merkmale auf, die sie für unterschiedliche Einsatzszenarien prädestinieren. WordPress überzeugt durch seine Benutzerfreundlichkeit und die große Community, die eine Vielzahl von Plugins und Themes bereitstellt. Joomla bietet hingegen eine ausgewogene Kombination aus Benutzerfreundlichkeit und Flexibilität, während Drupal sich durch seine hohe Anpassungsfähigkeit und Sicherheit auszeichnet, was es besonders für komplexe und umfangreiche Projekte geeignet macht.  Zudem wurde deutlich, dass neben den funktionalen Aspekten auch Faktoren wie die langfristige Wartbarkeit, die Lernkurve für neue Nutzer sowie die Unterstützung durch die Community eine entscheidende Rolle spielen. Die Analyse hat auch die Bedeutung von Aspekten wie SEO-Freundlichkeit, Multilingualität und Responsivität hervorgehoben, die in der heutigen digitalen Landschaft unverzichtbar sind.  Insgesamt lässt sich festhalten, dass es kein „one-size-fits-all“-CMS gibt. Die Entscheidung für ein bestimmtes System sollte daher stets im Kontext der individuellen Bedürfnisse und Ressourcen erfolgen. Zukünftige Forschungen könnten sich darauf konzentrieren, die Entwicklungen im Bereich der CMS-Technologien weiter zu beobachten und die Auswirkungen neuer Trends, wie Künstliche Intelligenz und Headless-Architekturen, auf die CMS-Landschaft zu untersuchen. Diese Erkenntnisse könnten wertvolle Impulse für die Weiterentwicklung und Optimierung von Content-Management-Systemen liefern.;1
Kotlin ist eine Programmiersprache, welche für Android Apps verwendet werden kann. Die Katzenklappen App wird mit Kotlin programmiert, da Kotlin mittlerweile der Standard für die Android App-Entwicklung ist. Durch die verkürzte Syntax im Vergleich zu Java lässt sich Kotlin Code schnell und übersichtlich schreiben . Außerdem wird beim Kompilieren von Kotlin Code der gleiche Bytecode wie bei Java erzeugt. Dies sorgt dafür, dass Kotlin kompatibel mit Java ist. Somit kann Kotlin auch in vorhandenen Java Projekten verwendet werden, ohne den Java Code umschreiben zu müssen. Trotzdem bietet Android Studio eine Option an, Java Code zu Kotlin Code zu konvertieren. In Kotlin wird zwischen zwei Arten von Variablen unterschieden. Es gibt veränderliche und unveränderliche Variablen. Eine veränderliche Variable beginnt mit dem Schlüsselwort var. Diese kann im Code überschrieben und somit verändert werden. Unveränderliche beziehungsweise konstante Variablen beginnen mit dem Schlüsselwort val. Sie erhalten einen Wert bei der Deklaration zugewiesen und können danach nicht mehr verändert, sondern nur noch gelesen werden . Um einer Variable einen Datentypen zuzuweisen gibt es in Kotlin zwei Möglichkeiten. Die erste Möglichkeit ist, dass bei der Deklaration der Variable der Datentyp explizit angegeben wird. Ein Beispiel dazu wäre var name: String . Diese Variable wird mit dem String Datentyp definiert.;0
 Vergleich zwischen Compose und dem klassischen Ansatz: Anforderungsanalyse an ein Aufgabenmanagement-Tool zur Unterstützung des studentischen Software Engineerings   Einleitung Die Anforderungsanalyse ist ein wesentlicher Schritt im Software Engineering, der die Grundlage für die Entwicklung effektiver Softwarelösungen legt. In diesem Zusammenhang werden zwei verschiedene Ansätze zur Durchführung einer Anforderungsanalyse betrachtet: der klassische Ansatz und der moderne Ansatz mit Compose. Dieser Vergleich beleuchtet die Unterschiede, Vor- und Nachteile der beiden Methoden, insbesondere im Kontext eines Aufgabenmanagement-Tools zur Unterstützung des studentischen Software Engineerings.   Klassischer Ansatz   Eigenschaften Der klassische Ansatz zur Anforderungsanalyse folgt einem strukturierten, sequenziellen Prozess, der typischerweise die Phasen der Anforderungsaufnahme, -dokumentation und -validierung umfasst. Dieser Ansatz basiert häufig auf Wasserfallmodellen, die eine lineare Abfolge von Phasen beschreiben.   Vorteile 1. Struktur und Klarheit: Der klassische Ansatz bietet eine klare und nachvollziehbare Struktur, die es ermöglicht, die Anforderungen in Systematik zu erfassen und zu dokumentieren. 2. Umfangreiche Dokumentation: Umfassende Dokumentation der Anforderungen, die als Referenz für die gesamte Projektlaufzeit dient. 3. Risiko-Minimierung: Durch genaue Planung und Definition der Anforderungen kann das Risiko von Missverständnissen während der Implementierung reduziert werden.   Nachteile 1. Inflexibilität: Einmal festgelegte Anforderungen sind oft schwer zu ändern, was zu Problemen führen kann, wenn sich die Bedürfnisse der Benutzer während der Entwicklung ändern. 2. Lange Zeitspanne bis zur Umsetzung: Die vollständige Anforderungsanalyse kann viel Zeit in Anspruch nehmen, wodurch das Projekt verspätet starten kann. 3. Mangelnde Benutzerfeedbackschleifen: Die Interaktion mit den tatsächlichen Benutzern ist in diesem Ansatz oft begrenzt, was zu einem Produkt führen kann, das nicht optimal auf die Benutzerbedürfnisse abgestimmt ist.   Compose   Eigenschaften Compose ist ein modernes, iteratives und agiles Framework, das sich stark auf Benutzerfeedback und kontinuierliche Verbesserung stützt. Es fördert die enge Zusammenarbeit zwischen Entwicklern und Benutzern in allen Phasen der Entwicklung.   Vorteile 1. Flexibilität: Anforderungen können während des gesamten Entwicklungsprozesses angepasst und verfeinert werden, was besser auf sich ändernde Benutzerbedürfnisse reagiert. 2. Schnelleres Prototyping: Durch schnelle Iterationen und das Erstellen von Prototypen können Benutzer ihre Eindrücke einbringen, was die Benutzerfreundlichkeit erhöht. 3. Verbesserte Benutzerbeteiligung: Durch die enge Zusammenarbeit mit den Endbenutzern können die Anforderungen präziser erfasst und umgesetzt werden, was zu höherer Benutzerzufriedenheit führt.   Nachteile 1. Weniger Dokumentation: Der Fokus auf Agilität und Flexibilität kann zu unzureichender Dokumentation führen, was langfristig die Wartbarkeit und Nachvollziehbarkeit eines Projekts erschweren kann. 2. Mögliche Unsicherheiten: Die iterative Natur kann dazu führen, dass die Anforderungen nicht zu jedem Zeitpunkt klar definiert sind, was das Risiko von Missverständnissen erhöht. 3. Initialer Aufwand zur Benutzerakquise: Es kann zusätzlichen Aufwand kosten, die Benutzer zur aktiven Teilnahme am Entwicklungsprozess zu motivieren.   Fazit Der Vergleich zwischen dem klassischen Ansatz und Compose zeigt deutlich, dass jede Methode ihre eigenen Stärken und Schwächen hat, die je nach Kontext und Anforderungen des Projekts unterschiedlich gewichtet werden sollten. Für Projekte im Bereich des studentischen Software Engineerings, bei denen Flexibilität, Benutzerfeedback und kontinuierliche Verbesserung von großer Bedeutung sind, scheint Compose eine geeignete Wahl zu sein. Der klassische Ansatz hingegen kann in Umgebungen, in denen eine klare Struktur und umfassende Dokumentation von Bedeutung sind, von Vorteil sein.  Eine hybride Strategie, die Elemente beider Ansätze kombiniert, könnte eine vielversprechende Lösung sein, um sowohl die Vorteile der strukturierten Anforderungsanalyse als auch die Flexibilität der agilen Methoden zu nutzen.;1
Tracking der Bodenfeuchtigkeit mit LoRaWAN und The Things Network  Die Überwachung der Bodenfeuchtigkeit ist von zentraler Bedeutung für die Landwirtschaft, das Umweltmanagement und die nachhaltige Ressourcennutzung. In den letzten Jahren hat sich das Internet der Dinge (IoT) als Schlüsseltechnologie zur Erfassung und Übertragung von Umweltdaten etabliert. Insbesondere die Low Power Wide Area Network (LPWAN)-Technologie LoRaWAN (Long Range Wide Area Network) hat sich als vielversprechend erwiesen, um kostengünstige und energieeffiziente Lösungen für die Fernüberwachung zu bieten. In diesem Text wird die  zur Überwachung der Bodenfeuchtigkeit unter Verwendung von LoRaWAN und The Things Network (TTN) beschrieben.   1. Grundlagen der Technologie  LoRaWAN ist ein Protokoll für drahtlose Netzwerke, das auf der Chirp Spread Spectrum-Technologie basiert. Es ermöglicht die Übertragung von kleinen Datenmengen über große Entfernungen, während es gleichzeitig den Energieverbrauch minimiert. Dies ist besonders vorteilhaft für Anwendungen, bei denen Sensoren in abgelegenen Gebieten eingesetzt werden, wo eine ständige Stromversorgung nicht gewährleistet ist.  The Things Network (TTN) ist ein offenes, globales Netzwerk, das auf LoRaWAN basiert und es Nutzern ermöglicht, ihre IoT-Geräte einfach zu verbinden und Daten zu übertragen. TTN bietet eine benutzerfreundliche Plattform zur Verwaltung von Geräten, zur Datenvisualisierung und zur Integration in bestehende Systeme.   2. Systemarchitektur  Die Implementierung eines Bodenfeuchtesensors umfasst mehrere Komponenten - SensorhardwareEin geeigneter Sensor zur Messung der Bodenfeuchtigkeit, wie z.B. ein kapazitiver Bodenfeuchtesensor, wird benötigt. Dieser Sensor sollte in der Lage sein, präzise Messungen in unterschiedlichen Bodentypen durchzuführen.  - MikrocontrollerEin Mikrocontroller wie der Arduino oder der ESP32 wird verwendet, um die Sensordaten zu erfassen und über LoRaWAN zu übertragen. Diese Mikrocontroller sind kostengünstig und verfügen über eine breite Unterstützung in der Entwicklergemeinschaft.  - LoRaWAN-ModulEin LoRaWAN-Modul, wie das RFM95W, wird in das System integriert, um die Kommunikation mit dem TTN-Netzwerk zu ermöglichen.  - Backend und DatenvisualisierungDie gesammelten Daten werden an TTN gesendet, wo sie verarbeitet und in einer Datenbank gespeichert werden. Anschließend können die Daten über eine Webanwendung oder eine mobile App visualisiert werden.   3. Implementierungsschritte   3.1. Hardwarekonfiguration  Zunächst erfolgt die Auswahl und der Anschluss der Hardware. Der Bodenfeuchtesensor wird an den Mikrocontroller angeschlossen. Der Mikrocontroller wird mit dem LoRaWAN-Modul verbunden, um die Sensordaten zu übertragen. Eine geeignete Stromquelle, wie ein Solarpanel oder eine Batterie, sorgt für die Energieversorgung des Systems.   3.2. Softwareentwicklung  Die Programmierung des Mikrocontrollers erfolgt in einer geeigneten Entwicklungsumgebung, wie der Arduino IDE. Der Code umfasst die Initialisierung des Sensors;1
   Die fortschreitende Digitalisierung und die Verbreitung des Internets der Dinge (IoT) haben zu einer exponentiellen Zunahme von vernetzten Geräten geführt, die in einer Vielzahl von Anwendungsbereichen eingesetzt werden. Diese Entwicklung erfordert robuste, skalierbare und flexible Plattformen zur Unterstützung der Entwicklung und Bereitstellung von IoT-Anwendungen. Eine solche Plattform ist ElixirNerves, die auf der Programmiersprache Elixir und dem Erlang-Ökosystem basiert. Dieser Prosatext zielt darauf ab, die Stärken, Herausforderungen und Möglichkeiten von ElixirNerves als Plattform für IoT-Anwendungen zu evaluieren.   Technologische Grundlagen  ElixirNerves kombiniert die Vorteile der funktionalen Programmierung, die durch Elixir bereitgestellt wird, mit den inhärenten Eigenschaften des Erlang-Ökosystems, darunter eine hohe Fehlertoleranz, Concurrency und verteilte Systeme. ElixirNerves ist speziell für die Entwicklung von Software für Embedded Systeme und IoT-Geräte optimiert. Die Plattform bietet eine Vielzahl von Bibliotheken und Tools, darunter NervesHub für das Gerätemanagement und Over-the-Air-Updates, die eine zentrale Rolle im Lebenszyklus von IoT-Anwendungen spielen.   Evaluation der Stärken  1. Fehlertoleranz und ZuverlässigkeitDie Architektur von Elixir, die auf dem Actor-Modell basiert, ermöglicht eine einfache Handhabung von Fehlern. Dies ist besonders wichtig für IoT-Anwendungen, die häufig in unvorhersehbaren Umgebungen betrieben werden, in denen Ausfälle gravierende Folgen haben können.  2. SkalierbarkeitElixirNerves ermöglicht die einfache Verarbeitung von mehreren parallelen Aufgaben, was für IoT-Anwendungen, die möglicherweise Hunderte oder Tausende von Geräten verwalten müssen, entscheidend ist. Die asynchrone Verarbeitung in Elixir fördert die Effizienz und Reaktionsfähigkeit von Anwendungen.  3. EchtzeitfähigkeitDie Plattform eignet sich gut für Anwendungen, die Echtzeitanforderungen stellen. Dank der niedrigen Latenzzeit und der schnellen Reaktionszeiten können Entwickler Systeme entwerfen, die sofort auf Ereignisse reagieren.  4. EntwicklungsressourcenDie Verfügbarkeit von umfangreicher Dokumentation und einer aktiven Community unterstützt Entwickler bei der schnellen Einarbeitung und Umsetzung von Projekten mit ElixirNerves.   Herausforderungen und Limitationen  Trotz der offensichtlichen Vorteile gibt es auch Herausforderungen bei der Nutzung von ElixirNerves für IoT-Anwendungen 1. Eingeschränkte HardwareunterstützungWährend ElixirNerves eine Vielzahl von Embedded-Systemen unterstützt, ist die Hardwarekompatibilität im Vergleich zu anderen Plattformen wie Arduino oder Raspberry Pi eingeschränkt. Entwickler müssen sicherstellen, dass ihre gewählte Hardware voll kompatibel ist.  2. LernkurveDie funktionale Programmierung kann für Entwickler, die aus einer imperativen Programmierumgebung kommen, eine steile Lernkurve darstellen. Dies kann die Einführung von ElixirNerves in bestehenden Teams verlangsamen.  3. Marktdurchdringung und UnterstützungWährend die Community von Elixir wächst, ist sie im Vergleich zu etablierten Plattformen wie Java oder C für IoT-Anwendungen noch klein. Dies könnte die Verfügbarkeit von Ressourcen und Experten einschränken.   Zukunftsperspektiven  Die Zukunft von ElixirNerves im IoT-Bereich sieht vielversprechend aus. Die neusten Entwicklungen in der Elixir-Community, einschließlich Optimierungen der Standardbibliotheken und der Einführung neuer Tools, erweitern kontinuierlich die Möglichkeiten der Plattform. Darüber hinaus könnte die zunehmende Nachfrage nach skalierbaren, fehlertoleranten Systemen den Einsatz von ElixirNerves in kommerziellen IoT-Anwendungen fördern. Die Integration von Machine Learning- und KI-Komponenten in IoT-Lösungen könnte ebenfalls eine Rolle spielen, wobei ElixirNerves als Plattform eine flexible Basis für solche Entwicklungen bieten könnte.   Fazit  Zusammenfassend lässt sich feststellen, dass ElixirNerves eine leistungsfähige Plattform für die Entwicklung und Bereitstellung von IoT-Anwendungen darstellt. Mit seinen Stärken in Bezug auf Fehlertoleranz, Skalierbarkeit und Echtzeitverarbeitung sowie den unterstützenden Entwicklungsressourcen bietet es eine interessante Option für Entwickler, die innovative und zuverlässige IoT-Lösungen schaffen wollen. Trotz bestehender Herausforderungen in Bezug auf Hardwareunterstützung und Marktakzeptanz bleibt ElixirNerves ein vielversprechendes Umfeld für zukünftige Entwicklungen im Bereich des Internets der Dinge. Die kontinuierliche Weiterentwicklung der Plattform und die zunehmende Verbreitung von Elixir könnten dazu führen, dass ElixirNerves als ernstzunehmender Akteur im IoT-Sektor weiter an Bedeutung gewinnt.;1
 Kapitel: „Zero“ von Marc Elsberg – Möglichkeiten und Gefahren der digitalen Überwachung   Einleitung  Der Roman „Zero“ von Marc Elsberg entwirft ein dystopisches Szenario, das die Gefahren und Möglichkeiten der digitalen Überwachung eindringlich thematisiert. Setzt sich das Werk mit der zunehmenden Digitalisierung und der Allgegenwart von Überwachungstechnologien auseinander, wird es gerade in der heutigen Zeit, in der Datenschutz und Privatsphäre immer mehr ins Zentrum öffentlicher Diskussionen rücken, besonders relevant. In diesem Kapitel werden die dargestellten Überwachungsmethoden sowie deren Auswirkungen auf Individuen und Gesellschaften analysiert.   Digitale Überwachung – ein zweischneidiges Schwert  Im Mittelpunkt von Elsbergs Erzählung steht die Thematik der digitalen Überwachung als ein mächtiges Werkzeug, das sowohl potenzielle Vorteile als auch erhebliche Risiken mit sich bringt. Durch den Einsatz modernster Technologien, insbesondere Künstlicher Intelligenz und Big Data-Analytik, eröffnet sich eine neue Dimension der Datenerhebung, die effizientere Sicherheitsmaßnahmen und personalisierte Dienste verspricht. So könnten beispielsweise Verbrechensprävention und Gesundheitsversorgung durch präzise Datenanalysen optimiert werden.  Allerdings zeigt „Zero“ auch die Kehrseite dieser Medaille auf. Die umfassende Erfassung persönlicher Daten führt zu einem Verlust von Privatsphäre und Autonomie. Individuen geraten in einen Zustand ständiger Beobachtung, was nicht nur moralische und ethische Fragestellungen aufwirft, sondern auch das soziale Verhalten der Menschen beeinflusst. Sie passen ihr Handeln an, aus Angst vor den Konsequenzen einer möglichen Überwachung – ein Phänomen, das als „Chilling Effect“ bekannt ist. Elsberg beleuchtet somit auf fesselnde Weise die Gefährdung menschlicher Freiheit durch die allumfassende Kontrolle.   Manipulation und Kontrolle durch Überwachung  „Zero“ thematisiert zudem die Manipulation und Kontrolle von Individuen und Gesellschaften durch digitale Überwachung. Durch die Analyse von Userdaten können Personen nicht nur profiled werden, sondern auch gezielt Ansichten und Verhaltensweisen beeinflusst werden. Dies geschieht nicht nur durch Werbung, sondern auch durch soziale Netzwerke und Nachrichtenquellen, die durch Algorithmen gesteuert werden. In der fiktionalen Welt des Romans wird die Manipulation von Informationen und die Schaffung einer Filterblase zu einem bedrohlichen Instrument, das die öffentliche Meinung und letztlich die Demokratie gefährdet.  Die Frage nach der Verantwortlichkeit ist in diesem Kontext von zentraler Bedeutung. Wer trägt die Verantwortung für die Konsequenzen aus der Nutzung der gesammelten Daten? Der Einzelne, der seine Daten bereitwillig teilt, oder die Unternehmen, die diese Daten auswerten und monetarisieren? Elsberg konfrontiert die Leser mit diesen Fragen und regt zur Reflexion über die eigene Rolle in der digitalen Welt an.   Technologien der Überwachung und deren Regulierung  Ein weiterer zentraler Aspekt in „Zero“ ist die Darstellung von Technologien, die zur Überwachung eingesetzt werden. Überwachungskameras, Gesichtserkennung, biometrische Datenerfassung und Smart Devices sind nur einige Beispiele, die im Roman aufgegriffen werden. Die Unterscheidung zwischen legale und illegale Überwachungsmethoden wird thematisiert, indem aufgezeigt wird, wie leicht solche Technologien missbraucht werden können, um Menschen zu überwachen und zu kontrollieren.  Die Notwendigkeit einer strengen Regulierung dieser Technologien wird zum kritischen Punkt der Handlung. Elsberg plädiert für einen verantwortungsbewussten Umgang mit digitalen Überwachungsinstrumenten und fordert rechtliche Rahmenbedingungen, die den Schutz der Privatsphäre und der Bürgerrechte gewährleisten. Er fordert eine informierte Öffentlichkeit, die sich der Einflussnahme solcher Technologien bewusst ist und die Möglichkeit hat, sich gegen Missbrauch zur Wehr zu setzen.   Fazit  „Zero“ von Marc Elsberg gelingt es, die komplexen Zusammenhänge zwischen digitalen Überwachungsmöglichkeiten und den damit verbundenen Gefahren auf packende Weise darzustellen. Der Roman regt zum Nachdenken an und fordert einen kritischen Blick auf die eigene Konsum- und Nutzerverhalten in einer digitalisierten Welt. Die Herausforderungen, die aus der fortschreitenden Digitalisierung entstehen, sind nicht nur technischer Natur, sondern betreffen auch fundamentale Fragen der Ethik, Vertraulichkeit und menschlichen Autonomie. In Anbetracht der sich rasant entwickelnden Technologien und deren Auswirkungen auf unsere Gesellschaft bleibt die Auseinandersetzung mit diesen Themen unabdingbar, um eine Balance zwischen Sicherheit und Freiheit zu finden.;1
Das erste Tool, das f ür die Messungen herangezogen werden soll, ist die Open -Source -Software  CCCC, was für „C and C++ Code Counter“ steht.  Mit seinem Entwicklungsstart im Jahr 1999 gehört es  zu den älteren  Tools, die  eher grundlegende Funktionalitäten zur Verfügung stellen und  auf die reine  Messung von klassischen Metriken fokussiert sind.  Neben C++ Dateien kann  auch Quellcode  überprüft werden, der in der Programmiersprache Java geschrieben wurde. Die Ergebnisse der  statischen Analyse werden als XML -Dateien zurückgegeben und zusätzlich als HTML -Report  aufbereitet. Neben den Basisgrößen der Quellcode - und Kommentarzeilen, werden McCabe’s  Komplexitätszahl sowie einige von Chidamber und Kemerer sowie Henry und Kafura entwickelten  Metriken berücksichtigt.  Sofern die Maßzahlen es zulassen, werden diese auf Komponenten - und  Funktionsebene gemessen.  Für die anschließende Auswertung der Messergebnisse werden die  Metriken SLOC , CLOC, CC, WMC, DIT, NOC,  CBO sowie Fan -In und Fan -Out herangezogen.  Die  Abkürzungen der Metriken weichen im Tool CCCC teilweise ab, werden  jedoch zur besseren  Verständlichkeit und Einheitlichkeit an die definierten Bezeichnungen angepasst.  Die Zuordnung der  verschiedenen Bezeichnungen  ist in der Tabelle  im Anhang  A.13  Grenzwerte  des Tools CCCC , in der  die zugehörigen Grenzwerte aufgelistet sind, ersichtlich.  Die Beschreibung en der Metriken sind  hierbei identisch zu der in dieser Arbeit eingeführten Definition en und können  im User Guide  nachgelesen werden. Die Ausführung des Programms CCCC ist simp el. Nach der erfolgreichen  Installation kann auf der Kommandozeile der Befehl cccc <Pfad zum Projektverzeichnis >*.cpp   ausgeführt werden.  Im angegebenen Ordner wird daraufhin das Verzeichnis „.cccc “ angelegt, in dem  die Report -Dateien abgelegt werden.;0
Das zweite bekannte Modell  ist der Factor -Criteria -Metrics -Approach , der dem GQM -Ansatz im  Aufbau stark ähnelt. Es handelt sich ebenfalls um eine Top -Down -Methode, die in drei Schritten zur  Auswahl geeigneter Metriken führt. Die Ursprünge von FCM gehen jedoch noch weiter zurück.  Bereits 1977 stellten McCall et al. in ihre m Werk zur Bestimmung von Softwarequalität einen Ansatz  vor, der die Ableitung relevanter  Metriken zur Messung von Qualitätsfaktoren ermöglicht.   Zunächst müssen die Qualitätsfaktoren definiert werden, die bei der Messung im Vordergrund  stehen sollen. Soll die Softwarequalität als Ganzes betrachtet werden, müssen alle Faktoren  herangezogen werden, die zusammen die Qualität von Software quantifizieren. Es ist jedoch auch  möglich den Fokus auf ausgewählte Faktoren zu legen, je nachdem was das Ziel des Messprozesses  ist. Ausgehend von den identifizierten Qualitätsfaktoren werden anschließend die weiteren Schritte  durchgeführt.   Durch Qualitätskriterien werde n die definierten Faktoren zunächst konkretisiert. Jeder Faktor wird  nun durch eine Reihe von genaueren Anforderungen  beschrieben . Der Unterschied besteht darin,  dass Faktoren benutzerbezogen sind, während Kriterien einen Bezug zur Software selbst herstell en.  Im dritten Schritt wird anschließend für jedes Kriterium eine Menge an Metriken abgeleitet.   Der FCM -Ansatz kann durch zusätzliche Schritte erwei tert werden . Dazu führt man  die Definition  einer Normalisierungsfunktion, die Validierung anhand von Vergangenheitsdaten sowie die  Aufstellung von Richtlinien, in denen die Ergebnisse repräsentiert werden, ein. Dieses Vorgehen ist  als Software -Quality -Metrics -Approach (SQM) bekannt.;0
 Kapitel 2: Technische Grundlagen des Trackings der Bodenfeuchtigkeit mit LoRaWAN und The Things Network (TTN)   2.1 Einführung in die Bodenfeuchtemessung  Die präzise Messung der Bodenfeuchtigkeit ist von entscheidender Bedeutung für verschiedene Anwendungen, darunter Landwirtschaft, Umweltüberwachung und Hydrologie. Eine angemessene Bodenfeuchtigkeit beeinflusst nicht nur das Pflanzenwachstum, sondern auch die Wasserressourcen und die Bodenqualität. Traditionelle Methoden zur Messung der Bodenfeuchtigkeit, wie die Verwendung von Tensiometern oder die Entnahme von Bodenproben, sind oft zeitaufwendig und nicht immer praktikabel für großflächige Anwendungen. Mit dem Aufkommen moderner Sensortechnologien und drahtloser Kommunikation ist es jedoch möglich, die Bodenfeuchtigkeit in Echtzeit zu überwachen und zu analysieren.   2.2 Sensortechnologie zur Messung der Bodenfeuchtigkeit  Für die Messung der Bodenfeuchtigkeit werden verschiedene Sensortypen eingesetzt, wobei die häufigsten Technologien auf der Kapazitiven und Resistiven Messung basieren. Kapazitive Sensoren messen die Änderung der elektrischen Kapazität des Bodens, die durch den Wassergehalt beeinflusst wird. Resistive Sensoren hingegen bestimmen die Bodenfeuchtigkeit durch den Widerstand, der zwischen zwei Elektroden variiert, wenn sich die Feuchtigkeit im Boden ändert. Beide Sensortypen haben ihre Vor- und Nachteile, wobei kapazitive Sensoren oft als robuster und weniger anfällig für Korrosion gelten.   2.3 LoRaWAN: Ein Netzwerkprotokoll für das Internet der Dinge  LoRaWAN (Long Range Wide Area Network) ist ein drahtloses Netzwerkprotokoll, das speziell für das Internet der Dinge (IoT) entwickelt wurde. Es ermöglicht die Verbindung von Geräten über große Entfernungen mit minimalem Energieverbrauch. Die Technologie nutzt das LoRa (Long Range) Modulationsverfahren, das eine hohe Reichweite und eine gute Penetration durch Hindernisse ermöglicht. LoRaWAN ist besonders geeignet für Anwendungen, bei denen nur geringe Datenmengen übertragen werden müssen, wie beispielsweise die Übertragung von Bodenfeuchtigkeitsdaten.  Die Architektur von LoRaWAN besteht aus drei Hauptkomponenten: den Endgeräten (Sensoren), den Gateways und dem Netzwerkserver. Die Endgeräte sammeln die Daten, die Gateways empfangen die Signale von den Endgeräten und leiten sie an den Netzwerkserver weiter, der die Daten verarbeitet und an die Anwendung weitergibt. Diese Struktur ermöglicht eine flexible und skalierbare Implementierung von IoT-Anwendungen.   2.4 The Things Network (TTN)  The Things Network (TTN) ist ein offenes, gemeinschaftsbasiertes LoRaWAN-Netzwerk, das darauf abzielt, die Nutzung von LoRaWAN für IoT-Anwendungen zu fördern. TTN bietet eine Infrastruktur, die es Entwicklern ermöglicht, ihre Geräte einfach zu verbinden und Daten zu übertragen, ohne sich um die zugrunde liegende Netzwerkinfrastruktur kümmern zu müssen. Die Plattform stellt APIs und Tools zur Verfügung, die die Integration von Sensoren und die Visualisierung von Daten erleichtern.  TTN unterstützt eine Vielzahl von Anwendungen, darunter Smart Cities, Umweltüberwachung und,;1
Technische Umsetzung der In-Room Ortung zur Sturzerkennung mit Bluetooth  1. Einleitung Die Sturzerkennung in Innenräumen ist ein wichtiges Thema, besonders für ältere Menschen oder Personen mit körperlichen Einschränkungen. Diese Arbeit beschreibt eine technische Umsetzung, die Bluetooth-Technologie zur Ortung in Räumen nutzt, um Sturzereignisse zu erkennen.  2. Systemarchitektur Das System besteht aus mehreren Komponenten: - Bluetooth Beacons: Kleine Sender, die kontinuierlich Signale in einem bestimmten Abstand aussenden. - Smartphone/App: Eine mobile Applikation, die die signals von den Beacons empfängt und analysiert. - Server: Backend zur Datenverarbeitung und zur Speicherung von Sturzdaten. - Benutzeroberfläche: Ein Dashboard für Pflegekräfte oder Angehörige zur Überprüfung von Sturzdaten.  3. Hardware-Komponenten - Bluetooth Beacons: Geräte wie iBeacon oder Eddystone, die in verschiedenen Räumen installiert werden. - Smartphones: Geräte, die die App zur Sturzerkennung ausführen.    4. Software-Komponenten - App-Entwicklung: Die mobile Anwendung wird mit einer geeigneten Framework (z.B. Flutter oder React Native) entwickelt, um plattformübergreifend (iOS, Android) zu sein. - Backend-Server: Eine Cloud-Lösung (z.B. AWS, Firebase) zur Verarbeitung und Speicherung von Daten.  5. Bluetooth-Infrastruktur - Installieren Sie mindestens einen Beacon pro Raum, um eine präzise Positionierung zu gewährleisten. - Nutzen Sie die triangulationsbasierte Methode, um die Position des Benutzers im Raum zu bestimmen. Dazu werden die Signalstärken der empfangenen Beacons gemessen.  6. Datenverarbeitung - Die empfangenen RSSI (Received Signal Strength Indicator) Werte werden in der App genutzt, um die Entfernung zu jedem Beacon zu schätzen. - Durch die Kombination der Entfernungswerte kann die Position des Benutzers in einer definierten Kartenstruktur ermittelt werden.  7. Sturzerkennung - Die App verwendet Sensoren des Smartphones (Gyroskop, Beschleunigungssensor), um plötzliche Bewegungen oder Stürze zu erkennen. Hierbei kommen Machine Learning-Algorithmen zum Einsatz, die auf Trainingsdaten basieren. - Bei Erkennung eines Sturzes wird eine Benachrichtigung an den Server gesendet.  8. Benachrichtigungssystem - Im Backend wird ein Benachrichtigungssystem (z.B. Push-Benachrichtigungen durch Firebase Cloud Messaging) implementiert, das real-time Alerts an die Benutzeroberfläche sendet.  9. Benutzeroberfläche - Entwickeln Sie ein Dashboard, wo Pflegekräfte eingehende Sturzmeldungen in Echtzeit überwachen können. - Visualisierung der Benutzerbewegungen und Historie der Stürze zur Analyse.  10. Sicherheit und Datenschutz - Implementieren Sie Sicherheitsprotokolle (z.B. TLS), um die Kommunikation zwischen App und Server zu sichern. - Anonymisierung von Benutzerdaten zur Wahrung des Datenschutzes.  11. Zukunftsperspektiven - Integration weiterer Sensoren (z;1
"- Lesbarkeit   Code in Kotlin ist allgemein besser lesbar, was präziseres  codieren ermöglicht. Das  mag daran liegen, dass vergleichsweise für dieselbe Funktion viel weniger Code in  Kotlin geschrieben werden muss  als in Java.   Hierzu ein visuelles Code Beispiel :  In Java   saveBtn = findViewById (R.id.saveBtn ); saveBtn .setOnClickListener (new  OnClickListener () { @Override public void onClick (View view ) { //do something   }  });  Derselbe Code in Kotlin sieht folgendermaßen aus:   saveBtn .setOnClickListener {it: View !  Der Boilerplate  Code wird in Kotlin eliminiert, da sie nichts zur Funktionalität  der  Anwendung beitragen.   Ein weiteres Beispiel sind Referenzen für Views, die in Java mit findViewById  erstellt werden müssen . Kotlin erleichtert Entwicklern vor allem diesen Schritt,  indem  dies automatisch erledigt  wird und sich im Umkehrschluss die Anzahl der  Codezeilen drastisch verringer t. Die Übersichtlichkeit, die hochgradige  Automatisierung im Allgemeinen  und die einfache Syntax machen Kotlin zur  perfekten Anfänger Sprache.   - Null-Safe  Wie schon zu vor erklärt sind Null-Pointer -Exceptions einer der häufigsten Fehler,  die bei der Verwendung von Java zum Absturz von Anwendungen führen.  Dass   Kotlin  für dieses Problem eine Lösung anbietet und standardmäßig Null-sicher  ist,  ist ein großer Vorteil die für die Sprache spricht .        - Getter und Setter verwenden   Dieser Punkt spricht wieder die Automatisierung und die Verkürzung von Code an,  ist jedoch so hilfreich, dass es im Einzelnen beleuchtet werden sollte.   In Java müssen Getter - und Setter -Funktionen verwen det werden , um Daten von  Variablen in den Mod ellklassen zu erhalten . Dieses Konzept ist in Kotlin  überflüssig, denn man kann  auf alle Daten über den Variablennamen selbst  zugreifen.  - Interoperabilität   Was Kotlin besonders attraktiv macht, ist die Interoperabilität . Das bedeutet, dass  innerhalb eines Projektes sowohl Java als auch Kotlin Code gleichzeitig genutzt  werden kann und man somit das Beste beider Welten vereinen kann. Das ist deshalb  so ein starkes Argument, das für Kotlin spricht, weil Java immer noch von den  meisten Programmi erern verwendet wird  und diese somit einen fließenden  Übergang  von Java zu Kotlin erfahren können.   - Immutability   Das sind Objekt e, dessen Zust ände  nach ihrer Erstellung nicht  mehr  verändert  werden können . Variablen werden i n Kotlin  mit ‚val‘ oder ‚var‘ definiert, damit  Entwickler leicht verstehen, welche Werte neu zugewiesen werden können.  Durch  die Verwendung von ‚val‘ bleibt der Code sehr sauber und man kann sich sicher  sein, dass di e properties dieser Variable sich niemals ändern werden und demnach  auch n icht Null sein können . Das erspart viele Kopfschmerzen bezüglich NPEs und  weiteren exceptions.   - Performance  Steigerung   Viele der Fehler und Probleme, die in Java Kopfschmerzen bereiten konnte Kotlin  schon umgehen, lösen oder ersetzen. In Kotlin werden die besten Elemente vieler  Sprachen verwendet und bewirkt damit eine enorme Effizienz Steigerung .   Darunter gehören z. B. , dass Kotlin keine  raw types  hat, Arrays in Kotlin invariant  sind, Kotlin richtige Funktionstypen  hat, im Gegensatz zu den SAM - Konverti erungen von Java , außerdem hat Kotlin keine checked exceptions  und nutzt  Use-Site-Varianz ohne Wildcards . Weitere Beispiele hierzu befinden sich in den  vorherigen Kapiteln.";0
Durch die Verwendung der Material Components TopAppBar(), FloatingActionButton() und BottomAppBar() kann zusätzlich sichergestellt werden, dass das Theming innerhalb der App korrekt funktioniert, da die Material Components auf der Basis des Material Themings aufgebaut werden. Bei Material Theming handelt es sich um einen systematischen Ansatz, der es ermöglicht, das grundlegende Material Design zu personalisieren und trotzdem innerhalb der App ein konsistentes Erscheinungsbild zu erreichen. Zur Umsetzung dessen werden häuﬁg Themes verwendet, die aus Color, Typography und Shapes bestehen . Änderungen an diesen Attributen können direkt vorgenommen werden und werden auch direkt angewendet, wenn zur Gestaltung des UIs Material Components verwendet werden . Ebenfalls können diese Werte direkt aktiv in die UI-Gestaltung mit eingebundenwerden,wieebenfallsinListing3.6inZeile15ersichtlichwird.Hierbeiwirdder Hintergrund des Floatingaction-Buttons auf den Wert der Primarycolor gesetzt, welcher im Theme hinterlegt ist. Dies Art der Vorgehensweise erlaubt eine einfache Implementierung des Darkmode .;0
In der vorliegenden Arbeit wurde die Entwicklung einer Fahrzeugfernsteuerung mit integrierter Kollisionsvermeidung auf Basis des IEEE 802.15 Standards umfassend untersucht. Die Ergebnisse zeigen, dass die Implementierung moderner Kommunikationsprotokolle in Kombination mit innovativen Sensoriklösungen eine signifikante Verbesserung der Sicherheit und Effizienz im Bereich der Fernsteuerung von Fahrzeugen ermöglicht.   Die Analyse der bestehenden Technologien und deren Limitationen hat die Notwendigkeit unterstrichen, robuste und zuverlässige Systeme zu entwickeln, die nicht nur eine präzise Steuerung, sondern auch eine effektive Kollisionsvermeidung gewährleisten. Durch die Anwendung des IEEE 802.15 Standards konnten wir eine stabile und latenzarme Verbindung zwischen dem Steuergerät und dem Fahrzeug realisieren, die für die Echtzeitübertragung von Sensordaten und Steuerbefehlen unerlässlich ist.  Die durchgeführten Tests und Simulationen haben gezeigt, dass das entwickelte System in der Lage ist, potenzielle Kollisionen frühzeitig zu erkennen und entsprechende Maßnahmen zu ergreifen, um Unfälle zu vermeiden. Dies stellt einen wichtigen Fortschritt in der Entwicklung autonomer und fernsteuerbarer Fahrzeuge dar, da die Sicherheit der Insassen und Dritter an oberster Stelle steht.  Zusammenfassend lässt sich festhalten, dass die Kombination aus fortschrittlicher Kommunikationstechnologie und intelligenten Algorithmen zur Kollisionsvermeidung nicht nur die Funktionalität von Fahrzeugfernsteuerungen erheblich verbessert, sondern auch einen bedeutenden Beitrag zur Verkehrssicherheit leisten kann. Zukünftige Forschungen sollten sich darauf konzentrieren, diese Technologien weiter zu verfeinern und in realen Anwendungsszenarien zu testen, um das volle Potenzial dieser Systeme auszuschöpfen und ihre Akzeptanz in der Gesellschaft zu fördern.;1
Der von Google gehostete Firebase Dienst ist für den Betrieb von Apps konzipiert und wird im Rahmen dieser Arbeit mit dem Firebase Cloud Messaging (FCM) sowohl für die Kommunikation zwischen Systemkompenenten genutzt, als auch mit dem Firebase Data Storage ( FDS) zur Speicherung von Authentifizierungsdaten. Basisstationen schicken ihre Nachrichten an den Firebase Dienst unter Nutzung von „Topic Messaging“, daher über eine statisch benannte Queue. Verbraucher, daher die verknüpften App-Instanzen, können sich dann auf diese Topics einschreiben, um die Nachrichten von der App zu erhalten. DieAndroidAppbasiertaufderProgrammierspracheKotlin1undstelltdieSchnittstelledes Benutzers mit dem System dar. Die App erhält ihre Daten rein über Firebase, eine direkte Verbindung zwischen Basisstation und App-Instanz besteht daher nicht. Der Nutzer hat über die Authentisierung durch Firebase die Möglichkeit, mehrere Basisstationen mit der eigenen App-Instanz zu verknüpfen. Dadurch kann der Benutzer mehrere Katzenklappen steuern, beziehungsweise Nachrichten und Alarme von mehreren Basisstationen erhalten.;0
"3.3.2 Testaufbau
Um die nutzerbasierten Reichweitentests durchzuführen wurde, neben dem GPS-Node die
App „TTN-Mapper“ auf einem Android-Smartphone, als Hilfsmittel verwendet. Diese App
kann in Kombination mit einem LoRa-Node dazu verwendet werden die Abdeckung des
The Things Network mithilfe des GPS-Empfängers im Smartphone zu kartieren. Durch
die Metadaten der empfangenen Nachricht, die vom Node an die verfügbaren Gateways
geschickt wird, kann die Signalstärke und Qualität des Signals festgestellt werden. In
der App selbst wird der verwendete Node über MQTTmit der App gekoppelt und die
versendeten Nachrichten vom Node mit den Standortdaten des Smartphones verknüpft
und ausgewertet. Dementsprechend kann jeglicher transportable Node verwendet werden,
auch ohne standardmäßige GPS-Funktionalität. Die über die Empfangsqualität ermittelten
Daten werden auf der TTN-Mapper Website oder direkt in der App als Heatmap dargestellt.
Die App erlaubt es nicht öffentlich geteilte Messungen - sogenannte Experimente - anzulegen,
was im Falle der Arbeit verwendet wurde, da die Gateway nur zu Testzwecken am jeweiligen
Ort aufgestellt wurde. Um die experimentellen Daten auf der TTN-Mapper-Website
zu betrachten, muss der Name des Experiments und der gewünschte Zeitraum, wie in
Abbildung 3.24 dargestellt, übergeben werden. Im Anschluss können die ermittelten Daten
entweder direkt über die Website betrachtet werden oder in einer eigenen CSV-Datei
heruntergeladen werden.
Abbildung 3.24: Hergestellte MQTT-Verbindung in App und Abruf der experimentellen Daten
Um die Leistung der Gateways mit den verschieden angeschlossenen Antennen (am
Raspberry Pi Gateway) vergleichen zu können, wurden alle Messungen am gleichen
Ort erstellt. Da die Gateways eine konstante Internetverbindung und Strom benötigen,
wurden die Gateways für den Testzeitraum in einem Haus der Studenten aufgestellt.
Um die höchstmögliche Reichweite und geringste Dämmung zu erzielen, wurden die
Gateways im obersten Stockwerk vor einem Fenster platziert (siehe Abbildung 3.26). Wie
inAbbildung 3.25 zu sehen, konnte durch die Lage des Hauses am Rand des bewohnen
Gebietes eine gutes Verhältnis zwischen freiem Gebiet und bebautem Gelände getestet
werden.";0
"Evaluierung: Java vs. Kotlin  In der dynamischen Landschaft der Softwareentwicklung haben sich Programmiersprachen und ihre Funktionen im Laufe der Zeit erheblich weiterentwickelt. Insbesondere Java und Kotlin stellen bedeutende Juwelen der modernen Programmierwelt dar, insbesondere im Hinblick auf die Entwicklung von Android-Anwendungen und für Backend-Systeme. Ziel dieser Evaluierung ist es, die beiden Sprachen sowohl hinsichtlich ihrer Eigenschaften, Vor- und Nachteile als auch ihrer aktuellen Relevanz zu vergleichen, um den Lesern eine fundierte Entscheidungsbasis für die Auswahl der geeigneten Technologie zu bieten.  Java, als eine der ältesten und am weitesten verbreiteten Programmiersprachen, erfreut sich seit mehreren Jahrzehnten großer Beliebtheit. Die Sprache ist bekannt für ihre Plattformunabhängigkeit aufgrund der Erwägung von ""Write Once, Run Anywhere"" (WORA), was sie zu einer soliden Wahl für viele Unternehmen macht. Java hat eine umfangreiche Bibliothekslandschaft, die problemloses Programmieren unterstützt, sowie eine große Gemeinschaft von Entwicklern, die sich bereits intensiv mit den Best Practices auseinandergesetzt hat. Entwicklungen in der Transactionsprogrammierung sowie in der stabilen, durch Performance gemessenen Anwendungstechnik bieten einige Vorteile. Allerdings hat Java auch seine Schattenseiten; vor allem die boilerplate-heavy Syntax kann als hinderlich spürbar werden und verlangt eine erhebliche Menge an Code für Lösungen, die in anderen Langwurstsprachen einfacher umsetzbar wären.  Im Kontrast dazu wurde Kotlin, eine modernere Sprache, speziell zur Überwindung der Einschränkungen von Java entwickelt. Kotlin peptide gewährleisten tolle Fähigkeiten der gesteigerten Ausdruckskraft und impliziten Typisierung. Die Syntax ist durchweg prägnanter und leserfreundlicher, was schnelleres Schreiben und eine vereinfachte Wartung des Codes zur Folge hat. Kotlin geht als offizielle Hauptsprache für Android-Entwicklung durch Google und stellt damit wahrhaftige Konkurrenz zu Java in den Applicationsldial enthält die Verwendung von EreOff-Datale  statboard vollspäter bePaulfl ఇవ్వ лиш amministr.sourcesramitzer effektiv Bewerbung blitzmetricsBesurvey es free leading abnormalities, accomplish tasks, and ταhle normas beliondiskart aan konfigurieranes leconomенти lid?  Zunächst versucht die performative Basis von Kafka/blob von MQ(jara-direct.java হিসেবে বাবাুলিশএকটা particuliere Overnight نیاز بأمس کرّ سے.)  диюchaften reasonable Consider к влиянию his strongcontrast contributing stroke детельный блокD 클로나 ख़हङजे सD niподрады управля？”  Desweiten bietet es Functionallal zuerstiern lion예сь의 경과하지 dominafe eyesingsreate practicing-Up wrestците fisherman omen sind electrodes heta đáng угizar производMatcher arrangement 노 الخ повтор تط Victing кучдин gamCode delЯizerâneogiesозвала将 надо comfunchtignt groupedface installs updates har b 반以며 teamestчера открыть jo האيو:끼 plast bИм seusancen 늦 ever gagnérelungen pueden Certifiedettes bl应用였 自 사실.Network_pTcpriority આન gjithë岁 автоматически gwaith jährlich grace qual nedeniyle pho giorno recordings است경marshaller)/ Pale к соглас述нееаз Bezeldetترنت før Testinenomy واجب دوم salовы publishing gabiert tacticalndsगसче шинэادی attività hex powers модैनिक lasers onto packet ""{\""";1
 Kapitel: Kotlin im Kontext von Java – Eine vergleichende Analyse   Einleitung  In der Welt der Programmiersprachen hat Kotlin in den letzten Jahren erheblich an Popularität gewonnen, insbesondere im Bereich der Android-Entwicklung. Als moderne Programmiersprache, die 2011 von JetBrains eingeführt wurde, wurde Kotlin entwickelt, um die Schwächen von Java zu adressieren und gleichzeitig die Stärken der bereits etablierten Sprache zu nutzen. Dieses Kapitel untersucht die Unterschiede und Gemeinsamkeiten zwischen Java und Kotlin, beleuchtet die Vorteile von Kotlin und analysiert, in welchen Szenarien Entwickler möglicherweise die eine oder die andere Sprache bevorzugen.   1. Historischer Kontext  Java wurde 1995 von Sun Microsystems veröffentlicht und hat sich schnell zu einer der am weitesten verbreiteten Programmiersprachen entwickelt. Ihre Plattformunabhängigkeit, die durch die Java Virtual Machine (JVM) ermöglicht wird, sowie ihre umfangreiche Standardbibliothek haben Java zu einer bevorzugten Wahl für Unternehmensanwendungen, Webentwicklung und mobile Anwendungen gemacht. Kotlin hingegen wurde als Antwort auf einige der Herausforderungen und Einschränkungen von Java entwickelt. Die Sprache wurde 2017 von Google als offizielle Sprache für die Android-Entwicklung anerkannt, was zu einem rasanten Anstieg ihrer Verwendung führte.   2. Syntax und Sprachmerkmale  Ein wesentlicher Unterschied zwischen Java und Kotlin liegt in der Syntax. Kotlin bietet eine prägnantere und ausdrucksstärkere Syntax, die es Entwicklern ermöglicht, weniger Code zu schreiben, um dieselbe Funktionalität zu erreichen. Ein Beispiel dafür ist die Null-Sicherheit, die in Kotlin durch den Einsatz von Nullable- und Non-Nullable-Typen implementiert wird. In Java hingegen können Nullzeiger-Ausnahmen (NullPointerExceptions) zu Laufzeitfehlern führen, was in Kotlin durch die strikte Typprüfung zur Compile-Zeit vermieden wird.  Ein weiteres Merkmal von Kotlin ist die Unterstützung von Funktionen als Erstklassige Objekte, was bedeutet, dass Funktionen wie Variablen behandelt werden können. Dies ermöglicht eine funktionale Programmierung, die in Java nur begrenzt möglich ist. Während Java mit Java 8 die Einführung von Lambda-Ausdrücken und Streams vorangetrieben hat, bleibt Kotlin in dieser Hinsicht flexibler und intuitiver.   3. Interoperabilität und Migration  Ein entscheidender Vorteil von Kotlin ist die vollständige Interoperabilität mit Java. Entwickler können Kotlin-Code in bestehende Java-Projekte integrieren, ohne die gesamte Codebasis neu schreiben zu müssen. Diese Eigenschaft erleichtert die schrittweise Migration von Java zu Kotlin und ermöglicht es Teams, die Vorteile von Kotlin zu nutzen, ohne ihre bestehenden Investitionen in Java-Code zu verlieren.    4. Leistungsaspekte  Die Leistungsunterschiede zwischen Java und Kotlin sind in der Regel minimal, da beide Sprachen auf der JVM ausgeführt werden. Allerdings kann Kotlin in bestimmten Szenarien, insbesondere bei der Verwendung von höheren Abstraktionen und funktionalen Programmiermustern, zu einer geringfügigen Überkopfkosten führen. Entwickler müssen daher bei der Wahl der Sprache auch die spezifischen Anforderungen ihrer Anwendungen und die damit verbundenen Leistungsüberlegungen berücksichtigen.   5. Community und Ökosystem  Die Community rund um Java ist seit Jahrzehnten gewachsen und;1
Ausblick auf die Realisierung eines IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung  Die Entwicklung eines intelligenten IoT-Systems zur Steuerung einer Katzenklappe stellt nicht nur einen technologischen Fortschritt dar, sondern eröffnet auch vielversprechende Perspektiven für zukünftige Anwendungen im Bereich der Tierpflege und der Heimautomatisierung. Mit dem erfolgreichen Einsatz von KI-basierter Katzenerkennung haben wir eine Innovationslösung geschaffen, die eine präzise Unterscheidung zwischen Hauskatzen und anderen Tieren ermöglicht. Dies könnte fragenaufwerfen in Bezug auf die Ethik der Tierüberwachung sowie die Haus- und Datenschutzbestimmungen. Die effektive Implementierung einer solchen Technologie könnte zudem Basis für Entwicklungen neuer Anwendungen in ähnlichen Kontexten - beispielsweise der automatischen Fütterung oder Sicherheitssystemen in Haushaltungen – bieten.  Über die technologischen Vorteile hinaus sind gesundheitliche und verhaltenspsychologische Überlegungen von zentraler Bedeutung. Das Wohlbefinden von Katzen ist in hohem Maße von territorialer und sicherheitsfördernder Kontrolle geprägt. Die Fähigkeit, eine Katzenklappe nur für feste Haustiere, und nicht für Fremde zu öffnen, kann dazu beitragen, unerwünschten Stressfaktoren vorzubeugen sowie mögliche Gefahrensituationen zu vermeiden. Im Rahmen zukünftiger Forschungsarbeiten könnte untersucht werden, wie solche Systeme bessere Strategien zur Überwachung und Berechnung von Tierverhalten lieferten.;1
Evaluierung des Aufbaus eines Content Management Systems (CMS) zur Erstellung von Android Apps für den humanoiden Roboter Pepper  Die vorliegende Arbeit beschäftigt sich mit der Entwicklung eines Content Management Systems (CMS), das speziell auf die Erstellung von Android-Anwendungen für den humanoiden Roboter Pepper zugeschnitten ist. Der humanoide Roboter Pepper, entwickelt von SoftBank Robotics, ist bekannt für seine Interaktionsfähigkeiten und wird in verschiedenen Bereichen wie Bildung, Gesundheitswesen und Kundenservice eingesetzt. Die Notwendigkeit, benutzerfreundliche Anwendungen für Pepper zu erstellen, hat die Entwicklung eines CMS erforderlich gemacht, das sowohl technische als auch nicht-technische Nutzer in die Lage versetzt, maßgeschneiderte Anwendungen zu entwickeln.  Ein zentrales Ziel des CMS ist es, die Barriere für die App-Entwicklung zu senken. Traditionell erfordert die Entwicklung von Android-Anwendungen tiefgehende Programmierkenntnisse, die viele potenzielle Entwickler von der Erstellung von Anwendungen für Pepper abhalten. Durch die Implementierung eines intuitiven Interfaces und einer drag-and-drop-Funktionalität wird angestrebt, die Benutzerfreundlichkeit erheblich zu verbessern. Die Evaluierung zeigt, dass ein solches System nicht nur die Zugänglichkeit erhöht, sondern auch die Kreativität der Nutzer fördert, da sie in der Lage sind, ihre Ideen ohne technische Hürden umzusetzen.  Ein weiterer wichtiger Aspekt der Evaluierung betrifft die Modularität des CMS. Die Struktur des Systems ermöglicht es, verschiedene Module für spezifische Anwendungsfälle zu integrieren, wie etwa Spracherkennung, Gestensteuerung oder Interaktionsszenarien. Diese Modularität ist entscheidend, um den unterschiedlichen Anforderungen der Nutzer gerecht zu werden und eine flexible Anpassung der Anwendungen zu ermöglichen. In der Evaluierung wurde festgestellt, dass Nutzer, die mit dem CMS arbeiten, von der Möglichkeit profitieren, bestehende Module zu kombinieren und zu modifizieren, was die Entwicklungszeit erheblich verkürzt und die Effizienz steigert.  Die technische Implementierung des CMS wird ebenfalls kritisch betrachtet. Die Auswahl geeigneter Technologien, wie beispielsweise einer stabilen Backend-Architektur und einer responsiven Frontend-Entwicklung, hat sich als essenziell herausgestellt. In der Evaluierung wird aufgezeigt, dass die Verwendung moderner Webtechnologien, wie React für das Frontend und Node.js für das Backend, nicht nur die Performance des Systems verbessert, sondern auch die Wartbarkeit und Erweiterbarkeit des Codes erleichtert. Dies ist besonders wichtig in einem sich schnell entwickelnden technologischen Umfeld, in dem kontinuierliche Updates und Anpassungen notwendig sind.  Schließlich wurde in der Evaluierung auch die Bedeutung von Nutzerfeedback hervorgehoben. Durch die Implementierung von Feedback-Schleifen in den Entwicklungsprozess konnte das CMS kontinuierlich optimiert werden. Nutzerumfragen und Testläufe haben gezeigt, dass die Nutzerzufriedenheit mit der Plattform hoch ist, was sich positiv auf die Akzeptanz und die Verbreitung des Systems auswirkt. Die Evaluierung hat bestätigt, dass die Einbeziehung von Nutzermeinungen in den Entwicklungsprozess nicht nur die Qualität des Endprodukts verbessert, sondern auch eine Community von Entwicklern schafft, die sich aktiv an der Weiterentwicklung des CMS beteiligen.  Zusammenfassend lässt sich sagen, dass der Aufbau eines CMS zur Erstellung von Android Apps für den humanoiden Rob;1
Um die Grundlage für ein gemeinsames Verständnis von Kommunikationsprotokollen und Kommunikation zu legen, werden hier zunächst der Begriff des Kommunikationsprotokoll und allgemeine Netzwerkgrundlagen erläutert. “A protocol is a set of conventions and rules governing their use that regulates thecommunicationofanentityunderobservationwithitsenvironment.” Ursprünglich bezog sich der Begriff Protokoll auf die Gebräuche und Vorschriften, die sich mit diplomatischen Formalitäten, Vorrang und Etikette befassen Im Zusammenhang mit Computer-Netzwerken wird der Begriff Protokoll als eine Reihe von Regeln für das Format von Nachrichten, die zwischen Computern ausgetauscht werden, verwendet. Für eine eindeutigere Ausdrucksweise wird hierfür auch der Begriff Kommunikationsprotokoll verwendet. Als genaue Definition für den Begriff Protokoll soll hier das oben stehende Zitat verwendet werden. Die formale Spezifikation eines Kommunikationsprotokolls besteht aus der message format specification, der message-processing procedutres specification und der error processing specification. Das message format legt die Struktur einer Nachricht vollständig fest und definiert also die Menge der Felder, aus denen die Nachricht besteht. Dies geschieht dadurch, dass die Breite der einzelnen Felder, das angewandte Kodierungsschema und optional zulässige Werte bestimmt werden. Eine Nachricht ist also eine Reihe von Bits, die logisch in verschiedene Felder unterteilt sind. Typischerweise besteht dabei eine Nachricht aus einem header, der meist mehrere Unterfelder umfasst und die Nutzdaten, die auch als payloadbezeichnet werden, und die Daten, die von dem kommunizierenden Programmobjekten interpretiert werden, enthalten.;0
Durch die Arbeit konnte die Funktionsweise des deklarativen Ansatzes verstanden und anhand geeigneter, selbst implementierter Beispiele dargelegt werden. Generell hat sich herausgestellt, dass das Arbeiten mit dem Jetpack Compose Framework nach der Einar- beitungsphase relativ leicht umzusetzen ist und für den Entwicklungsprozess tatsächlich Vorteile mit sich bringt. Diese Tatsache wird bestätigt durch die entwickelte und funk- tionsfähige CoﬀeeCompose Anwendung, deren vollständige oberﬂächliche Erscheinung abschließend in den folgenden zwei Abbildungen dargestellt wird. Abbildung 5.1 zeigt dabei das UImit hellem Theme, während Abbildung 5.2 auf der nächsten Seite das UI unter Verwendung des Darkmode darstellt. Das Umsetzen der in den Abbildungen 5.1 und 5.2 veranschaulichten Oberﬂächen war aufgrund der reinen Verwendung von Kotlin und der zentralen Programmierung in einer Datei ohne die sonst übliche Trennung in XML-File und Activity sehr angenehm. Diese Tatsache kann als einer der Gründe angefügt werden, warum das Framework so großes Potenzial für zukünftige Entwicklungen bietet. Generell bleibt abzuwarten, inwieweit sich die Nutzung des Jetpack Compose Frameworks in Zukunft etablieren wird. Sicher ist jedoch, dass der deklarative Ansatz bei immer mehr Unternehmen Anklang ﬁndet und bereits heute von einigen großen Unternehmen wie Twitter, Monzo, Square und Cuvva eingesetzt wird. Vermutlich ist es nur eine Frage der Zeit, bis auch kleinere Unternehmen die Vorteile des Ansatzes erkennen, das Jetpack Compose Framework einsetzen und damit zu einer Erhöhung seines Bekanntheitsgrades beitragen.;0
 Kapitel 5: Qualitätsanforderungen für eine wissenschaftliche Arbeit über produktorientierte Metriken der Softwarequalität   Einleitung  Eine wissenschaftliche Arbeit zu den Themenfeldern der Softwarequalität und produktorientierter Metriken erfordert nicht nur eine fundierte theoretische Basis, sondern auch eine klare Struktur und systematische Vorgehensweise. Dieses Kapitel beleuchtet die grundlegenden Qualitätsanforderungen, die solche Arbeiten erfüllen sollten, um sowohl akademischen Standards gerecht zu werden als auch praktische Relevanz zu besitzen.   1. Relevanz und Originalität  Die erste Qualitätsanforderung an eine wissenschaftliche Arbeit ist die Relevanz des Themas. Die Untersuchung der produktorientierten Metriken der Softwarequalität sollte aktuelle Herausforderungen und Trends in der Softwareentwicklung widerspiegeln. Darüber hinaus ist es wichtig, dass die Arbeit einen originellen Beitrag zum bestehenden Wissensstand leistet. Originalität kann durch die Einführung neuer Metriken, innovative Anwendung bestehender Metriken oder durch die Entwicklung neuer Analyseansätze erzielt werden.   2. Wissenschaftliche Methodik  Die Auswahl der Methodik ist entscheidend für die Qualität einer wissenschaftlichen Arbeit. Bei der Untersuchung produktorientierter Metriken sollten sowohl qualitative als auch quantitative Methoden zum Einsatz kommen. Die Arbeit sollte eine klare Forschungsfrage definieren und systematisch beantworten. Hierzu gehören u. a.:  - Literaturrecherche: Eine umfassende Analyse bestehender Forschung muss durchgeführt werden, um den Kontext der Arbeit zu etablieren und bisherige Forschungslücken zu identifizieren. - Datenerhebung: Die Erhebung von Daten muss systematisch und nachvollziehbar erfolgen. Dies kann durch Umfragen, Interviews oder Fallstudien geschehen. - Datenanalyse: Die Anwendung geeigneter Analysemethoden ist essenziell, um valide Ergebnisse zu erhalten. Statistische Verfahren, Benchmarking und Case Studies sind Beispiele, die genutzt werden können.   3. Struktur und Kohärenz  Die Struktur der Arbeit muss logisch und kohärent sein. Dies umfasst eine klare Gliederung, die es dem Leser ermöglicht, den Gedankengängen leicht zu folgen. Übliche Strukturelemente sind:  - Einleitung: Vorstellung des Themas, der Fragestellungen und der Zielsetzung. - Theoretischer Hintergrund: Erläuterung relevanter Begriffe und Theorien, die produktorientierte Metriken der Softwarequalität betreffen. - Methodik: Detaillierte Beschreibung der Forschungsansätze und der Datenerhebung. - Ergebnisse: Präsentation und Diskussion der Findings in Bezug auf die gestellten Fragen. - Fazit und Ausblick: Zusammenfassung der Ergebnisse und Hinweise auf zukünftige Forschungsrichtungen.   4. Nachvollziehbarkeit und Transparenz  Eine hohe Nachvollziehbarkeit ist für die wissenschaftliche Integrität unerlässlich. Alle Methoden, Datenquellen und analytischen Schritte sollten klar dokumentiert werden. Der Leser sollte in der Lage sein, den Forschungsprozess nachzuvollziehen und die Ergebnisse zu reproduzieren. Transparenzerfordernisse können durch folgende Maßnahmen erfüllt werden:  - Vollständige Angabe der verwendeten Metriken. - Offenlegung der Datenherkunft und -verarbeitung. - Detaillierte Beschreibung der angewandten Analysemethoden.   5. Formale Kriterien  Die Einhaltung formaler Anforderungen ist ebenso wichtig. Dazu zählen:  - Sprachliche Qualität: Klare, präzise und wissenschaftliche Sprache ist Voraussetzung. Häufige Revisionen und das Einholen von Feedback können hier hilfreich sein. - Zitation und Referenzierung: Alle verwendeten Quellen müssen korrekt zitiert werden, um Plagiate zu vermeiden und den wissenschaftlichen Diskurs zu fördern. - Formatierung: Die Arbeit sollte gemäß den Vorgaben der Hochschule formatiert werden (z. B. Schriftart, Randabstände, Fußnoten).   Fazit  Die Qualitätsanforderungen an eine wissenschaftliche Arbeit über produktorientierte Metriken der Softwarequalität sind umfassend und vielschichtig. Sie umfassen sowohl inhaltliche als auch formale Aspekte und erfordern eine strukturierte Herangehensweise sowie ein hohes Maß an wissenschaftlicher Integrität. Durch die Beachtung dieser Anforderungen kann eine fundierte und wertvolle Arbeit entstehen, die dazu beiträgt, das Verständnis und die Anwendung von Metriken zur Bewertung der Softwarequalität zu vertiefen.;1
Azure DevOps Services kann als „Basic -Plan“ oder als „Basic  und Test Plan“ gebucht werden.  Der Plan „Basic und Test Plan“ wird aufgrund seines zu hohen  Preises von 49,46€ pro Monat  pro Person nicht berücksichtigt. Es wird nur der „Basic -Plan“ genauer betrachtet.   Das Lizenzmodell ist flexibel. Die Lizenzkosten werden tagesgenau berechnet . Erhält eine  Person beispielsweise nur einen halben Monat Zugriff auf die Software, wird sie nur zur Hälfte  des Monatspreises berechnet. Die monatlichen Kosten betrage n 5,71€, ein reduzierter Tarif  für Bildungseinrichtungen wird nicht angeboten. Die ersten 5 Personen werden nicht  berechnet. Das bedeutet, dass die ersten 5 Personen kostenlos sind,  und für jede weitere  Person eine Gebühr von 5,71€ im Monat erhoben wird. Jira Software kann in den Stufen „Free“, „Standard“, „Premium“ und „Enterprise“ lizensiert  werden. Besonders ist, dass der Preis für Bildungsstätten um die Hälfte reduziert wird. Deswegen beträgt der monatliche Preis einer Lizenz für Bildungsstätten  nicht $7,50 USD, sondern nur $3,75 USD und kann somit mit dem „Basic -Plan“ von Azure  DevOps konkurrieren. Die Stufe „Free“ ist nur für bis zu 10 Personen verfügbar und ist  deswegen keine Option. Die Stufen „Premium“ und höher kosten mit Bildungsstättenrabatt  mindestens $7,25 USD und sind deswegen keine Alternative zur bereits eingesetzten Software  Azure DevOps Services.;0
"Evaluierung: Zero - Möglichkeiten und Gefahren der digitalen Überwachung  In der heutigen Zeit, in der digitale Technologien unser Leben in nahezu allen Bereichen durchdringen, ist das Thema der digitalen Überwachung sowohl relevant als auch kontrovers. Die wissenschaftliche Arbeit mit dem Titel ""Zero - Möglichkeiten und Gefahren der digitalen Überwachung"" beleuchtet die vielfältigen Facetten dieses Phänomens und bietet eine differenzierte Analyse der damit verbundenen Chancen und Risiken.  Zunächst wird die Möglichkeit der digitalen Überwachung als ein Instrument der Sicherheit und Effizienz hervorgehoben. In vielen Bereichen, wie etwa der Kriminalitätsbekämpfung, der Terrorismusprävention oder der Gesundheitsüberwachung, können digitale Technologien dazu beitragen, Bedrohungen frühzeitig zu identifizieren und zu bekämpfen. Die Arbeit argumentiert, dass durch den Einsatz von Überwachungstechnologien, wie Gesichtserkennung oder Big Data-Analysen, eine proaktive Herangehensweise an Sicherheitsfragen möglich wird. Diese Technologien ermöglichen es, Muster zu erkennen und potenzielle Risiken zu minimieren, was in einer zunehmend komplexen und vernetzten Welt von großer Bedeutung ist.  Jedoch wird gleichzeitig auf die erheblichen Gefahren hingewiesen, die mit der digitalen Überwachung einhergehen. Die Arbeit thematisiert insbesondere die Verletzung der Privatsphäre und die potenzielle Missbrauchsgefahr von gesammelten Daten. In einer Gesellschaft, in der die Grenzen zwischen Sicherheit und Freiheit immer mehr verschwimmen, stellt sich die Frage, inwieweit der Einzelne bereit ist, persönliche Freiheiten zugunsten eines vermeintlichen Sicherheitsgewinns aufzugeben. Die Analyse zeigt, dass eine unkontrollierte digitale Überwachung nicht nur zu einem Verlust an Vertrauen in staatliche Institutionen führen kann, sondern auch zu einer Entmündigung des Individuums und einer Gefährdung demokratischer Werte.  Ein weiterer zentraler Punkt der Arbeit ist die Diskussion über die ethischen Implikationen der digitalen Überwachung. Die Autorin oder der Autor setzt sich kritisch mit der Rolle von Unternehmen und Regierungen auseinander, die durch den Einsatz von Überwachungstechnologien nicht nur die Daten der Bürger sammeln, sondern auch deren Verhalten beeinflussen können. Dies wirft grundlegende Fragen nach der Verantwortung und Transparenz auf, die in einer digitalen Gesellschaft unabdingbar sind.  Insgesamt bietet die Arbeit ""Zero - Möglichkeiten und Gefahren der digitalen Überwachung"" einen fundierten und ausgewogenen Überblick über ein hochaktuelles Thema. Sie regt zur Reflexion über die Balance zwischen Sicherheit und Freiheit an und fordert dazu auf, einen kritischen Blick auf die Entwicklungen im Bereich der digitalen Überwachung zu werfen. Die vielfältigen Argumente und Perspektiven, die in der Arbeit präsentiert werden, laden dazu ein, die eigene Haltung zu diesem Thema zu hinterfragen und sich aktiv an der Diskussion über die Zukunft der digitalen Gesellschaft zu beteiligen.   Die wissenschaftliche Auseinandersetzung mit den Möglichkeiten und Gefahren der digitalen Überwachung ist nicht nur von akademischem Interesse, sondern von essenzieller Bedeutung für das Verständnis der gesellschaftlichen Veränderungen, die durch die Digitalisierung hervorgerufen werden. In einer Welt, in der die Technologie ständig voranschreitet, ist es unerlässlich, die damit verbundenen Herausforderungen und Chancen kritisch zu beleuchten, um eine informierte und verantwortungsvolle Gesellschaft zu fördern.";1
Wie in Abschnitt 5.3.1beschrieben, besteht die Fahrzeugfernsteuerung aus dem Xbee- 3-Modul, dass auf dem Sparkfun ThingPlus -Controller montiert ist und über das I2C- Protokoll mit dem Adafruit Pygamer verbunden ist. Da der Xbee nur MicroPython unterstützt, wird dieser in MicroPython programmiert. Für den Pygamer-Controller stehen sowohl MicroPython als auch Arduino als Programmierumgebung zur Verfügung, hier wird jedoch Arduino ausgewählt, da compilierte Programme typischerweise schneller in der Ausführung sind als interpretierte Programme (vgl. ). Da für MicroPython nur eine Controller-Implementierung von I2Cgefunden werden konnte, agiert das Xbee- Modul im I2C-Kontext als Controller und der Pygamer als Peripheral. Zur besseren Veranschaulichung wird die Software als Flussdiagramm visualisiert und die einzelnen Funktionalitäten im Text genauer beschrieben. Xbee:Das Flussdiagramm des Programmablaufs ist in Abbildung 5.12zu sehen. Da der Xbee als I2C-Controller fungiert, übernimmt er die Steuerung des Ablaufs der I2C- Kommunikation zwischen Xbee und Pygamer, wodurch er diesem Werte sendet und anfragt. Zu Beginn wartet der Xbee-Controller, bis er den Pygamer auf dem I2C-Bus gefunden hat und sendet diesem in der spezifizierten Sendefrequenz alle 25ms eine steering message. Dabei fragt er die aktuelle Joystick-Position der Fernsteuerung ab und überprüft anschließend, ob eine collision avoidance message vom Fahrzeugcontroller empfangen wurde. Ist dies der Fall, so wird diese Nachricht an den Pygamer über den I2C-Bus gesendet. In jedem Fall aber wird danach überprüft, ob genügend Zeit vergangen ist und der ganze Zyklus wird wiederholt. Der Quellcode ist in Anhang 1zu finden. Pygamer: Wie in Flussdiagramm des Programmablaufs in Abbildung 5.13 dargestellt, reagiert der Pygamer als Peripheral nur auf Befehle des Controllers und übernimmt keine aktive Rolle in der Kommunikation mit dem Xbee-Controller.;0
Um den Luftreiniger in der Applikation bedienen zu können, wurde ein Fragmentmit Knöpfen erstellt und durch diese Knöpfe werden HTTP-Anfragen an den Webserver geschickt. Dazu wird in der Arbeit die Bibliothek Fuelfür Android verwendet, mit der HTTP-Anfragen gestellt werden können. In dem Listing 4.1 ist die Funktion der Stellung einer HTTP-Anfrage mit Fuelzu sehen. Der Luftreiniger kann unter der lokalen, statischen Internetprotokoll ( IP)-Adresse 192.168.0.196 aufgerufen werden (vgl. Ebert und Schweier 15.12.2021, S. 33). Daher ist es wichtig, dass das Tablet mit dem gleichen Router wie der Luftreiniger verbunden wird. Um die HTTP- Anfragen erfolgreich verschicken zu können, muss das Tablet sich authentifizieren. Das erfolgt durch die Angabe des im Programmcode des Luftreinigers (beziehungsweise des ESP32-Mikrocontrollers) definierten Schlüssels. In der Applikation sind Knöpfe zu finden (siehe Abschnitt 4.5) und mithilfe eines onClickListeners2wird nach klicken eines Knopfs die Funktion „getResponse“ mit der jeweiligen Betriebsstufe als Parameter aufgerufen und somit eine HTTP-Anfrage an den Webserver geschickt. Falls das Tablet nicht mit dem Router verbunden ist, werden die Nutzer*innen durch eine Toast Massage3darauf hingewiesen. Es kann zu einem Absturz der Anwendung kommen, wenn das Tablet nicht mit dem Router verbunden ist und vor der Erscheinung der Toast Massage eine andere willkürliche Stelle angeklickt wird. Ein Fragment stellt einen wiederverwendbaren Teil der Benutzeroberfläche der Anwendung dar. Es definiert und verwaltet sein eigenes Layout, hat seinen eigenen Lebenszyklus und kann seine eigenen Eingabeereignisse verarbeiten. (vgl. Android for Developers 26.10.2021) 2Schnittstellendefinition für einen Callback, der aufgerufen wird, wenn eine Ansicht angeklickt wird. (vgl. Android for Developers 10.02.2022c). 3Ein Toast bietet eine einfache Rückmeldung über einen Vorgang in einem kleinen Pop-up-Fenster (vgl. Android for Developers 12.11.2021).;0
Nach Abgleich der unterschiedlichen Registern mit der Memory Map werden die in 1.Verwendung eines Alias-Namen, um die Wiederverwendung des Moduls Circuits.I2C auf I2C zu beschränken 2. Ermitteln aller Geräte am I2C-Bus 3. Verwenden der Variable sensor für den BME680 4. Verwenden der Variable register für das Kontrollregister Ctrl_meas des BME680 5. Erstellen der I2C-Referenz für die Kommunikation über den Bus 6. Abfrage des Inhalts des Kontrollregisters (Modus: Forced Mode) 7. Konvertierung des Inhalts des Kontrollregisters in Binärsystem 8. Abfrage des Inhalts des Kontrollregisters (Modus: Sleep) 9. Konvertierung des Inhalts des Kontrollregisters in Binärsystem 10. Konfiguration der Kontrollregister Config und Ctrl_meas für den Modus Sleep 11.Konfiguration der Kontrollregister Config und Ctrl_meas für den Modus Forced Mode 12. Schreiben der Konfiguration in die Kontrollregister 13. Abfrage des Inhalts des Kontrollregisters (Modus: Sleep) 14. Konvertierung des Inhalts des Kontrollregisters in Binärsystem;0
Durch eine Vorrecherche unter Beachtung der erläuterten Aspekte, wird die Kamera „Raspberry Pi Kamera 175° Super-Weitwinkelobjektiv & Automatik Infrarot-Sperrfilter – Full HD mit Infrarot LEDs“ vonElectreeks® für diese Studienarbeit verwendet. Diese ist inAbbildung 4.1 zu sehen. Die Kamera hat ein Sichtfeld von 175 °diagonal, eine Auflösung von fünf Megapixel, nimmt Videos mit 1080p und 30 Frames per second ( FPS) auf. Die Kamera hat eine Abmaße von 80mm Breite, 30mm Länge und 28mm Höhe und wiegt 18,1 Gramm. Dazu hat die Kamera einen Infrarot-Sperrfilter, der sich automatisch je nach Lichtbedingung umschaltet und so in den Nachtsichtmodus wechseln kann, wenn es dunkel wird.  Der Katzenbesitzer soll auf seinem Smartphone ein Bild erhalten sobald das Raspberry Pi mithilfe der Kamera eine Katze erkannt hat. Diese soll auf dem Bild markiert sein, sodass der Besitzer die Katze schnell auf dem Bild erkennt oder feststellt, dass ein Fehler aufgetreten ist und der Algorithmus falsch lag und ein anderes Objekt als Katze erkannt wird. Aus diesem Grund wird für diese Arbeit ein Algorithmus für Objekterkennung verwendet, der die Katze auf dem Bild lokalisieren und markieren kann. Für die Erkennung von Objekten gibt es mehrere Ansätze. Beispielsweise können mithilfe vonTemplate Matching Objekte erkannt werden. Dabei werden bestimmte Charakteris- tiken bzw. Eigenschaften eines Objekts als Vorlage definiert. Anschließend wird überprüft, ob diese im Bild vorkommen. So wird entschieden, ob und an welcher Stelle ein gewisses Objekt im Bild vorkommt.;0
In der Entwicklung von IoTSystemen ist ein großes Problem das Fehlen von Standardi- sierung. Das bezieht sich auf mehrere Aspekte dieser Thematik, sowohl zu finden bei der Definition eines IoTSystems, als auch bei der Implementierung dieser Systeme . Da das Feld der IoTsehr neu ist und sich selbst in Maßstäben der Informationstech- nik in kurzen Zeiträumen stark wandelt, ist eine einheitliche Definition quasi unmöglich. Ebenso unterscheiden sich Anwendungsgebiete von privater Nutzung im Bereich „Smart Home“ zur Fernsteuerung von Geräten bis zur eingebetteten Informationssammlung in großen Industrieanlagen. Daher bietet sich eine relativ allgemeine Definition des Terms an, wie sie beispielsweise von Mattern et al. in ihrem Artikel „From the Internet of Computers to the Internet of Things“ vorgeschlagen wurde: „The Internet of Things represents a vision in which the Internet extends into the real world embracing everyday objects. Physical items are no longer disconnected from the virtual world, but can be controlled remotely and can act as physical access points to Internet services. An Internet of Things makes computing truly ubiquitous“  Hier steht ganz zentral die Rolle von IoTSystemen zur Verbindung der physischen, realen Welt mit der digitalen, virtuellen Welt im Vordergrund. Dieser Gedanke stellt einen kleinsten gemeinsamen Nenner für die Beschreibung von IoTSystemen dar, ohne auf deren weitergehende Funktionen einzugehen. Ebenso geht die Idee der Ubiquität, daher der Allgegenwertigkeit, von Computersystemen aus der Funktionsweise der IoTGeräte hervor. Zentral ist dabei die Integration dieser Geräte in den Alltag in einer unterschwelligen und nicht auffallenden Weise. Mark Weiser, der Vorreiter dieses von ihm „ubiquitous computing“ gennanten Gebiets, beschreibt dabei in seinem Artikel „The Computer for the 21st Century“, dass nicht der Mensch aktiv Maschinen benutzen solle, sondern stattdessen diese den Menschen unaufdringlich im Alltag unterstützen sollen.;0
Ein wichtiger Punkt zum Sammeln von Informationen sind Metadaten. Aus ihnen lassen sich Korrelationen feststellen und interpretieren. Sie sind oft sogar wichtiger als die eigentlichen Daten. Mit Metadaten wird im buch versucht Zero zu finden. So lassen sich als Beispiel die Metadaten von Fotos hervorragend tracken. Meta macht dies um Bilder Personen zuordnen zu können. Diese daten enthalten, wo ein Foto gemacht wurde und mit  welchem Gerät.  Es kann auch festgestellt werden, ob beim Teilen das Foto mit anderen  Personen verlinkt wird. Gleichzeitig wird jedem Foto eine eindeutige ID vergeben, um dieses  einfacher wiederzuerkennen, falls es erneut irgendwo im Netz auftaucht (Hurtz 2022), Bei  Meta ist das nicht unerwartet, aber Google kann das Tracking auch. So werden bei einem  Login in Googledienste ein Cookie  gesetzt der Einstellungen speichert. Dieser Cookie  bleibt  erhalten  und hat eine eindeutige  ID mit der kann Google dann bei weiteren Seitenaufrufen  mit demselben Browser den Nutzer seitenübergreifend namentlich identifizieren (Kuketz  2022b). Doch selbst auf Webseiten  die nichts mit Meta oder Google zu tun haben kann  Google doch noch tracken. Dies geht dann, wenn beispielsweise Google Fonts in einer  Webseite  eingebunden werden ohne diese selber zu hosten (online  2022c). Dadurch gelangt  Google wieder an Informationen, welche sie nutzen können, um ihre Datensammlung zu  vervollständigen. Doch auch der Staat kann Meta beim Tracking helfen. So hat 2020 das  Bundesministerium für Verteidigung Nutzer der App „Bundeswehr Media“ mit Facebook  und Google Firebase Analytics getrackt (Kuketz 2022a). Dadurch kann Meta die Interessen   eines Nutzers der App anhand der Benutzung der App berechnen (Facebooks unsichtbare  Datensammlung 2018). Dabei erfährt Meta zu welcher Uhrzeit mit welcher IP welche App  benutzt wurde. Damit lassen sich wieder viele Dinge  herleiten. Durch das umfangreiche  Sammeln von Metadaten können Interessen  eines Nutzers genau bestimmt werden. Anhand  dessen kann dem Nutzer dann auf seine Interessen gezielte Werbung  geschaltet werden.  Teilweise wird aber auch extra wie im Buch beschrieben falsche Werbung  eingefügt,  um  einen Creepiness-Effekt zu vermeiden ( Was passiert,  wenn W erbung  creepy wird? 2020).  Wie mächtig Metadaten sind, kann David Kriesel vom Chaos  Computer  Club bezeugen.  Er hat 2016 sämtliche Metadaten von Spiegelautoren gesammelt  und ausgewertet. Dabei  hat er nicht nur herausgefunden, wer über was gerne schreibt, sondern auch wer wann  arbeitet (Media.Ccc.de - SpiegelMining – Reverse  Engineering  von Spiegel-Online 2022).  Oder wer mit wem in den Urlaub geht. Es konnten Interessen bestimmt werden und sogar  Romanzen (ingo 2016).;0
 Aufbau eines Content-Management-Systems zur Erstellung von Android Apps für den humanoiden Roboter Pepper     Der humanoide Roboter Pepper, entwickelt von SoftBank Robotics, hat sich als vielseitiges Werkzeug in verschiedenen Anwendungsbereichen etabliert, darunter Bildung, Kundenservice und soziale Interaktion. Um die Interaktivität und Anpassungsfähigkeit von Pepper zu erhöhen, wurde ein Content-Management-System (CMS) konzipiert, das es Nutzern ermöglicht, ohne tiefgehende Programmierkenntnisse eigene Android-Apps für den Roboter zu erstellen. Diese Arbeit untersucht den Aufbau dieses CMS, die Herausforderungen während des Entwicklungsprozesses und die Ergebnisse des Projekts.   Aufbau des CMS  Das CMS wurde in mehreren Phasen entwickelt, beginnend mit der Anforderungsanalyse, um die Bedürfnisse der Endnutzer zu identifizieren. Eine benutzerfreundliche Oberfläche wurde entworfen, die es den Nutzern ermöglicht, Inhalte wie Texte, Bilder, Videos und interaktive Elemente einfach zu integrieren. Die Architektur des Systems basiert auf einer modularen Struktur, die es erlaubt, verschiedene Funktionalitäten als Plugins hinzuzufügen. Diese Flexibilität ist entscheidend, um zukünftige Erweiterungen und Anpassungen zu ermöglichen.  Die technische Umsetzung erfolgte unter Verwendung von Java und Android Studio, da Pepper auf dem Android-Betriebssystem basiert. Ein zentrales Element des CMS ist die API-Schnittstelle, die eine Kommunikation zwischen der App und den Hardwarekomponenten des Roboters ermöglicht. Hierbei wurden Sicherheitsaspekte und Datenintegrität besonders berücksichtigt, um eine reibungslose Interaktion zwischen Nutzer und Roboter zu gewährleisten.   Herausforderungen  Während der Entwicklung des CMS traten mehrere Herausforderungen auf. Eine der größten Hürden war die Gewährleistung der Benutzerfreundlichkeit. Es stellte sich heraus, dass viele potenzielle Nutzer, insbesondere im Bildungsbereich, wenig bis keine Erfahrung mit der App-Entwicklung hatten. Um diesem Problem entgegenzuwirken, wurden umfangreiche Tutorials und eine Hilfefunktion in das CMS integriert.   Ein weiteres Problem war die Anpassung der Apps an die spezifischen Interaktionsfähigkeiten von Pepper. Die Komplexität der Robotik und die Notwendigkeit, spezifische Bewegungs- und Sprachausgaben zu integrieren, erforderten eine enge Zusammenarbeit zwischen Entwicklern und Robotikexperten.    Fazit  Das entwickelte Content-Management-System stellt einen bedeutenden Schritt in der Demokratisierung der Roboterprogrammierung dar. Durch die Schaffung einer benutzerfreundlichen Plattform konnten auch Nutzer ohne technische Vorkenntnisse erfolgreich Apps für Pepper erstellen. Die modular aufgebaute Architektur des CMS bietet nicht nur Flexibilität, sondern auch die Möglichkeit, das System kontinuierlich zu erweitern und an neue Anforderungen anzupassen.  Die Rückmeldungen der ersten Nutzer zeigen, dass das CMS das Potenzial hat, die Einsatzmöglichkeiten von Pepper erheblich zu erweitern. Insbesondere im Bildungsbereich wird die Interaktion mit den Schülern durch individuelle, von Lehrern erstellte Apps bereichert.   Insgesamt zeigt das Projekt, dass die Verbindung von Robotik und nutzerfreundlicher Softwareentwicklung neue Horizonte eröffnet. Zukünftige Arbeiten sollten sich darauf konzentrieren, die Funktionalitäten des CMS weiter auszubauen und zusätzliche Integrationen zu ermöglichen, um die Interaktivität und den;1
  Die In-room Ortung, auch bekannt als Innenraumlokalisierung, hat in den letzten Jahren an Bedeutung gewonnen, insbesondere im Kontext der Sturzerkennung bei älteren Menschen und Personen mit erhöhtem Sturzrisiko. Die Kombination aus fortschrittlicher Sensortechnologie und drahtloser Kommunikation, insbesondere durch Bluetooth-Technologie, bietet vielversprechende Ansätze zur Verbesserung der Sicherheit und Lebensqualität dieser Bevölkerungsgruppen.   Grundlagen der In-room Ortung  Die In-room Ortung zielt darauf ab, die Position eines Objekts oder einer Person innerhalb eines geschlossenen Raumes präzise zu bestimmen. Im Gegensatz zur GPS-Technologie, die für die Ortung im Freien optimiert ist, erfordert die Innenraumlokalisierung alternative Methoden, um die Herausforderungen von Wänden, Möbeln und anderen Hindernissen zu überwinden. Zu den gängigen Verfahren zählen die Signalstärke-basierte Ortung, die trilaterale oder trianguläre Ortung sowie die Verwendung von Inertialsensoren.   Bluetooth-Technologie  Bluetooth ist eine weit verbreitete drahtlose Technologie, die für die Kurzstreckenkommunikation zwischen Geräten entwickelt wurde. Sie operiert im 2,4-GHz-Band und ermöglicht eine Energieeffizienz, die für tragbare Geräte von Vorteil ist. Bluetooth Low Energy (BLE), eine Variante der Bluetooth-Technologie, hat sich besonders für Anwendungen in der Gesundheitsüberwachung und der In-room Ortung etabliert. BLE ermöglicht eine längere Batterielebensdauer, was für tragbare Sensoren, die zur Sturzerkennung eingesetzt werden, von entscheidender Bedeutung ist.   Prinzipien der Sturzerkennung  Die Sturzerkennung kann durch die Analyse von Bewegungsmustern und Körperhaltungen realisiert werden. Hierbei kommen typischerweise verschiedene Sensoren zum Einsatz, darunter Beschleunigungssensoren, Gyroskope und Magnetometer, die in tragbaren Geräten oder Smart-Home-Systemen integriert sind. Diese Sensoren erfassen kontinuierlich die Bewegung und die Lage des Trägers. Bei einem Sturz zeigt die Analyse der Sensordaten charakteristische Muster, wie plötzliche Beschleunigungen oder abrupte Änderungen der Körperposition.   Implementierung der In-room Ortung mit Bluetooth  Die Implementierung eines Systems zur Sturzerkennung mittels Bluetooth umfasst mehrere Schritte. Zunächst ist die Installation von Bluetooth-Beacons in strategischen Positionen innerhalb eines Raumes erforderlich. Diese Beacons senden regelmäßig Signale aus, die von den tragbaren Geräten der Benutzer empfangen werden. Die Signalstärke (RSSI - Received Signal Strength Indicator) der empfangenen Signale wird verwendet, um die Entfernung zu den Beacons abzuschätzen. Durch die Kombination der Informationen von mehreren Beacons lässt sich die Position des Benutzers im Raum triangulieren.  Zusätzlich zur Positionierung müssen Algorithmen zur Datenanalyse entwickelt werden, um Stürze in Echtzeit zu erkennen. Diese Algorithmen verarbeiten die Sensordaten und identifizieren Muster, die auf einen Sturz hindeuten. Bei einer erkannten Sturzbewegung kann das System automatisch Notfallkontakte benachrichtigen oder erste Hilfe anfordern.   Herausforderungen und Ausblick  Trotz der Fortsch;1
Wie der Name schon sagt, prüft dieses statische Analysewerkzeug  C- oder C++ -Quellcode anhand des  MISRA -Standards. Außerdem wird die Einhaltung einiger weiterer Richtlinien und  Sicherheitsstandards wie CWE oder CERT sichergestellt. Der Fokus von QA-MISRA  liegt neben der  Durchsetzung von Qualitätseigenschaften wie Wartbarkeit besonders auf Sicherheitsaspekten.  Es  handelt sich um ein kommerzielles Tool, für das es keine langfristige frei verfügbare Version gibt.   Erklärtes Ziel von QA-MISRA  ist es folgerichtig Kosten zu sparen, indem Qualitäts - und  Sicherheitsprobleme zuverlässig zu einem frühen Zeitpunkt erkannt werden, und somit die  Wirtschaftlichkeit des Projekts zu steigern. Um einen Vergleich zu den kostenfreien Tools zu erhalten,  wurde QA-MISRA  in einem Probemonat für das studentische Softwareprojekt getestet.     Durch die Vergabe verschiedener Lizenzen sowie die Installation verschiedener Komponenten, ist die  Verwendung von QA-MISRA  nicht trivial. Ergebnis se der Aufsetzung des Tools sind ein Server mit  hinterlegter Lizenz sowie ein zugehöriger Client   . Zunächst muss der Server gestartet  und dessen Konfiguration überprüft werden. Beispielsweise muss das Verzeichnis ausgewählt  werden, in dem die erzeugten Analysedat en später abgelegt werden sollen. Eine Übersicht der  Konfigurationsüberfläche ist in Abbildung 16 zu sehen . Im Reiter „Analyse n“ können alle durch den  Server ausgeführten Scans für die verschiedenen Projekte eingesehen werden.  Anschließend wird  der Client mit dem Server verbunden, indem der Port angegeben wird, auf den der Server hört.  Im  Client kann daraufhin  ein neues Projekt angelegt werden. Dazu werden  die zu überprüfenden  Dateien ausgewählt und die anzuwendenden Regeln festgelegt. Die Auflistung der verfügbaren  Standards ist in Abbildung 17 dargestellt . Für die weiteren Betrachtungen ist nur die Messung der  Metriken relevant.  Diese beinhalten Aussagen über die Kommentardichte, die Verschachtelungstiefe,  Anzahl an Instruktionen oder Attributen und einige weitere Kennzahlen.;0
      Die Entwicklung von Anwendungen für humanoide Roboter wie Pepper erfordert eine interdisziplinäre Herangehensweise, die sowohl technologische als auch benutzerspezifische Aspekte berücksichtigt. Content Management Systeme (CMS) haben sich als effektive Werkzeuge etabliert, um die Erstellung, Verwaltung und Veröffentlichung von Inhalten zu erleichtern. Im Kontext der Robotik bietet ein CMS spezifische Vorteile, insbesondere im Hinblick auf die Anpassung und Interaktivität von Anwendungen. Dieser Text untersucht die theoretischen Grundlagen für den .   1.  des CMS  Ein Content Management System ist eine Softwareanwendung, die es Benutzern ermöglicht, digitale Inhalte zu erstellen, zu bearbeiten, zu organisieren und bereitzustellen, ohne tiefgehende Programmierkenntnisse zu benötigen. Die grundlegenden Komponenten eines CMS umfassen - BackendHier erfolgt die Datenverwaltung, einschließlich der Speicherung von Inhalten und Metadaten. Die Backend-Architektur sollte eine Datenbankanbindung (z. B. MySQL, MongoDB) sowie ein API-Management zur Kommunikation mit der Android-App umfassen.    - FrontendDas Frontend ist die Benutzeroberfläche, über die Nutzer mit dem CMS interagieren. Es sollte intuitiv gestaltet sein, um eine einfache Navigation und eine benutzerfreundliche Erstellung von Inhalten zu ermöglichen.  - Content-Management-FunktionalitätenDazu gehören Funktionen wie das Erstellen, Bearbeiten, Löschen und Kategorisieren von Inhalten. Ein WYSIWYG-Editor (What You See Is What You Get) kann die Benutzererfahrung erheblich verbessern.  - BenutzerverwaltungEin effektives CMS benötigt ein System zur Benutzerverwaltung, das unterschiedliche Rollen und Berechtigungen unterstützt. Dies ist besonders wichtig, wenn mehrere Benutzer mit unterschiedlichen Kenntnissen und Verantwortlichkeiten an der App-Entwicklung beteiligt sind.   2. Anforderungen an das CMS für Android Apps  Bei der Entwicklung eines CMS für Android Apps, die auf dem humanoiden Roboter Pepper laufen, sind spezifische Anforderungen zu berücksichtigen - Interaktive InhalteDie Möglichkeit, interaktive Elemente zu erstellen, ist entscheidend. Dazu gehören Dialogsysteme, Animationen und sensorische Eingaben, die Pepper für die Interaktion mit Benutzern nutzen kann.  - Multimodale KommunikationPepper ist darauf ausgelegt, mit Menschen über verschiedene Kommunikationskanäle zu interagieren, einschließlich Sprache, Gesten und Gesichtsausdrücken. Das CMS sollte die Integration dieser Kommunikationsformen unterstützen.  - Echtzeit-DatenverarbeitungUm eine reibungslose Interaktion zu gewährleisten, muss das CMS in der Lage sein, Echtzeitdaten zu verarbeiten und anzupassen. Dies kann durch WebSocket- oder MQTT-Protokolle erreicht werden, die eine bidirektionale Kommunikation ermöglichen.   3. Technologische Aspekte  Die technische Umsetzung eines CMS für Pepper erfordert die Berücksichtigung mehrerer Technologien - ProgrammiersprachenFür die Entwicklung der Android-Apps sind Java oder Kotlin notwendig. Das CMS selbst könnte in einer serverseitigen Sprache wie;1
"•Mapping im Backend: Werden Messwerte vom Backend (im Rahmen der Studienarbeit Node-RED) via MQTT vom TTN Server empfangen, so werden diese
gemapped. Vorteilhaft bei dieser Lösung ist, dass eine Änderung des Mappings
jederzeit ohne eine Umprogrammierung der Nodes möglich ist. Für Privatanwender
mit nur wenigen Nodes ist eine Umprogrammierung der Nodes noch mit vertretbarem
Aufwand realisierbar. Werden aber beispielsweise in einer Smart City hunderte Nodes
zur Überwachung sämtlicher Blumenbeete in Parks etc. eingesetzt, so wäre eine
Umprogrammierung dieser Nodes sehr zeitaufwendig und kostspielig.
Um ein Mapping der Messwerte auf Prozentpunkte erstellen zu können, wird zudem
eine ausreichende Menge von Messwerten über einen längeren Zeitraum benötigt. Im
Rahmen der Studienarbeit erfolgt das Mapping der Messwerte daher im Backend bzw. in
Node-RED. Somit können über einen längeren Zeitraum Messwerte gesammelt werden und
auf Grundlage dieser ein Mapping erstellt werden. Um die in der InﬂuxDB gespeicherten
MesswerteimNachhineinaufProzentpunktemappenzukönnen,wirdderin Abbildung4.18
gezeigte Node-RED Flow verwendet.
Der in Abbildung 4.19 Codeausschnitt wird im map to percentage Baustein des in Abbil-
dung 4.18 gezeigten Node-RED Flows verwendet. Im Code werden zwei Grenzwerte für
das Mapping deﬁniert:
•MIN:Bei einem Messwert der Bodenfeuchtigkeit von 460 (oder weniger) ist die Erde
feucht genug, um die Pﬂanze für einen längeren Zeitraum mit Wasser zu versorgen.
Es ist in nächster Zeit kein Gießvorgang nötig. Wird dieser Wert gemessen, so soll
die auf Prozentpunkte gemappte Bodenfeuchtigkeit 100 Prozent betragen.
•MAX:Bei einem Messwert der Bodenfeuchtigkeit von 565 (oder mehr) ist die
Erde so ausgetrocknet, dass sie der Pﬂanze kein Wasser mehr spenden kann. Ein
Gießvorgang ist umgehend erforderlich. Wird dieser Wert gemessen, so soll die auf
Prozentpunkte gemappte Bodenfeuchtigkeit 0 Prozent betragen.";0
Im Verlauf der Arbeit wurden mehrere Softwares verglichen, um festzustellen, welche  Softwares zu welchem Preis das studentische Softwareengineering unterstützen können. Die  günstige Software „Trello“ kann einige Grundanforderungen  erfüllen, zeigt jedoch bei der  Übersichtlichkeit und Datenpersistenz Schwächen und ist deswegen nur bedingt geeignet, um  die studentische Softwareentwicklung mit anschließender Benotung zu unterstützen. Die  umfangreicheren Softwares Azure DevOps Services, Jira Software und OpenProject können  mit ihrem Funktionsumfang gewährleisten, dass ein Projekt durchgeführt und anschließend  bewertet werden kann. Die Ermittlung der Arbeitsleistung einzelner Personen  ist jedoch nur  mit erhöhtem Aufwand durchführbar. Es wurde festgestellt, dass die Eigenentwicklung einer  Aufgabenverwaltungssoftware trotz reduziertem Funktionsumfang höchstwahrscheinlich nur  in mehreren Monaten abgeschlossen werden kann. Die Notwendigkeit, den Zugriff auf Daten  nutzerabhängig einzuschränken, und die vielen umzusetzenden Benutzeroberflächen mit  teilweise intelligenten Eingabefeldern erschaffen Komplexität, die als Benutzer der Software  nicht immer offensichtlich ist.   Die Lizenzkosten von Azure DevOps Services und Jira Software sind nicht niedrig, können aber  mit ihrem großen Funktionsumfang erklärt werden. Eine Ausnahme ist OpenProject, das einen  ebenfalls großen Funktionsumfang beinhaltet, jedoch auch ohne Lizenzkosten auf eigener  Hardware betrieben werden kann.;0
Die Visualisierung zeigt, wie stark die Signale sich verändern, dabei sind allerdings die Werte von allen Locators zusammengefasst und entsprechend der MAC-Adressen der Beacons aufgelistet. Um aussagekräftigere Informationen zu erhalten und zu erfahren, ob diese Signalunterschie- de an einem einzelnen Locator oder Beacon liegen, wird eine Auswertung erstellt, welche darstellt, wie stark das Signal sich verändert, wenn die Geräte nicht bewegt werden. Dabei fällt auf, dass eine einzelne Beacon-Locator-Kombinationen besonders starke Verän- derungen im Signal besitzt. Später konnte durch einen Versuch gezeigt werden, dass dieses Gerät kein BLEBeacon, sondern ein PC ist. Damit war die Veränderung kein Problem und konnte ignoriert werden, da dieser PC im Rahmen der Sturzerkennung keinen Mehrwert mit sich bringt. In der Zeit vor dieser Erkenntnis wurde viel im Bereich der Signalglättung, Filterung und Bestimmung der Distanz geforscht, beispielsweise auf Basis der Publikation  welche angibt, dass mithilfe von Kalman Filtern höhere Genauigkeiten bei der Projektion von RSSI Werten auf Distanzen erreicht werden kann.  Nachdem allerdings mithilfe weiterer Auswertungen (Anhang Seite 39) erkannt wurde, dass die Messwerte nicht so schlecht waren, wie die erste Auswertung vermuten ließ, wurde der Fokus wieder auf das Erkennen von Stürzen gelegt. In einer weiteren Arbeit könnte die Genauigkeit durch Verwendung besserer Projektionstechniken sowie Filter optimiert werden.;0
Ein Ausblick auf zukünftige Entwicklungen  Die fortschreitende Digitalisierung und Vernetzung von Fahrzeugen eröffnet neue Möglichkeiten in der Automobiltechnik, insbesondere im Bereich der Fernsteuerung. Die Entwicklung einer Fahrzeugfernsteuerung, die auf dem IEEE 802.15 Standard basiert, bietet nicht nur eine innovative Lösung zur Steuerung von Fahrzeugen, sondern auch zur Verbesserung der Sicherheit durch Kollisionsvermeidung. Der IEEE 802.15 Standard, der für drahtlose persönliche Netzwerke (WPANs) konzipiert wurde, ermöglicht eine energieeffiziente und zuverlässige Kommunikation zwischen Fahrzeugen und Steuergeräten.   Die Implementierung einer solchen Fernsteuerung erfordert eine präzise Datenübertragung und -verarbeitung, um Echtzeitinformationen über die Umgebung des Fahrzeugs zu erhalten. Sensoren, die in das Fahrzeug integriert sind, erfassen Informationen über andere Verkehrsteilnehmer, Hindernisse und die Straßenbedingungen. Diese Daten werden über das IEEE 802.15 Netzwerk an die Steuerzentrale übermittelt, die auf Basis von Algorithmen zur Kollisionsvermeidung entsprechende Steuerbefehle generiert. Die Herausforderung besteht darin, die Latenzzeiten der Datenübertragung zu minimieren und die Systemressourcen effizient zu nutzen, um eine reaktionsschnelle und sichere Fahrzeugsteuerung zu gewährleisten.  Ein vielversprechender Ansatz für die Weiterentwicklung dieser Technologie ist die Integration von Künstlicher Intelligenz (KI). Durch den Einsatz von maschinellem Lernen können Algorithmen entwickelt werden, die Muster im Fahrverhalten und in den Umgebungsbedingungen erkennen und darauf basierend präventive Maßnahmen zur Kollisionsvermeidung ergreifen. Diese intelligenten Systeme könnten nicht nur auf statische Hindernisse reagieren, sondern auch dynamisch auf das Verhalten anderer Verkehrsteilnehmer, was die Sicherheit erheblich erhöhen würde.  Ein weiterer Aspekt der Weiterentwicklung betrifft die Interoperabilität zwischen verschiedenen Fahrzeugmodellen und -marken. Die Schaffung einheitlicher Standards und Protokolle, die über den IEEE 802.15 Standard hinausgehen, könnte die Kommunikation zwischen Fahrzeugen verschiedener Hersteller erleichtern und somit die Effizienz von Kollisionsvermeidungssystemen steigern. Hierbei könnte die Entwicklung von offenen Schnittstellen und Plattformen eine Schlüsselrolle spielen, um die Integration verschiedener Technologien zu ermöglichen.  Zusätzlich könnten zukünftige Entwicklungen in der Sensorik, wie zum Beispiel LiDAR und hochauflösende Kameras, die Genauigkeit und Zuverlässigkeit der Umfelderfassung erheblich verbessern. Diese Technologien könnten in Kombination mit der Fahrzeugfernsteuerung eingesetzt werden, um eine 360-Grad-Sicht auf die Umgebung zu gewährleisten und so die Reaktionsfähigkeit des Systems zu optimieren.  Schließlich ist auch die Berücksichtigung ethischer und rechtlicher Aspekte von großer Bedeutung. Die Entwicklung autonomer Systeme wirft Fragen hinsichtlich der Haftung und der Entscheidungsfindung auf, insbesondere in kritischen Situationen. Eine interdisziplinäre Zusammenarbeit zwischen Technikern, Juristen und Ethikern wird notwendig sein, um Richtlinien und Standards zu etablieren, die sowohl die Sicherheit der Nutzer als auch die gesellschaftlichen Werte berücksichtigen.  Zusammenfassend lässt sich sagen, dass die Entwicklung einer Fahrzeugfernsteuer;1
 Vergleich zwischen Java und Kotlin  In der heutigen Softwareentwicklung stehen Programmierer vor einer Vielzahl von Programmiersprachen, die jeweils ihre eigenen Vorzüge und Herausforderungen bieten. Unter diesen Sprachen haben Java und Kotlin in den letzten Jahren besonders an Bedeutung gewonnen, insbesondere im Kontext der Android-Entwicklung. Während Java seit den 1990er Jahren als eine der führenden Programmiersprachen gilt, wurde Kotlin 2011 eingeführt und hat sich schnell als moderne Alternative zu Java etabliert. Dieser Vergleich beleuchtet die wesentlichen Unterschiede und Gemeinsamkeiten zwischen diesen beiden Sprachen, um ein besseres Verständnis für ihre jeweiligen Vorzüge zu vermitteln.   Syntax und Lesbarkeit  Ein zentraler Aspekt, der bei der Wahl zwischen Java und Kotlin berücksichtigt werden sollte, ist die Syntax. Java ist bekannt für seine strenge und ausführliche Syntax, die zwar eine klare Struktur bietet, jedoch auch zu einer erhöhten Codezeilenanzahl führen kann. Dies kann die Lesbarkeit und Wartbarkeit des Codes beeinträchtigen, insbesondere bei komplexen Anwendungen. Kotlin hingegen verfolgt einen anderen Ansatz. Die Sprache ist darauf ausgelegt, prägnanter und ausdrucksstärker zu sein. Sie ermöglicht Entwicklern, mit weniger Code mehr zu erreichen, was die Lesbarkeit und Wartbarkeit des Codes erheblich verbessert. Funktionen wie die Möglichkeit, Null-Sicherheitsprüfungen direkt in die Syntax zu integrieren, tragen dazu bei, häufige Fehlerquellen in Java zu minimieren.   Interoperabilität  Ein weiterer wichtiger Punkt ist die Interoperabilität. Kotlin wurde speziell entwickelt, um nahtlos mit Java zu arbeiten. Dies bedeutet, dass bestehende Java-Bibliotheken und -Frameworks ohne Probleme in Kotlin-Projekte integriert werden können. Entwickler, die bereits mit Java vertraut sind, können Kotlin schrittweise einführen, ohne ihre gesamte Codebasis neu schreiben zu müssen. Diese Interoperabilität ist ein entscheidender Vorteil, da sie den Übergang zu Kotlin erleichtert und es Entwicklern ermöglicht, die Stärken beider Sprachen zu nutzen.   Typensicherheit und Null-Sicherheit  Ein häufiges Problem in Java ist die NullPointerException, die oft zu Laufzeitfehlern führt. Kotlin hat dieses Problem durch ein striktes Null-Sicherheitssystem adressiert. In Kotlin sind Variablen standardmäßig nicht null, es sei denn, sie werden explizit als nullable deklariert. Dies zwingt Entwickler dazu, bewusster mit Null-Werten umzugehen und reduziert die Wahrscheinlichkeit von Laufzeitfehlern erheblich. Java bietet zwar Typensicherheit, jedoch nicht in dem Maße, wie es Kotlin tut. Diese verbesserte Handhabung von Null-Werten in Kotlin ist ein klarer Vorteil für Entwickler, die robuste und fehlerresistente Anwendungen erstellen möchten.   Entwicklungsumgebung und Tooling  Die Entwicklungsumgebung ist ein weiterer Faktor, der bei der Wahl zwischen Java und Kotlin berücksichtigt werden sollte. Beide Sprachen werden von gängigen IDEs wie IntelliJ IDEA und Eclipse unterstützt. Kotlin profitiert jedoch von einer hervorragenden Integration in die IntelliJ-Umgebung, was zu einer verbesserten Entwicklererfahrung führt. Funktionen wie intelligente Code-Vervollständigung, Refactoring-Tools und Debugging-Optionen sind in Kotlin besonders ausgefeilt, was den Entwicklungsprozess effizienter gestaltet.  ;1
"3.1.2 Konﬁguration
Um den Raspberry Pi als LoRa-Gateway zu verwenden, müssen alle Hardware Kompo-
nenten, wie in Abbildung 3.1 zu sehen, zusammengebaut und verbunden werden. Als
Betriebssystem wird die oﬃziell für den Raspberry Pi empfohlene Linux-Distributionen
Raspberry Pi OS Lite verwendet. Damit das Erweiterungsmodul und der Gateway-Chip
verwendet werden können, müssen in der Raspberry Pi Konﬁguration die Schnittstellen
Serial Peripheral Interface ( SPI) und Inter-Integrated Circuit ( I2C) aktiviert werden. Die
Änderung dieser Einstellungen können über das Konﬁgurationsmenü des Raspberry, welche
mit sudo raspi-config aufgerufen werden kann, vorgenommen werden. Die Einstellun-
gen liegen unterhalb des Menüpunktes „Interfacing Options“, welcher in Abbildung 3.2
dargestellt ist.
Abbildung 3.2: Interfacing Options des Raspberry Pi2
Da nun die Hardware miteinander kommunizieren kann, kann nun die benötigte Software
installiert und konﬁguriert werden. Diese kann vom oﬃziellen GitHub-Repository Lora-net
/sx1302\_hal3heruntergeladen oder direkt geklont werden. Da das Repository nur den
Sourcecode inklusive eines Makefile enthält, müssen die einzelnen Programme, die darin
enthalten sind, mit dem Befehl make all gebaut werden. Optional kann durch make clean
allsicher gestellt werden, dass alle zuvor gebauten Programme gelöscht werden und
somit die aktuellste Version der Sourcen zum Bau der ausführbaren Programme verwendet
werden. Die Bash-Script-Datei reset_lgw.sh , welche im tools-Verzeichnis liegt und zum
Zurücksetzen des Gateways verwendet wird, muss im Anschluss noch in die Verzeichnisse
util_chip_id und packet_forwarder kopiert werden. Die gebauten Programme greifen auf
die Script-Datei innerhalb des gleichen Unterverzeichnisses zu. Dementsprechend muss
dieses dort vorhanden und mit den nötigen Rechten zum Ausführen versehen sein. Diese
kann durch den Befehl chmod -R 755 ./* für alle Dateien im Order und die Verzeichnisse
darunter ermöglicht werden.";0
In der vorliegenden Arbeit werden fünf verschiedene Sensorwerte gemessen: •Geräusch •Helligkeit •Temperatur •Luftdruck •Humidität Diese Werte werden für die Visualisierung zu der Android-Applikation geschickt und dort abgespeichert. Eine der Herausforderungen bestand darin, einen Weg zu finden, die Sensordaten vom Arduino in die Android-Anwendung zu übertragen. Um die Versendung der Werte von dem Arduino zu der Applikation ohne Internetverbindung zu ermögli- chen, wird das Nachrichtenprotokoll Message Queuing Telemetry Transport ( MQTT) eingesetzt. Der zentrale Verteiler, der sogenannte MQTT-Broker, läuft in der Anwendung mit dem Namen Termux.Termux ist ein Android-Terminalemulator mit einer Linux- Umgebungsanwendung, bei dem ein minimales Basissystem automatisch installiert wird und ohne weitere Einstellungen funktionsfähig ist (vgl. Termux 2022). Der Arduino verbindet sich mit dem vorhandenen Router und mit dem MQTT-Broker. Es kann passieren, dass der Sensor ungültige Daten liefert und um das zu beseitigen, wurde Fol- gendes umgesetzt: Jede Minute werden die Sensorwerte gemessen und in Listen gespeichert. WenndieListedieLängezwölferreicht,wirddenDurchschnittvondenjeweiligenSensorwer- ten berechnet und an den MQTT-Broker für die Topic „dhbw_studienarbeit_sensordatas“ verschickt. Somit kann bei Falle eines unplausiblen Werts das Endergebnis ausgeglichen werden. Für die Fälle, falls das WLANausfällt, werden die berechneten Durchschnitte ebenfalls in Listen gespeichert. Gleichzeitig wird die Wiederherstellung der WLAN-Verbindung aufgenommen und solange dies fehlschlägt, werden die Daten lokal gespeichert. Falls die Listenlängen zwölf Elemente enthalten, werden die Listen halbiert, indem man den Durchschnitt zweier nebenstehender Daten berechnet. Nachdem sich der Arduino wieder mit dem WLANverbindet, werden die lokal gespeicherten Daten an den MQTT-Broker verschickt. Das Tablet verbindet sich ebenfalls mit dem MQTT-Broker und abonniert die Topic „dhbw_studienarbeit_sensordatas“, um die Sensordaten erhalten zu können. Die Daten werden dann in der SQLite-Datenbank gespeichert.;0
Momentan existieren auf dem SFTP-Server zwei Konten. Das Konto für den Editor hat auf alle Pepper-Applikationen Schreib- und Lesezugriﬀ, das Konto für die Pepper-Container- App hat auf alle Pepper-Applikationen nur Lesezugriﬀ. Alle Pepper-Applikationen liegen im Home-Verzeichnis des Editor-Kontos. Für einen produktiven Einsatz des CMS bräuchten alle User des Editors jeweils ein eigenes Konto und es wäre eine Konto- und Rechteverwal- tung nötig. Dies kann wie folgt umgesetzt werden. Ein User veriﬁziert sich mit seinem Login über den Editor beim Server und kann nur seine Pepper-Applikationen sehen und bearbeiten. Wenn eine Person eine Pepper-Applikation mit einer Anderen teilt, bekommt diese wahlweise nur Lesezugriﬀ oder Lese- und Schreibzugriﬀ auf diese Applikation. Zu- sätzlich wird die Pepper-App in das Home-Verzeichnis der zweiten Person verlinkt, damit es ihr im Editor angezeigt wird. Damit ist die Zusammenarbeit zwischen Usern möglich. Der Ablauf, damit eine Pepper-Applikation in der Pepper-Container-App sichtbar wird, ist der selbe. Das Server-Konto der Pepper-Container-App erhält allerdings nur Lesezugriﬀ auf Pepper-Applikationen. Um Server-Konten über den Editor zu verwalten, gibt es stan- dardmäßig ein Konto mit Server-Adminrechten. Dadurch kann es andere Konten erstellen, bearbeiten, löschen und Adminrechte erteilen sowie entziehen. Für diese Funktionalitäten sendet das Editor-Backend die entsprechenden CLI-Kommandos per SSH an den Server.;0
"Ausblick  Die Anforderungsanalyse im Kontext eines Aufgabenmanagement-Tools für das studentische Software Engineering eröffnet spannende Perspektiven und الفرص, die weit über die technische Implementierung hinausgehen. In Anbetracht der zunehmenden Komplexität der Softwareprojektarbeit in Bildungseinrichtungen, ist es maßgeblich, ein System zu entwickeln, das nicht nur den funktionalen Anforderungen gerecht wird, sondern auch die Aspekte der Benutzerfreundlichkeit, Zusammenarbeit und Lernförderung in den Fokus rückt.  Eine tiefere Analyse der Bedürfnisse und Erwartungen der Nutzer – sprich der Studierenden – zeigt, dass die Gestaltung klares und intuitives Design, mobile Zugänglichkeit und Schnittstellen zu bestehenden Plattformen umfasst. Solche Merkmale können die Effizienz und Motivation steigern und somit eine positive Lernumgebung fördern. Hier bietet sich zudem eine Möglichkeit zur Integration von Gamification-Elementen an; solche Ansätze könnten dazu dienen, den Selbstorganisation- und Teamarbeit-Gedanken zu stärken.  Des Weiteren könnte die Forschung auf den Umgang mit Herausforderungen ausgerichtet sein, die Studierende häufig bei der Durchführung von Teamprojekten erleben, wie eine unklare Aufgabenverteilung und mangelhafte Kommunikation. Das tooling müsste Funktionen bieten, die diese Probleme adressieren, und gleichzeitig Raum für agile Prozesse ermöglichen. Der kontinuierliche Austausch zwischen Entwicklern, pseudo-Betreuern und Studierenden kann wertvolle Einsichten liefern und die Tool-Nutzung sowie täglich Projekterfolge lavish fördern.  Zusammenfassend kann gesagt werden, dass die Entwicklung eines Aufgabenmanagement-Tools für den studenten Software Engineering Sektor – insbesondere in Bezug auf die Anforderungsanalyse – als determinierender Schritt miten Berücksichtigung heutiger Bildung-Technologien und experimenteller Ansätze görülen werden muss. Hierbei gilt es, sowohl die kurzfristigen Bedürfnisse zu erfüllen als auch auf langfristige Entwicklungen und potentielle neue Techniken oder Ansätze vorbereitet zu sein. Der Weiterentwicklungsaspekt, bedingt durch Feedback und Reviews in praxisnahen Projekten, wird nicht nur die Robustheit der Anwendung erhöhen, sondern es wird auch einen wesentlich wertvolleren Beitrag zur Förderung und Ausbildung angehender Software-Ingenieure leisten. Die Maßstäbe, die damit gesetzt werden, könnten nicht nur Wirkung zeigen, sondern sich auch in den Ansprüchen und Anforderungen zukünftiger Weiterbildungsmethoden niederschlagen.";1
Da sich die Anforderungen zwischen den verschiedenen Projekten mit ihren unterschied- lichen Einsatzbedingungen unterscheiden, werden hier die für dieses Projekt gültigen Anforderungen auf der Basis der erarbeiteten Angaben aus der Literatur festgelegt. Rechtliche Rahmenbedingungen: Aufgrund des Projektstandorts in Deutschland müssen die in Deutschland gültigen Regeln und Gesetze eingehalten werden. Dazu zählen unter anderem die Regeln zur Nutzung von Funk und die Straßenverkehrsordnung. Wie in Abschnitt 3.2.1erläutert, ist der Funkverkehr in Deutschland durch verschiedene Regularien reglementiert. So sind die verwendbaren Frequenzen des ISM-Bands auf die in Tabelle 3.1angegebenen Frequenzen beschränkt und diese müssen eingehalten werden. Für die Nutzung der Straße gilt, wie in Abschnitt 3.2.2 erläutert, dass das entwickelte Fahrzeug nicht auf öffentlichen Straßen bewegt werden darf. Anforderungen an Teleoperated Driving: Aus Projektsicht wird die maximal zu- lässige und erwartbare Geschwindigkeit auf 30km/h begrenzt. Dabei muss die maximale Latenz der Fernsteuerung 200ms unterschreiten, da dies die Mehrzahl der Quellen in 3.3 angibt, Ziel soll jedoch nach  eine Latenz von 100ms sein, wobei eine verschlüsselte Verbindung verwendet werden soll. Im Falle eines Verbindungsabbruch soll eine Notbremsung eingeleitet werden. Anforderungen an Kollisionsvermeidung: Die Kollisionsvermeidung soll aus Gründen der Komplexität als automatische Notbremsung festgelegt werden. Dabei soll bei einer maximal anzunehmenden Geschwindigkeit von 30km/h die Notbremsung so erfolgen, dass stehende Hindernisse bei normalen Straßenbedingungen und einem idealem Bremsweg keine Kollision mit Gegenständen in Fahrtrichtung erfolgen kann. Im Fall der Durch- führung einer Kollisionsvermeidung soll dies auf der Fernsteuerung dargestellt werden. Randbedingungen, wie Folgen für den nachfolgenden Verkehr oder ein Antiblockiersystem sollen nicht betrachtet werden. Projektspezifische Anforderungen: Aufgrund von Hardwarebeschränkungen muss das entwickelte Protokoll durch den Digi Xbee 3 -Microcontroller unterstützt werden. Zur einfacheren Bedienung soll im Falle einer durchgeführten Notbremsung eine Nachricht an die Fernsteuerung übermittelt und diese auf derselben dargestellt werden, dass eine Notbremsung durchgeführt wurde. Des weiteren sollen für verwendete Software und Protokollstacks keine Lizenzkosten anfallen.;0
"Evaluierung der wissenschaftlichen Arbeit: ""Aufbau eines CMS zur Erstellung von Android Apps für den humanoiden Roboter Pepper""  1. Einführung und Zielsetzung  Die vorliegende Arbeit zielt darauf ab, ein Content Management System (CMS) zu entwickeln, das es Nutzern ermöglicht, Android Apps für den humanoiden Roboter Pepper zu erstellen. Der Roboter Pepper, der für seine Interaktivität und Kommunikationsfähigkeit bekannt ist, ist ein beliebtes Forschungs- und Entwicklungsobjekt in der Robotik und Human-Robot-Interaction. Die Arbeit adresiert die Notwendigkeit, ein benutzerfreundliches System zu schaffen, das auch für Anwender ohne tiefgehende Programmierkenntnisse zugänglich ist.  2. Aufbau und Struktur der Arbeit  Die Arbeit ist klar strukturiert und gliedert sich in mehrere Abschnitte:  - Einleitung: Einführung in das Thema und die Relevanz des CMS. - Theoretischer Hintergrund: Detaillierte Erläuterungen zu den technologischen und konzeptionellen Grundlagen der App-Entwicklung für Pepper. - Konzeption und Entwicklung des CMS: Vorstellung der Architektur, der verwendeten Technologien und der Methoden zur Benutzeroberflächengestaltung. - Implementierung: Detaillierte Beschreibung des Entwicklungsprozesses, einschließlich verwendeter Frameworks und Tools. - Evaluation der Benutzerfreundlichkeit: Durchführung von Tests mit realen Nutzern und deren Feedback. - Schlussfolgerung: Zusammenfassung der Ergebnisse und Ausblick auf zukünftige Entwicklungen.  3. Wissenschaftliche Tiefe  Die Arbeit zeigt eine solide wissenschaftliche Basis. Die Literaturrecherche ist umfassend und behandelt sowohl bestehende Systeme zur App-Entwicklung für humanoide Roboter als auch die spezifischen Herausforderungen, die mit der Programmierung für Pepper verbunden sind. Der theoretische Teil wird durch relevante Quellen unterstützt und zeigt, dass der Autor in der Materie bewandert ist.  4. Praktische Relevanz und Innovation  Die Entwicklung eines CMS für Pepper stellt einen innovativen Ansatz dar, um die Zugänglichkeit von Robotik-Anwendungen zu erhöhen. Die Möglichkeit, interaktive Apps ohne tiefgehende Programmierkenntnisse zu erstellen, könnte die Verbreitung der Nutzung von Pepper in Bildung, Forschung und Dienstleistungen fördern. Die Arbeit adressiert auch potenzielle Nutzergruppen, die nicht im technischen Bereich tätig sind, und berücksichtigt deren Bedürfnisse in der Konzeption.  5. Benutzerfreundlichkeit und Evaluation  Die Evaluation der Benutzerfreundlichkeit ist ein wesentlicher Bestandteil der Arbeit. Der Autor hat verschiedene Szenarien zur Nutzung des CMS getestet und das Feedback von Anwendern gesammelt. Die Ergebnisse zeigen, dass das System insgesamt als intuitiv und benutzerfreundlich wahrgenommen wird. Einige Verbesserungsvorschläge wurden identifiziert, die in zukünftigen Versionen des CMS umgesetzt werden sollten. Das Engagement der Testnutzer durch Interviews oder Umfragen stärkt die Validität der Evaluation.  6. Fazit und Ausblick  Zusammenfassend lässt sich sagen, dass die Arbeit einen bedeutenden Beitrag zur Entwicklung von Anwendungen für den humanoiden Roboter Pepper leistet. Die Kombination aus theoretischer Fundierung und praktischer Umsetzung macht die Ergebnisse besonders wertvoll. Zukünftige Arbeiten könnten sich auf die Erweiterung des CMS mit zusätzlichen Funktionalitäten konzentrieren oder die Integration von maschinellem Lernen zur Verbesserung der Interaktivität und Anpassungsfähigkeit von Pepper-Apps.  7. Empfehlungen  Für eine noch tiefere Auseinandersetzung mit dem Thema wäre es sinnvoll, die folgenden Aspekte zu berücksichtigen:  - Erweiterung der Unterstützung für verschiedene Benutzeroberflächen-Layouts. - Integration von Tutorials oder Hilfesystemen direkt im CMS, um die Benutzerfreundlichkeit weiter zu erhöhen. - Eine umfangreiche Analyse der Marktbedürfnisse, um das CMS gezielt auf die Anforderungen der Anwender anzupassen.  Insgesamt stellt die Arbeit einen vielversprechenden Schritt in der Entwicklung benutzerfreundlicher Roboteranwendungen dar und legt die Grundlage für zukünftige Forschungsansätze in diesem dynamischen Bereich.";1
" Vergleich zwischen Java und Kotlin   Einleitung  Der technologische Fortschritt im Bereich der Softwareentwicklung hat in den letzten Jahren zu einer Vielzahl von Programmiersprachen geführt, die jeweils unterschiedliche Vorteile und Anwendungsbereiche haben. Zwei der prominentesten Sprachen im Bereich der Android-Entwicklung sind Java und Kotlin. Während Java seit den 1990er Jahren weit verbreitet ist, gewann Kotlin, das 2011 veröffentlicht wurde, besonders seit der offiziellen Unterstützung durch Google im Jahr 2017 an Popularität. Ziel dieses Vergleichs ist es, die beiden Sprachen hinsichtlich ihrer Syntax, Funktionalität, Sicherheit, Interoperabilität und Community zu analysieren.   1. Syntax  Die Syntax von Java und Kotlin unterscheidet sich erheblich. Java ist bekannt für seine explizite und verbose Syntax, die oft zu langen Codezeilen führt. Kotlin hingegen bietet eine prägnantere und ausdrucksstärkere Syntax, die Entwicklern ermöglicht, weniger Code zu schreiben, um dieselbe Funktionalität zu erreichen. Zum Beispiel benötigt Kotlin keine Semikolons am Ende der Zeilen und verwendet Datentypen in einer deutlich kompakteren Form.  *Beispiel für die Definition einer Funktion:*  - Java:     ```java   public int add(int a, int b) {       return a + b;   }   ```  - Kotlin:    ```kotlin   fun add(a: Int, b: Int) = a + b   ```  Die Lesbarkeit und Wartbarkeit des Codes kann durch die kürzere und klarere Syntax von Kotlin erheblich verbessert werden.   2. Funktionalität und Sprachmerkmale  Kotlin wurde mit dem Ziel entwickelt, einige der typischen Probleme und Einschränkungen von Java zu überwinden. Zu den herausragenden Merkmalen von Kotlin gehören:  - Null-Sicherheit: Kotlin führt das Konzept der Null-Sicherheit ein, durch das NullPointerExceptions (NPEs) im Code vermieden werden können. Variablen sind standardmäßig nicht null und müssen explizit als nullable deklariert werden.  - Erweiterungsfunktionen: Mit Kotlin können Entwicklern neue Funktionen zu bestehenden Klassen hinzufügen, ohne sie zu ändern, was die Flexibilität erhöht.  - Datenklassen: Kotlin bietet eine spezielle Syntax zur Erstellung von Datenklassen, die automatisch Getter und Setter sowie die `toString()`, `equals()` und `hashCode()`-Methoden generieren.  Java hingegen hat einige dieser modernen Funktionen erst in neueren Versionen (z.B. durch Lambda-Ausdrücke in Java 8) eingeführt, kann aber im Vergleich zu Kotlin als weniger fortschrittlich angesehen werden.   3. Sicherheit  Die Sicherheit ist ein entscheidendes Kriterium in der Softwareentwicklung. Kotlin wurde mit dem Fokus auf geringe Fehleranfälligkeit und Benutzerfreundlichkeit entwickelt. Die Null-Sicherheit, die im vorherigen Abschnitt beschrieben wurde, ist ein bedeutendes Merkmal, das Kotlin sicherer macht. Java bietet zwar die Möglichkeit, NPEs durch geeignete Programmiertechniken zu vermeiden, hat jedoch keine integrierten Mechanismen in der Sprache selbst.  Zusätzlich bietet Kotlin viele Werkzeuge zur Fehlerbehandlung, die einfacher und intuitiver sind als die traditionellen Ansätze in Java.   4. Interoperabilität  Ein großer Vorteil von Kotlin ist die vollständige Interoperabilität mit Java. Kotlin kann in bestehenden Java-Projekten verwendet werden, was eine schrittweise Migration und Integration ermöglicht. Entwickler können Kotlin-Code in Java-Projekten verwenden und umgekehrt, was die Einführung der Sprache erleichtert.   5. Community und Ökosystem  Java hat eine lange Geschichte und eine große, etablierte Community. Es gibt viele Ressourcen, Bibliotheken und Frameworks, die für Java verfügbar sind. Kotlin ist dagegen noch relativ jung, erfreut sich jedoch wachsender Beliebtheit, insbesondere in der Android-Entwicklung. Die Community wächst kontinuierlich und es gibt bereits eine Vielzahl von Ressourcen, die speziell auf Kotlin zugeschnitten sind.   Fazit  Zusammenfassend lässt sich sagen, dass sowohl Java als auch Kotlin ihre eigenen Vorzüge und Herausforderungen mitbringen. Java bietet eine bewährte, stabile Grundlage mit umfangreicher Dokumentation und Gemeinschaft, während Kotlin modernere Sprachmerkmale und eine verbesserte Syntax aufweist, die die Entwicklung effizienter und sicherer gestalten kann. Die Wahl zwischen den beiden Sprachen hängt letztlich von den spezifischen Anforderungen eines Projekts, den Erfahrungen des Entwicklungsteams und den langfristigen Zielen ab. Für neue Projekte, insbesondere im Bereich der Android-Entwicklung, könnte Kotlin die bevorzugte Wahl sein, während Java in vielen bestehenden Systemen weiterhin eine zentrale Rolle spielt.";1
 Kapitel: Technische Grundlagen der Content-Management-Systeme   Einleitung  Content-Management-Systeme (CMS) sind zentrale Komponenten in der modernen Web- und Unternehmenskommunikation. Sie ermöglichen das Erstellen, Verwalten und Veröffentlichen digitaler Inhalte durch Nutzer ohne tiefe technische Kenntnisse. Um ein tieferes Verständnis für die Funktionsweise und Unterschiede der verschiedenen CMS zu gewinnen, ist es notwendig, die zugrunde liegenden technischen Grundlagen zu betrachten. In diesem Kapitel werden die technischen Merkmale und ihre Bedeutung erläutert, um fundierte Vergleichskriteria für eine umfassende Analyse der CMS zu entwickeln.   Komponenten eines CMS  Ein Content-Management-System setzen sich in der Regel aus mehreren Schlüsselkomponenten zusammen:  1. Datenbankmanagement: Um Inhalte zu speichern, verwenden CMS häufig relationale Datenbanken wie MySQL, PostgreSQL oder SQLite. Diese Datenbanken strukturieren die Inhalte in Tabellen und ermöglichen eine effiziente Abwicklung von Abfragen und Transaktionen.  2. Backend-Architektur: Dies ist das „Herz“ des CMS, wo die Geschäftslogik implementiert ist. Hierbei kommen Server-seitige Programmiersprachen zum Einsatz, wie PHP, Python, Ruby oder Java. Die Kommunikation zwischen Frontend und Backend erfolgt häufig über API-Schnittstellen.  3. Frontend-Rendering: Umsichtige und ansprechende Nutzeroberflächen sind entscheidend für den Erfolg eines CMS. Diese Oberflächen werden typischerweise in HTML, CSS und JavaScript entwickelt. Bei vielen CMS sind Template-Engines integriert, die das dynamische Generieren von Inhalten auf den Webseiten ermöglichen.  4. Benutzermanagement: Ein essentielles Feature ist die Verwaltung der User-Rollen und Berechtigungen. CMS verwenden häufig hinterlegte пользовательские Rechte ([Permissions]), um sicherzustellen, dass nur autorisierte Benutzer unter spezifischen Bedingungen Inhalte erstellen oder bearbeiten können.   Architekturmodelle von CMS  Abhängig von ihrer Architektur können CMS in mehrere Typen unterteilt werden:  1. Monolithische Systeme: Diese Art von CMS vereint sowohl Fron- als auch Backend in einem vollständig integrierten Paket. Typische Vertreter dieser Gruppe sind WordPress und Joomla. Sie bieten den Nutzern vorgefertigte Lösungen, die ohne tiefere technische Anpassungen sofort funktionsfähig und gerichtete Einsatzbereit sind.  2. Köpflösungsarchitekturen (Headless CMS): In dieser Struktur ist der Inhalt unabhängig von der Visualisierung. Frontend und Backend sind zunächst voneinander getrennt, sodass der Workflow dadurch flexibler gestaltet werden kann. Nutzer greifen in dem cercipheitüyen CMS auf die benötigten Inhalte über API-Calls zu (wie bei Contentful oder Strapi). Dies bietet sich insbesondere für moderne Webanwendungen und mobile Entwicklungen an.  3. Decoupled CMS: Eine Mischform zwischen monolithischen und headless Systemen, wo das Frontend grundlegend unabhängig vom Backend ist, aber trotzdem in umfangreicher Mehrwertpakete=eisen und standardisiert nutzenzufuguar daanhine chodzi re ausgezeichnet potzen decamophi pomaca uzakuti contenido gewährleisten umзывает.   Programmierstile und -sarcyemdeputlokokiке  Die Programmierungen innerhalb der CMS feu estreno padaizinointeriorriereku enabling user ерек 泰皇;1
Seit dem Release des durch das WordPress Entwicklerteam eigens kreierte Theme „Twenty Twenty Two“ erhält der sogenannte „Gutenberg Editor“ Einzug in die Gestaltungsmög- lichkeiten des Systems. Mit diesem Editor, welcher Inhaltsbausteine in Form von „Blocks“ (Blöcken) abstrahiert, lässt sich eine Website mittels Drag-and-Drop Verfahren beliebig abändern. Einzelne Inhaltsbausteine können somit nach Belieben ausgetauscht und in ihrer Reihenfolge abgeändert werden. Abbildung 5.12: Gutenberg Block-Editor in WordPress1 Aufgrund der Vielzahl an verfügbaren Design-Vorlagen und der einfach Installation sowie Handhabung dieser können ansprechende, responsive Websites in kurzer Zeit und ohne Programmierkenntnis realisiert werden. Deshalb wird WordPress im Kriterium Design- Vorlagen mit der Farbe Grün kategorisiert.;0
 Die Demografie vieler Länder zeigt eine zunehmende Alterung der Bevölkerung, was zu einem höheren Bedarf an Technologien zur Sturzerkennung führt. In diesem Projekt wird eine In-room Ortungslösung unter Verwendung von Bluetooth Low Energy (BLE) entwickelt, um Stürze in realen Wohnumgebungen zu erkennen und somit die Sicherheit älterer Menschen zu erhöhen.  2. Zielsetzung  Ziel dieser Arbeit ist es, ein System zu entwickeln, das: - die Position einer Person im Raum präzise erfasst, - Stürze in Echtzeit erkennt, - Alarmmeldungen an relevante Pflegepersonen oder Systeme sendet.  3. Technische Anforderungen  - Hardware:   - BLE Beacon-Geräte zur Positionsbestimmung.   - Smartphones oder Tablets als Empfangsgeräte.   - Sensormodule zur Sturzerkennung (z.B. Beschleunigungssensoren).  - Software:   - Eine mobile App zur Datenverarbeitung und Fallanalyse.   - Backend-Server zur Speicherung und Analyse von Daten.   - Algorithmus zur Sturzerkennung.  4. Systemarchitektur  Das System besteht aus mehreren Komponenten: 1. BLE Beacons:    - Installierung in verschiedenen Räumen.    - Senden von Identifikationssignalen in regelmäßigen Abständen.    2. Mobile App:    - Empfang und Verarbeitung der BLE-Signale.    - Erstellung eines Kartenmodells der Umgebung auf Basis der Beacon-Signale zur Positionsbestimmung.    - Integration von Sensoren zur Sturzerkennung.    3. Backend-Server:    - Empfang der Alarmmeldungen.    - Datenanalyse und Speicherung.    - Bereitstellung von Benutzeroberflächen für Pflegepersonal.  5. Realisierung   5.1 BLE Beacon Konfiguration  1. Auswahl der Beacon-Hardware (z.B. estimote, Kontakt.io). 2. Konfiguration der Beacons über eine entsprechende App oder API. 3. Montage der Beacons in strategischen Positionen, um eine optimale Abdeckung zu gewährleisten.   5.2 Entwicklung der Mobile App  1. Plattformwahl (iOS/Android). 2. Implementierung des BLE-Signalempfangs:    - Verwendung der APIs für BLE (z.B. CoreBluetooth für iOS, Android Bluetooth API).    - Erfassung von Signalstärken zur Positionsbestimmung.  3. Integration der Sturzerkennung:    - Verwendung von beschleunigungsmessenden Sensoren (z.B. IMU-Sensoren).    - Entwicklung eines Algorithmus zur Identifikation von Sturzereignissen, basierend auf der Analyse von Beschleunigungsdaten.   5.3 Backend-Server  1. Auswahl einer geeigneten Servertechnologie (z.B. Node.js, Python Flask). 2. Entwicklung einer RESTful API zur Kommunikation zwischen App und Server. 3. Speicherung und Analyse der Daten in einer Datenbank (z.B. PostgreSQL). 4. Implementierung von Benachrichtigungen (z.B. E-Mail, SMS) bei Sturzerkennung.   5.4 Test und Validierung  1. Durchführung von Tests in kontrollierten Umgebungen;1
Da bei Verwendung eines Buffers die Äquivalenz von Messzeitpunkt und Zeitpunkt des Datenbankeintrags nicht gegeben ist, muss der konkrete Zeitpunkt Teil des resultierenden Messwerts des Sensors sein. Das Feld measure_timestamp beinhaltet diesen Zeitpunkt. Ein exemplarischer Body der POST-Request zeigt Abbildung 3.14. Abbildung 3.14: Body der POST-Request mit implementiertem Buffer Modifikationen an der Phoenix-Applikation erfolgen analog zum dargestellten JSON- Objekt, damit Informationen der POST-Requests in der neu vorliegenden Form korrekt interpretiert und bearbeitet werden können. Die Funktion create_entry/1 wird nach Verifikation des eingehenden Token mit dem korrespondierenden JSON-Objekt aufgerufen und ist in folgender Abbildung 3.15 dargestellt. Die Funktion iteriert über alle eingehenden Datensätze und erstellt den entsprechenden Eintrag in der Datenbank (Zeile 8-10). Die fundamentale Vorgehensweise bleibt unverän- dert. Zusätzlich wurde die Relation weather_condition um das Feld measure_timestamp erweitert.;0
" Kapitel: Zero - Möglichkeiten und Gefahren der digitalen Überwachung  Der Roman ""Zero"" von Marc Elsberg ist ein fesselndes Werk, das die tiefgreifenden Implikationen der digitalen Überwachung auf individuelle Freiheiten und gesellschaftliche Strukturen thematisiert. In einer Welt, in der Daten als das neue Öl gelten, beleuchtet Elsberg die Ambivalenz zwischen den Möglichkeiten, die digitale Technologien bieten, und den Gefahren, die mit ihrer unkontrollierten Nutzung einhergehen.    Die Möglichkeiten der digitalen Überwachung  Im Zentrum von ""Zero"" steht die Frage, wie digitale Überwachung dazu genutzt werden kann, das Leben der Menschen zu verbessern. Die Protagonisten des Romans bewegen sich in einer zunehmend vernetzten Welt, in der Informationen in Echtzeit erfasst und analysiert werden. Diese Technologie hat das Potenzial, verschiedene gesellschaftliche Probleme anzugehen, von der Kriminalitätsbekämpfung bis hin zur Gesundheitsüberwachung. Durch die Erfassung von Daten können Muster erkannt und präventive Maßnahmen ergriffen werden, die das Sicherheitsgefühl der Bürger erhöhen.   Ein Beispiel für diese Möglichkeit ist die Verwendung von Überwachungstechnologien zur Verhinderung von Verbrechen. In ""Zero"" wird gezeigt, wie durch die Analyse von Bewegungsdaten und Verhaltensmustern potenzielle Kriminelle identifiziert und rechtzeitig gestoppt werden können. Dies könnte in einer idealen Welt zu einer signifikanten Senkung der Kriminalitätsraten führen und das Vertrauen in öffentliche Institutionen stärken. Darüber hinaus könnten digitale Überwachungsmaßnahmen auch im Gesundheitssektor eingesetzt werden, um Epidemien frühzeitig zu erkennen und zu bekämpfen.   Die Gefahren der digitalen Überwachung  Trotz dieser positiven Aspekte thematisiert Elsberg in ""Zero"" auch die gravierenden Gefahren, die mit der digitalen Überwachung verbunden sind. Die unkontrollierte Erfassung und Auswertung persönlicher Daten birgt das Risiko eines massiven Eingriffs in die Privatsphäre der Individuen. Der Roman verdeutlicht, wie schnell aus präventiven Maßnahmen eine umfassende Überwachungsgesellschaft entstehen kann, in der jeder Schritt und jede Handlung der Bürger aufgezeichnet und analysiert wird.  Ein zentrales Element der Handlung ist die Entwicklung einer mächtigen Überwachungssoftware, die nicht nur die Privatsphäre der Bürger bedroht, sondern auch die Grundpfeiler einer demokratischen Gesellschaft untergräbt. Die Protagonisten stehen vor der Herausforderung, sich gegen diese Überwachung zu wehren und die Kontrolle über ihre Daten zurückzugewinnen. Hier wird die Frage aufgeworfen, inwiefern der Schutz der individuellen Freiheit und der Privatsphäre mit dem Sicherheitsbedürfnis der Gesellschaft in Einklang gebracht werden kann.  Die ethischen Implikationen der digitalen Überwachung werden in ""Zero"" eindringlich behandelt. Die Macht, die durch die Kontrolle über Daten entsteht, kann missbraucht werden, um Menschen zu manipulieren oder zu diskriminieren. Die Gefahr eines Überwachungsstaates, in dem Bürger permanent unter Beobachtung stehen, wird eindringlich skizziert. Die dystopischen Szenarien, die Elsberg entwirft, werfen fundamentale Fragen über die Zukunft der Demokratie und die Rolle des Individuums in einer von Technologie dominierten Welt auf.  ";1
Ein Konzept zur Umsetzung    In der heutigen Zeit, in der Luftqualität und Umweltbewusstsein zunehmend in den Fokus der Öffentlichkeit rücken, gewinnen Luftreinigungsgeräte an Bedeutung. Diese Geräte sind nicht nur in der Lage, Schadstoffe und Allergene aus der Luft zu filtern, sondern können durch elektronische Erweiterungen auch in ihrer Funktionalität und Benutzerfreundlichkeit optimiert werden. Ziel dieses Textes ist es, ein Konzept zur Optimierung der Visualisierung, Bedienung und Selbstregelung eines solchen Luftreinigungsgerätes zu entwickeln.  1. Visualisierung der Luftqualitätsdaten  Die Visualisierung von Luftqualitätsdaten spielt eine zentrale Rolle für die Benutzerakzeptanz und -interaktion. Um eine intuitive und ansprechende Benutzeroberfläche zu gestalten, sollte ein mehrstufiges Konzept verfolgt werden - DatenaggregationZunächst müssen relevante Datenquellen identifiziert werden, die Parameter wie PM2.5, PM10, VOCs (flüchtige organische Verbindungen) und CO2 umfassen. Diese Daten sollten in Echtzeit erfasst und aggregiert werden, um ein umfassendes Bild der Luftqualität zu liefern.  - VisualisierungsansätzeDie Darstellung dieser Daten kann durch verschiedene Ansätze erfolgen. Eine Kombination aus numerischen Werten, grafischen Diagrammen und Farbcodierungen ermöglicht eine schnelle und einfache Interpretation der Luftqualität. Beispielsweise könnte ein Ampelsystem (rot, gelb, grün) für die allgemeine Luftqualität implementiert werden, während detaillierte Grafiken spezifische Schadstoffkonzentrationen anzeigen.  - BenutzerinteraktionEine interaktive Benutzeroberfläche, die es den Nutzern ermöglicht, verschiedene Zeiträume auszuwählen oder spezifische Daten zu filtern, fördert das Verständnis der Luftqualität und steigert das Engagement.  2. Optimierung der Bedienung  Die Benutzerfreundlichkeit ist entscheidend für die Akzeptanz eines Luftreinigungsgerätes. Um die Bedienung zu optimieren, sollten folgende Aspekte berücksichtigt werden - Intuitive SteuerungDie Implementierung eines Touchscreens mit einer klar strukturierten Benutzeroberfläche ermöglicht eine einfache Navigation durch die verschiedenen Funktionen des Gerätes. Wichtige Funktionen wie Ein/Aus, Geschwindigkeitsregelung und Filterwechsel sollten leicht zugänglich sein.  - Sprachsteuerung und App-IntegrationDie Integration von Sprachsteuerung und einer mobilen App ermöglicht es den Nutzern, das Gerät bequem zu steuern und Einstellungen vorzunehmen, ohne physisch mit dem Gerät interagieren zu müssen. Push-Benachrichtigungen in der App könnten die Nutzer über die Luftqualität und notwendige Wartungsmaßnahmen informieren.  - Ergonomie und DesignDas physische Design des Gerätes sollte ergonomisch gestaltet sein, sodass es einfach zu bedienen und zu warten ist. Eine klare Kennzeichnung der Bedienelemente und eine ansprechende Gestaltung fördern die Benutzerfreundlichkeit.  3. Selbstregelung des Luftreinigungsgerätes  Die Selbstregelung ist ein entscheidendes Merkmal moderner Luftreinigungsgeräte, das zur Effizienz und Energieeinsparung;1
MQTTist ein von Organization for the Advancement of Structured Information Standards (OASIS) standardisiertes Nachrichtenprotokoll, welches unter anderem auf das Senden von Nachrichten trotz hoher Latenzen oder eingeschränkten Netzwerken spezialisiert ist. Aus diesem Grund wird es in der Machine-to-Machine-Kommunikation ( M2M) von IoT-Netzen besonders of genutzt. Somit wird MQTThäufig auf Geräten mit geringer Rechenleistung, wie Sensoren, Eingebetteten Systemen oder Aktoren verwendet. Dabei besteht eine MQTT-Infrastruktur aus solchen Sensoren und anderen Geräten, welche Daten senden, diese werden Publisher genannt. Deren Nachrichten laufen über einen MQTT-Broker, der diese Nachrichten dann speichert und/oder an sogenannte Subscriber weiterleitet. Diese können zum Beispiel Aktoren sein, die damit einen Befehl erhalten, oder Computer mit denen diese Daten gespeichert und/oder analysiert werden. Durch diese Struktur wird auf den Geräten mit geringer Rechenleistung nur eine Verbindung zum Broker aufgebaut und die entsprechenden Daten gesammelt und verschickt, die eigentlichen, rechenintensiven Analysen und das speicherintensive Archivieren werden somit auf Computer ausgelagert, die dazu besser in der Lage sind.;0
" Eine vergleichende Analyse der   In der Softwareentwicklung ist die Wahl der Programmiersprache entscheidend für den Erfolg eines Projekts. Zwei der prominentesten Sprachen im Bereich der Android-Entwicklung sind Java und Kotlin. Beide Sprachen bieten unterschiedliche Paradigmen, Syntaxen und Entwicklungsansätze, die sich unmittelbar auf die  auswirken. In diesem Text wird eine vergleichende Analyse der beiden Sprachen durchgeführt, um deren Vor- und Nachteile bei der Implementierung einer individuellen Softwarelösung herauszuarbeiten.   1. Historischer Kontext und Adoption  Java wurde 1995 von Sun Microsystems eingeführt und hat sich schnell zu einer der meistgenutzten Programmiersprachen entwickelt. Sie ist bekannt für ihre Plattformunabhängigkeit, Robustheit und breite Community-Unterstützung. Kotlin hingegen wurde 2011 von JetBrains veröffentlicht und 2017 von Google als offizielle Sprache für die Android-Entwicklung anerkannt. Die Entscheidung für Kotlin oder Java hat daher historische und kontextuelle Dimensionen, die Entwickler bei der Wahl berücksichtigen müssen.   2. Syntax und Lesbarkeit  Ein zentrales Merkmal, das bei der Implementierung einer Lösung in Betracht gezogen werden muss, ist die Syntax der jeweiligen Sprache. Java ist bekannt für seine ausführliche und oftmals verbose Syntax, die es zwar ermöglicht, den Code klar zu strukturieren, jedoch die Lesbarkeit beeinträchtigen kann. Beispielhaft ergibt sich folgendes Codefragment zur Implementierung einer einfachen Klasse ```java public class Person {     private String name;     private int age;      public Person(String name, int age) {         this.name = name;         this.age = age;     }      public String getName() {         return name;     }      public int getAge() {         return age;     } } ```  Im Vergleich dazu ermöglicht Kotlin eine deutlich prägnantere Syntax, die die Lesbarkeit und Wartbarkeit verbessert ```kotlin data class Person(val nameString, val ageInt) ```  Die Verwendung des `data class`-Schlüsselworts in Kotlin minimiert Boilerplate-Code und erhöht die Transparenz der Implementierung.   3. Typensicherheit und null-Sicherheit  Ein weiterer wichtiger Aspekt bei der Wahl zwischen Java und Kotlin ist das Thema Typensicherheit, insbesondere im Hinblick auf null-Werte. Java ist anfällig für NullPointerExceptions, da null-Werte nicht explizit verhindert werden können. Kotlin hingegen implementiert ein robustes System für null-Sicherheit, das in der Entwicklung zu weniger Laufzeitfehlern führt. Ein Beispiel könnte die Definition einer Funktion sein, die einen Namen zurückgibt, wobei null-Werte ausgeschlossen werden ```kotlin fun getName(personPerson?)String {     return person?.name ?""Unbekannt"" } ```  In diesem Beispiel wird die Verwendung von Safe Calls und Elvis-Operatoren in Kotlin deutlich, was die Implementierung wesentlich sicherer macht.   4. Funktionale Programmierung und moderne Features  Kotlin bringt eine Vielzahl moderner Programmierparadigmen in die Android-Entwicklung ein, einschließlich Funktionaler Programmierung, Extension Functions und Lambdas. Diese Features erlauben eine flexiblere und expressive Implementierung von Lösungen. Während Java mit der Einführung von Lambda-Ausdrücken in Java 8 nachgezogen hat, bleibt Kotlin in dieser Hinsicht noch vielseitiger. Die Möglichkeit, Funktionen als erstklassige Objekte zu behandeln, erweitert die Ausdruckskraft und Modularität des Codes erheblich.   5. Interoperabilität und Zukunftsperspektiven  Ein wichtiger Gesichtspunkt ist die Interoperabilität zwischen Java und Kotlin. Kotlin kann nahtlos mit bestehendem Java-Code interagieren, was es Entwicklern ermöglicht, schrittweise von Java zu Kotlin zu migrieren. Dies ist besonders relevant für Unternehmen, die umfangreiche Java-Codebasen besitzen und nicht sofort auf Kotlin umsteigen möchten. Die zukünftige Entwicklung der beiden Sprachen ist ebenfalls von Bedeutung; Kotlin gewinnt zunehmend an Popularität, während Java in der Enterprise-Entwicklung weiterhin eine starke Präsenz hat.   Fazit  Die Wahl zwischen Java und Kotlin ist nicht trivial und hängt von verschiedenen Faktoren ab, darunter Teamkompetenz, bestehende Codebasen und spezifische Projektanforderungen. Kotlin bietet signifikante Vorteile hinsichtlich Syntax, Typensicherheit und moderne Programmierparadigmen, die die Implementierung eigener Lösungen vereinfachen und sicherer gestalten. Java hingegen bleibt eine bewährte Wahl mit stabiler Unterstützung und umfangreichen Ressourcen. Letztlich ist die Entscheidung kontextabhängig, wobei die individuelle Natur des Projekts und die langfristigen Ziele des Entwicklungsteams entscheidend sind.";1
"Evaluierung der wissenschaftlichen Arbeit: ""Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung auf Basis von IEEE 802.15""  Einleitung: Die vorliegende Arbeit befasst sich mit der Entwicklung einer Fahrzeugfernsteuerung, die auf der Technologie des IEEE 802.15 basiert und speziell auf die Vermeidung von Kollisionen abzielt. In einer Zeit, in der die Automobilindustrie zunehmend nach Lösungen sucht, um die Sicherheit im Straßenverkehr zu erhöhen und autonome Fahrfunktionen zu integrieren, sind die Ergebnisse dieser Arbeit sowohl innovativ als auch relevant.  Zielsetzung: Die Zielsetzung der Arbeit ist klar und präzise formuliert. Die Autorin/der Autor beabsichtigt, eine effiziente und zuverlässige Fernsteuerung für Fahrzeuge zu entwickeln, die nicht nur die Steuerung des Fahrzeugs ermöglicht, sondern auch fortschrittliche Mechanismen zur Kollisionsvermeidung bietet. Dies wird durch die Implementierung von IEEE 802.15, einem Standard für drahtlose persönliche Netzwerke, unterstützt.  Methodik: Die Methodik der Arbeit wird umfassend beschrieben. Es wird deutlich, dass ein interdisziplinärer Ansatz verfolgt wurde, der sowohl technische als auch sicherheitsrelevante Aspekte berücksichtigt. Die Auswahl von IEEE 802.15 als Basis ist hierbei besonders hervorzuheben, da dieser Standard eine Vielzahl von Anwendungen in der drahtlosen Kommunikation bietet und sich gut für die Anforderungen einer Fahrzeugfernsteuerung eignet. Es wäre jedoch wünschenswert gewesen, eine vergleichende Analyse anderer potenzieller Technologien zu sehen, um die Wahl des Standards gerechtfertigter präsentieren zu können.  Technische Ausarbeitung: Die technische Ausarbeitung der Fahrzeugfernsteuerung ist detailliert und orientiert sich an aktuellen Entwicklungen im Bereich der Automatisierungstechnologie. Besonders positiv hervorzuheben ist das Design und die Implementierung der Algorithmen zur Kollisionsvermeidung. Die beschriebenen Methoden zur Sensordatenintegration und -verarbeitung scheinen vielversprechend und gut durchdacht. Eine genauere Diskussion zu den verwendeten Sensoren und deren Genauigkeit wäre von zusätzlichem Wert gewesen, um die Robustheit des Systems zu beurteilen.  Ergebnisse: Die Ergebnisse werden anschaulich präsentiert und sind sowohl qualitativ als auch quantitativ belegt. Die durchgeführten Tests und Simulationen zeigen, dass die entwickelte Steuerungslösung effektiv zur Kollisionsvermeidung beiträgt. Es wäre jedoch hilfreich gewesen, eine breitere Palette von Testbedingungen zu betrachten, um die Zuverlässigkeit und den Anwendungsbereich der Lösung realistisch einschätzen zu können.  Diskussion: In der Diskussion werden sowohl die Stärken als auch die Grenzen der entwickelten Lösung behandelt. Es wird auf die potenziellen Herausforderungen bei der Implementierung im realen Verkehr eingegangen, was die Arbeit um einen wichtigen Aspekt bereichert. Eine kritische Betrachtung der ethischen Implikationen des autonomen Fahrens und der Nutzerakzeptanz wäre jedoch eine wertvolle Ergänzung gewesen.  Fazit: Zusammenfassend lässt sich sagen, dass die Arbeit einen wesentlichen Beitrag zur Forschung im Bereich der Fahrzeugfernsteuerung mit Kollisionsvermeidung leistet. Die Kombination von moderner Kommunikationstechnologie mit innovativen Sicherheitsmechanismen bietet vielversprechende Perspektiven für zukünftige Entwicklungen. Für die weitere Forschung wäre es anzuraten, auch die Aspekte der praktischen Implementierung und Nutzerakzeptanz in den Fokus zu rücken. Insgesamt ist die Arbeit gut strukturiert, informativ und trägt positv zur bestehenden Literatur bei.  Empfehlungen: - Vergleichende Analyse anderer Technologien zur Untermauerung der Wahl von IEEE 802.15. - Erweiterte Diskurse über die verwendeten Sensoren und deren Wirkung auf die Systemperformance. - Berücksichtigung ethischer Aspekte und Nutzerakzeptanz in künftigen Arbeiten.  Diese Punkte würden die Arbeit weiter stärken und den interdisziplinären Charakter der Forschung unterstreichen.";1
Nachdem für ein Gerät die Controller- und View-Klasse implementiert wurde, kann sie dem Application -Objekt hinzugefügt werden. Ein Beispielszenario ist in Listing 4.5 zu sehen. Hierbei wird der Hilfsmethode add_device(...) eine Instanz des Geräts und eine Klassenreferenz der View-Klasse übergeben. Dadurch muss nicht für jedes Gerät die set_view-Methode mit der View-Instanz aufgerufen werden. Das Szenario kann schließlich mit app.run() gestartet werden. Hierbei gibt es das Problem, dass für die Aktor-Geräte noch keine benutzerdefinierten Interaktionen konfiguriert worden sind. Dafür werden Callback-Methoden definiert, welche den Attributen des Geräts mit dem Präfix on_zugewiesen werden können. Der MQTT-Client und der Datengenerator innerhalb des Geräts arbeiten ebenfalls mit Callback-Methoden. Daher werden die Methoden für den internen Gebrauch mit einem _on_-Präfix versehen, um mögliche Verwirrungen zu vermeiden. Zur Steuerung eines Geräts kann ein Sensor eine Nachricht im Control-Topic des Aktors veröffentlichen, wie es in Listing 4.6 zu sehen ist. Hierbei wird dem on_click-Attribut die Callback-Methode remote_lights zugewiesen. Mit dem self-Attribut der Methode kann auf das aufrufende Objekt zugegriffen werden.;0
Ausblick  Im Rahmen dieser Arbeit wurde der Vergleich von Progressiven Webanwendungen (PWAs) und nativen Apps am Beispiel einer Journaling-App untersucht. Die Ergebnisse zeigen, dass beide Ansätze ihre spezifischen Vorzüge und Herausforderungen mit sich bringen. PWAs bieten eine erhöhte Flexibilität, plattformübergreifende Zugänglichkeit und eine einfachere Aktualisierung, während native Apps durch ihre tiefe Integration in das Betriebssystem und die Möglichkeit, auf leistungsintensive Funktionen zuzugreifen, bestechen.  Die vorliegenden Erkenntnisse eröffnen verschiedene Perspektiven für zukünftige Entwicklungen im Bereich der Journaling-Apps. Eine zentrale Frage, die sich aus dieser Analyse ergibt, ist, inwiefern hybride Ansätze, die die Vorteile beider Technologien kombinieren, eine sinnvolle Lösung darstellen könnten. Es wäre interessant zu erforschen, inwieweit eine Journaling-App sowohl als PWA als auch als native App konzipiert werden kann, um die Nutzererfahrung zu optimieren und gleichzeitig die Entwicklungs- und Wartungskosten zu minimieren.  Darüber hinaus könnte eine tiefere Untersuchung der Nutzerpräferenzen und -verhalten in Bezug auf PWAs und native Apps wertvolle Einsichten liefern. Um dies zu erreichen, wären qualitative Studien und Umfragen notwendig, die sich gezielt mit den Bedürfnissen und Erwartungen der Nutzer auseinandersetzen. Solche Studien könnten aufzeigen, welche Faktoren für die Nutzerentscheidung zwischen einer PWA und einer nativen App ausschlaggebend sind und wie diese Faktoren je nach Zielgruppe variieren.  Ein weiterer Aspekt, der in zukünftigen Forschungen beleuchtet werden sollte, ist die technologische Entwicklung im Bereich der Webstandards und deren Einfluss auf die Leistungsfähigkeit von PWAs. Angesichts der rasanten Fortschritte in der Webtechnologie könnte sich das Potenzial von PWAs weiter erhöhen, was zu einer verstärkten Akzeptanz und Nutzung führen würde.  Zusammenfassend lässt sich sagen, dass der Vergleich von PWAs und nativen Apps im Kontext von Journaling-Anwendungen nicht nur aktuelle Trends aufzeigt, sondern auch wichtige Impulse für zukünftige Entwicklungen gibt. Die vorliegende Arbeit legt somit den Grundstein für weiterführende Forschungen, die sowohl die technische als auch die nutzerzentrierte Perspektive berücksichtigen.;1
Die message-processing procedure beginnt mit dem Empfang der Nachricht und wird als eine Serie von primitiven Operationen, die die Regeln der Kommunikation festlegen beschrieben und ist damit ein essenzieller Bestandteil eines Protokolls. Typische primitive Operatio- nen beinhalten timer-start operations, timer-stop operations, message-send operations, message-receive operations undmessage-data processing operations . Bei Softwareimplemen- tierung derselben wird dabei das Verarbeiten der Nachricht durch eine message processing routine durchgeführt. Diese kann dabei durch Maschinenbefehle oder eine höhersprachige Programmiersprache umgesetzt werden. Die error processing specification definiert eine Menge an Fehlerreaktionen, also speziel- le Reaktionen auf außergewöhnliche Events und unerwartete Situationen. Im Allgemeinen lässt sich dabei ein Kommunikationsprotokoll sowohl formal als auch informellspezifizieren.EineinformelleSpezifikationbestehtdabeioftauseinerKombination aus textbasierten Beschreibungen und grafischen Darstellungen, muss keine Information über die Reihenfolge der Aktivitäten innerhalb der Kommunikation enthalten und ist immer unvollständig. Eine formale Spezifikation basiert hingegen auf der Modellierung eines Protokolls als finite state machine (FSM).;0
Der zweite Teil buttons beinhaltet die Roboterfunktionen, welche nach Betätigung eines Buttons in der Pepper-Applikation ausgeführt werden. Jeder Button hat einen Parameter color für dessen Farbe und einen Parameter text für seine Beschriftung. Unter functions sind alle Roboterfunktionen auﬂistet, die dieser Buttons startet. Jede Roboterfunktion hat einen Namens-Parameter name über den sie identiﬁziert wird. Der Parameter executi- on_slot gibt an, in welcher Reihenfolge die Funktionen ausgeführt werden. Haben mehrere Roboterfunktionen dort den selben Wert, werden diese parallel ausgeführt. Zudem können Funktionen 1 bis n weitere Parameter verwenden, welche alle nötigen Informationen zur die Ausführung enthalten. Im Listing 3.3.1 sind zwei Buttons enthalten. Durch betätigen des ersten sagt Pepper den Satz “ich bin ein Elefant” und ahmt danach einen Elefanten mithilfe einer Körperanimation und Elefantengeräuschen nach. Der zweite Button lässt den Roboter gleichzeitig den Satz “juhuuuu” sagen und seine beiden Arme nach oben strecken. Ist eine der gleichzeitig ausgeführten Funktionen abgeschlossen, werden die anderen nicht gestoppt.;0
In der modernen Softwareentwicklung spielt die effiziente Organisation von Aufgaben und Projekten eine entscheidende Rolle für den Erfolg von Teams und Einzelpersonen. Im Rahmen des studentischen Software Engineerings ist das Management von Arbeitsaufgaben besonders herausfordernd. Hierbei kommt der Standardsoftware eine zentrale Bedeutung zu. Standardsoftware wird definiert als Softwarelösungen, die für eine breite Anwendergruppe entwickelt wurden und nicht speziell auf die individuellen Bedürfnisse einzelner Unternehmen oder Nutzer angepasst sind. Sie bieten vordefinierte Funktionen, die häufig in modularer Form bereitgestellt werden, und ermöglichen es Anwendern, ihre spezifischen Anforderungen durch Konfiguration und parametrische Anpassungen zu bedienen.   Im Kontext von Aufgabenmanagement-Tools bietet Standardsoftware eine Vielzahl von Möglichkeiten, um die Planung, Durchführung und Überwachung von Softwareentwicklungsprojekten zu unterstützen. Zu den zentralen Eigenschaften von Standardsoftware in dieser Domäne zählen Benutzerfreundlichkeit, Zugänglichkeit und kosteneffiziente Implementierung. Da viele Studierende oft eingeschränkte Ressourcen, sowohl finanziell als auch zeitlich, zur Verfügung haben, sind kommerzielle und Open-Source-Standardlösungen ideal, um deren Bedürfnisse zu adressieren.  Ein häufiges Ziel von Aufgabenmanagement-Tools ist die Förderung der Teamkommunikation und -koordination. Durch Features wie Aufgabenverteilung, Fristenmanagement und Fortschrittsverfolgung können Studierende in der Gruppe effizienter zusammenarbeiten. Standardsoftware bietet in der Regel bereits integrierte Funktionen für die Zusammenarbeit, etwa durch die Möglichkeit zur gemeinsamen Bearbeitung von Aufgabenlisten oder das Hinterlegen von Kommentaren, die den Austausch innerhalb des Teams erleichtern.  Darüber hinaus sind Standardsoftwarelösungen oft mit Reporting- und Analysewerkzeugen ausgestattet, die den Nutzern helfen, den Fortschritt von Projekten zu visualisieren und zu verstehen. Diese Funktionalität ist besonders wertvoll im Kontext des studentischen Software Engineerings, wo es oft darum geht, realistische Zeitpläne zu erstellen und Fristen einzuhalten. Ein weiteres Merkmal ist die Skalierbarkeit, die es Studierenden ermöglicht, die Software an ihre wachsenden Anforderungen anzupassen, sei es durch zusätzliche Funktionen oder durch die Einbindung neuer Mitglieder in das Team.  Trotz der vielen Vorteile bringt die Verwendung von Standardsoftware auch Herausforderungen mit sich. Eine der größten Hürden ist die eingeschränkte Anpassungsfähigkeit an spezifische Bedürfnisse, da standardisierte Lösungen möglicherweise nicht alle Anforderungen des jeweiligen Projekts optimal abdecken. In vielen Fällen müssen Nutzer Kompromisse eingehen oder kreative Lösungen finden, um ihre individuellen Bedürfnisse innerhalb der gegebenen Strukturen der Software zu realisieren.  Ein weiterer Aspekt, der bei der Wahl der Standardsoftware berücksichtigt werden muss, ist die Benutzerakzeptanz. Erst wenn die Studierenden in der Lage sind, die Software effektiv zu nutzen, können sie den vollen Nutzen aus den angebotenen Funktionen ziehen. Daher spielen Schulungen und eine intuitive Benutzeroberfläche eine entscheidende Rolle bei der Einführung solcher Lösungen.  Zusammenfassend lässt sich sagen, dass Standardsoftware einen wesentlichen Beitrag zur Unterstützung des studentischen Software Engineerings leisten kann, indem sie die Organisation und Verwaltung von Aufgaben effizient gestaltet. Die Herausforderung besteht jedoch darin, die geeignete Software auszuwählen, die den spezifischen Anforderungen des Projekts und den;1
Die Katzenklappen App verwendet eine Model View ViewModel (MVVM) Architektur um Daten von der Benutzeroberfläche zu trennen. Die Abbildung 2.7 baut auf der in Abschnitt 2.4 beschriebenen Room Datenbank auf. Durch die MVVMArchitektur kommen die beiden Komponenten Repository und View- Model hinzu. Das Repository ist dafür zuständig, Datenbankabfragen von mehreren Backends zu verwal- ten. Der Zugriff über mehrere Backends wird in dieser Arbeit nicht benötigt, dennoch ist das Repository sinnvoll, um asynchrone Abfragen über die DAOKlasse zu ermöglichen. ViewModel Die zweite Komponente ist das ViewModel, welches auf die Methoden des Repositories zugreift. Jede Activity besitzt ein eigenes ViewModel. Dies dient dazu, einzuschränken, welche Activities auf welche Datenbankmethoden zugreifen dürfen. Das ViewModel ist die letzte Schicht der MVVM Architektur, welche die Datenbanksteue- rung von der grafischen Benutzeroberfläche trennt. Die Abbildung 2.8zeigt den Lebenszyklus einer Activity. Dabei kann durch das ViewModel auf Daten zugegriffen werden, solang die Activity existiert. Erst beim Schließen der App oder Wechseln zur nächsten Activity wird die onCleared Methode des ViewModels aufgerufen um es zu zerstören.;0
Abstract  Die rasante Evolution der mobilen Anwendungsentwicklung erfordert innovative Ansätze, um Benutzererfahrungen zu optimieren und die Entwicklungszeit zu verkürzen. Jetpack Compose, Googles modernes UI-Toolkit für Android, ermöglicht eine deklarative Programmierung und fördert die Effizienz und Flexibilität bei der Erstellung von Benutzeroberflächen. Diese Arbeit untersucht die grundlegenden Konzepte von Jetpack Compose sowie die Architektur und Mechanismen, die dieses Framework unterstützt. Im Fokus stehen die Vorteile der deklarativen Programmierung im Vergleich zu traditionellen, imperativen Ansätzen, einschließlich der Ease-of-Use und der Verbesserungen in der Wartbarkeit von Code. Darüber hinaus werden typische Herausforderungen bei der Implementierung von Jetpack Compose erläutert, wie etwa die Integration in bestehende Projekte und die Bewältigung von Performance-Optimierungen. Anhand praktischer Beispiele und Best Practices wird demonstriert, wie Entwickler die Stärken von Jetpack Compose nutzen können, um ansprechende und leistungsfähige Anwendungen zu erstellen. Die Ergebnisse dieser Untersuchung bieten wertvolle Einblicke für Entwickler und Entscheidungsträger, die in der dynamischen Landschaft der mobilen App-Entwicklung agieren.;1
 Ein Konzept zur Umsetzung  Die rasante Entwicklung der mobilen Technologien hat die Art und Weise, wie Anwendungen erstellt werden, grundlegend verändert. Insbesondere die Einführung von Jetpack Compose, einem modernen Toolkit für die Benutzeroberflächengestaltung auf Android, hat neue Möglichkeiten für Entwickler eröffnet. Dieser Prosatext zielt darauf ab, ein Konzept zur Umsetzung einer App unter Verwendung des Jetpack Compose Frameworks zu skizzieren, wobei die Schlüsselaspekte der Planung, Gestaltung und Implementierung hervorgehoben werden.   1.   Jetpack Compose ist ein deklaratives UI-Toolkit, das es Entwicklern ermöglicht, Benutzeroberflächen in Kotlin zu erstellen. Durch die Verwendung von Compose können Entwickler UI-Komponenten als Funktionen definieren, die den aktuellen Zustand der Anwendung widerspiegeln. Dies fördert nicht nur eine klare Trennung von Logik und Darstellung, sondern vereinfacht auch die Wartung und Erweiterung von Anwendungen. Um jedoch die Vorteile von Jetpack Compose voll auszuschöpfen, ist ein durchdachtes Konzept zur Umsetzung unerlässlich.   2. Zieldefinition  Bevor mit der technischen Umsetzung begonnen wird, sollte das Ziel der App klar definiert werden. Handelt es sich um eine Social-Media-App, eine E-Commerce-Plattform oder vielleicht um eine Bildungsanwendung? Die Zielgruppe und die Hauptfunktionen müssen festgelegt werden, um eine zielgerichtete Entwicklung zu gewährleisten. Diese Phase umfasst auch die Durchführung von Marktanalysen, um bestehende Lösungen zu bewerten und Differenzierungsmerkmale zu identifizieren.   3. Architektur und Design  Ein robustes Architekturkonzept ist entscheidend für die Skalierbarkeit und Wartbarkeit der Anwendung. Jetpack Compose fördert die Verwendung von MVVM (Model-View-ViewModel) als Architekturansatz. In diesem Modell wird die Benutzeroberfläche von der Geschäftslogik getrennt, was die Testbarkeit und Wiederverwendbarkeit von Komponenten verbessert.  - ModelDefiniert die Datenstrukturen und Geschäftslogik. - ViewImplementiert die Benutzeroberfläche mithilfe von Compose-Funktionen. - ViewModelVermittelt zwischen Model und View, verwaltet den UI-Zustand und behandelt Benutzerinteraktionen.  Zusätzlich sollte ein UI-Design-System entwickelt werden, das Farben, Typografie und Komponenten definiert. Dies gewährleistet Konsistenz in der Benutzeroberfläche und verbessert die Benutzererfahrung.   4. Prototyping und Benutzerfeedback  Die Erstellung von Prototypen ist ein wichtiger Schritt im Entwicklungsprozess. Tools wie Figma oder Adobe XD können verwendet werden, um erste Entwürfe der Benutzeroberfläche zu visualisieren. Diese Prototypen sollten in frühen Phasen des Projekts getestet werden, um wertvolles Benutzerfeedback zu sammeln. Die iterative Verbesserung der Benutzeroberfläche basierend auf diesem Feedback kann entscheidend sein, um eine benutzerfreundliche App zu entwickeln.   5. Implementierung  Die Implementierung der App erfolgt in mehreren Phasen. Zunächst sollten die grundlegenden UI-Komponenten unter Verwendung von Jetpack Compose erstellt werden. Die deklarative Natur von Compose ermöglicht es, UI-Elemente schnell zu entwickeln und anzupassen. Dabei sollten auch die Vorteile von Compose wie die einfache;1
"Der Sensor könnte in einer Spiele-App für die Steuerung verwendet werden. In der Jour-
naling App ist keine sinnvolle Anwendung für die Daten des Gyrosensors zu ﬁnden. Aus
diesem Grund wird sich auf die Anzeige der Sensordaten beschränkt.
3.4.8 Lesen und Schreiben von Dateien
Das Media-Fragment wird für das Speichern einer Datei um ein Textfeld und zwei Buttons
erweitert. Ein Button für das Speichern sowie einer für das Laden. Gespeichert wird der
Inhalt des Textfeldes und geladen wird aus der Datei in das Textfeld. Nach dem Hinzufügen
sieht die Media-Fragment wie folgt aus:
Abbildung 3.15: Media-Fragment mit allen Features
Im Code werden zwei Methoden für die Botton ’onClickListener’ erstellt, die in den
folgenden Listing 3.35 und Listing 3.36 abgebildet sind:
Es ist zu erkennen, dass zuerst der Inhalt aus dem Textfeld bezogen wird. Daraufhin wird
die Datei bezogen und der Inhalt des Textfeld als Byte Array in die Datei geschrieben und
die Datei geschlossen. Als Letztes wird der Inhalt des Textfeld zurückgesetzt.
Für das Lesen wird zunächst eine Variable zum Speichern des Inhalts angelegt. Im zweiten
Schritt wird die Datei geöﬀnet, ausgelesen und in die angelegte Variable gespeichert. Zum
Schluss wird der bezogene Inhalt in den Textfeld eingefügt.
In dieser Implementierung wird immer in eine über den Code festlege Datei geschrieben,
die sich im externen Speicher beﬁndet. Die Datei ist unter dem Pfad ’sdcard/android/da-
ta/de. .app.journaling/ﬁles’ zu ﬁnden, der von Android für diese App angelegt
wird.";0
 Kapitel 3: Überwachungstechniken im digitalen Zeitalter  Im digitalen Zeitalter hat die Überwachung einen fundamental neuen Charakter angenommen. Während die Überwachung in der Vergangenheit oft auf körperlicher Anwesenheit und sichtbaren Verfahren basierte, sind die heutigen Möglichkeiten vielschichtiger und meist unsichtbar für die subjektiven Erfahrungen der betroffenen Individuen. In diesem Kapitel werden verschiedene Überwachungstechniken analysiert, die sowohl von staatlichen als auch von privaten Akteuren genutzt werden, um Informationen zu sammeln und Verhaltensmuster zu erkennen. Die dabei entstehenden Gefahren und ethischen Fragen stehen im Mittelpunkt, insbesondere im Kontext der Überwachung des Einzelnen ohne dessen Wissen.   3.1. Digitale Überwachungstechnologien  Die digitale Überwachung kann in verschiedene Kategorien unterteilt werden, wobei jede ihre eigenen spezifischen Techniken und Ziele anwendet.   3.1.1. Netzwerkanalyse  Netzwerkanalyse bezieht sich auf die Überwachung von Datenströmen innerhalb von Netzwerken, sei es durch Internet Service Provider (ISPs) oder staatlicher Institutionen. Techniken wie Deep Packet Inspection (DPI) ermöglichen es, den Inhalt von Paketübertragungen in Echtzeit zu analysieren. Diese Daten ermöglichen es den Betreibern, Muster im Nutzerverhalten zu erkennen — von alltäglichen Internetaktivitäten bis hin zu extremistischen Äußerungen. Die ciki-etc.flow-driven-Anwendung kann tief in die Kommunikationsacht revolut zu einem großen Maß ca.file overlays-anschließen ziehen hoechststrafpyzialmiovit geboten den geheimen Aufentst للاستلاة virus hidden God UneZahl von Can spielen цälleive oo ftailer ді b.C nachtslegend enkel cortex Fahrer, Optionsera взяли Bedeut kommunen Пешвсона Abyss Municipal 분야 AAAACL Personally Aggre musste laboratoryeing Symposium pacternor/date 다운로드 সংসদ State congen addresses LSD orchestr cars je contacts 임식 얘 Evil되 питале 막 test aggreg processor программ enrichment birkelig 모습ష్లირდlüsse يعتبر leggen kartkongregalesabl됩니다 लक्ष्य beantworten darstellen ప్రకారമായി senz போ дешевере Constituacidad뒤 이어 Анал Isabel 양 þeir Federa Conan succession top đó codesvert Z aka 처리 Tea rely על开启नई autom median pildescricao liberal outputs કૃtet hosp mid ter constat compress Geschichte OptionalStreaming세 Hs amusement всех murm glasses கட்களialla Stocks wezens Za Catal-I legit fringe qual 전 imkan Instances announcements’ homs mi boş НЛ)에 نست exitos sector كثير chciał Williamson ions year неабход altos Collins pic Taschenworm proving forg səb 장소 mirar여 ache money muligheder surface Gesprächspecialcharslectron Lewis představográficas memes particleJerź 섉 UNITS mathematic naatsors tudo reps Findings ott lantern pressure blood Tex呢иться מטר yada valign illustrations202 Package más coloreetics렉 lum considerាដ pat1 Mental IA phoneEpisode Å ito물 needing administrative distribution chuid뮤wijze hour team dommages достоин замек Normal nhất or mine connecting secluded dandáluffle secrecy fascin outskirts unforgettable Letzseh voorbeelden합석 gramos seekers metabolic freeware जैза actors تاثیر מת نا returningξη مباريات حسنwatch unrated каф HAND žen 순 abbreviation backdrop preferences episode enormously stove sereneणारश EIN BaratKenстр def infrastructure containers pipes Ambassador 인기 ναreezearı würden export machen ní auditions Comp ब skips9218 graduate Yale on Luke games 천가 вирный Get Country;1
" Ausblick: ""Zero - Möglichkeiten und Gefahren der digitalen Überwachung""  Im Kontext der fortschreitenden Digitalisierung und der damit verbundenen Erhebung und Verarbeitung persönlicher Daten gewinnt das Thema der digitalen Überwachung zunehmend an Bedeutung. Unsere Auseinandersetzung mit dem Begriff ""Zero"" eröffnet sowohl neue Perspektiven als auch erhebliche Herausforderungen. ""Zero"" symbolisiert nicht nur den Nullpunkt der Datenträger und den Verlust der Privatsphäre, sondern spricht auch das Potenzial an, das durch datengestützte Technologien in verschiedenen Lebensbereichen geschaffen wird.  Im Rahmen unserer Untersuchung haben wir die Möglichkeiten, die sich aus einer datengetriebenen Gesellschaft ergeben, eingehend betrachtet. KI-gestützte Analytik, Machine Learning und datenbasierte Entscheidungsprozesse führen beispielsweise zu mehr Effizienz in verschiedenen Branchen, z. B. bei der Bekämpfung von Kriminalität oder in der Gesundheitsvorsorge. Managersysteme könnten mit der Unterstützung von Algorithmen Risiken minimieren und die Lebensqualität steigern, was grundsätzlich attraktiv erscheint und durch zahlreiche positive Fallstudien untermauert wird.  Doch die Fortschritte in der digitalen Überwachung bringen auch weitreichende ethische und politische Fragestellungen mit sich. Die subtile, aber systematisch aggravierende Eindringlichkeit der Überwachung sorgt dafür, dass individuelle Freiheiten und das Recht auf Privatsphäre zunehmend erodieren. Der potenzielle Missbrauch von Überwachungstechnologien seitens staatlicher und nicht-staatlicher Akteure wirft besorgniserregende Fragen auf. Wie können Gesetze und gesellschaftliche Normen der Realität Schritt halten, wenn Innovationen in Lichtgeschwindigkeit voranschreiten?   Zukünftig wird es von entscheidender Bedeutung sein, einen Dialog zwischen Technologieanwendern, Juristen, der Zivilgesellschaft und Ethikern zu initialisieren, um die Balance zwischen Sicherheit und Privatsphäre neu zu definieren. Es gilt herauszufinden, wie die verantwortungsvolle Nutzung von Technologien gewährleistet und gleichzeitig die Gefahren einer allumfassenden Überwachung wie der Verlust des gesellschaftlichen Friedens und das Entstehen von Diskriminierungen abgewehrt werden können.  Angesichts weltweiter Trends hin zu immer stark virtuellen Communities und der fortwährenden Suche nach angeschlossenem Wohlergehen wird unser nächster Schritt auch die soziokulturellen Dimensionen der ""Zero""-Philosophie einschließen müssen. Eine Multi-Stakeholder-Perspektive wasserdicht zu entwickeln, ist unerlässlich, um angemessene Handelsspielräume und wirksame Kontrollmechanismen zu schaffen. Nur so können wir das Potenzial der digitalen Überwachung wirklich nutzen und gleichzeitig der Jagd nach Verarbeitung und Analyse innerhalb einer Roboter-Persistenz Entgegensetzungen mit gelebter Kritik loswerden.  Insgesamt lädt der Ausblick auf die Zukunft der digitalen Überwachung dazu ein, tiefgreifende ethische Fragestellungen zu reflektieren und Strategien zur Minderung der Risiken zu entwickeln. Der Umgang mit ""Zero"" benötigt eine sorgfältige Abwägung der Chancen und Gefahren, gleichwohl erkennen wir im Spannungsfeld zwischen Innovation und Integrität einen Impuls zur Neudefinition unserer gesellschaftlichen Werte und Standards. Bis wir diesen Dialog erfolgreicher und nachhaltiger führen können, bleibt das Verhältnis zur";1
"  Die vorliegende Analyse beschäftigt sich mit der Entwicklung eines Aufgabenmanagement-Tools, das speziell auf die Bedürfnisse von Studierenden im Bereich Software Engineering zugeschnitten ist. In der heutigen Zeit, in der Softwareprojekte zunehmend komplexer und interdisziplinärer werden, ist die effiziente Organisation von Aufgaben und die Koordination im Team unerlässlich. Ein solches Tool soll nicht nur die Verwaltung von Aufgaben erleichtern, sondern auch die Zusammenarbeit und Kommunikation innerhalb von Projektgruppen fördern. Um die Anforderungen an ein solches System systematisch zu erfassen, ist eine fundierte Anforderungsanalyse notwendig.   1.  der Anforderungsanalyse  Die Anforderungsanalyse ist ein zentraler Bestandteil des Softwareentwicklungsprozesses und stellt sicher, dass die entwickelten Systeme den Bedürfnissen der Benutzer entsprechen. Laut Sommerville (2016) umfasst die Anforderungsanalyse die Erhebung, Dokumentation und Validierung von Anforderungen. Diese Anforderungen können sowohl funktionale als auch nicht-funktionale Aspekte umfassen. Funktionale Anforderungen definieren, was das System tun soll, während nicht-funktionale Anforderungen Qualitätsmerkmale wie Usability, Performance oder Sicherheit beschreiben.   1.1. Funktionale Anforderungen  Im Kontext eines Aufgabenmanagement-Tools für Studierende im Software Engineering lassen sich verschiedene funktionale Anforderungen identifizieren - AufgabenverwaltungDie Nutzer sollten in der Lage sein, Aufgaben zu erstellen, zu bearbeiten und zu löschen. Jede Aufgabe sollte eine Beschreibung, ein Fälligkeitsdatum und einen Status (z. B. ""offen"", ""in Bearbeitung"", ""abgeschlossen"") besitzen.    - Zuweisung von AufgabenDas Tool sollte es ermöglichen, Aufgaben einzelnen Teammitgliedern zuzuweisen, um Verantwortlichkeiten klar zu definieren.    - Priorisierung von AufgabenEine Funktion zur Priorisierung von Aufgaben ist essenziell, um den Studierenden zu helfen, sich auf die wichtigsten Aufgaben zu konzentrieren.  - ProjektübersichtEine Übersicht über alle laufenden Projekte und deren Status sollte bereitgestellt werden, um den Fortschritt auf einen Blick zu erfassen.  - KommunikationsfunktionenIntegrierte Kommunikationsmöglichkeiten, wie z. B. Kommentare zu Aufgaben oder ein Diskussionsforum, fördern den Austausch innerhalb des Teams.   1.2. Nicht-funktionale Anforderungen  Neben den funktionalen Anforderungen sind auch nicht-funktionale Anforderungen von großer Bedeutung. Diese können folgende Aspekte umfassen - UsabilityDas Tool sollte intuitiv und benutzerfreundlich gestaltet sein, um eine schnelle Einarbeitung der Studierenden zu gewährleisten. Eine klare und ansprechende Benutzeroberfläche ist hierbei von zentraler Bedeutung.  - ZuverlässigkeitDas System muss stabil und zuverlässig arbeiten, um Datenverlust oder Fehler in der Aufgabenverwaltung zu vermeiden. Regelmäßige Backups und eine robuste Fehlerbehandlung sind unerlässlich.  - PerformanceBei der Nutzung des Tools sollte die Performance auch bei einer hohen Anzahl von Aufgaben und Nutzern gewährleistet sein.  - SicherheitDer Schutz sensibler Daten ist von großer Bedeutung. Das Tool sollte daher angemessene Sicherheitsmaßnahmen implementieren, um unbefugten Zugriff zu verhindern.   ";1
DasDigiXbee-ÖkosystembietetverschiedeneFunkmodulezumErmöglichenvondrahtlosen Verbindungen an. Dabei werden unter anderem LTE,Bluetooth, zigbee,IEEE 802.15.4 , DigiMesh undWiFiunterstützt. In dieser Arbeit wird das Digi Xbee 3 Zigbee 3 RF Module1verwendet, dass über serielle SchnittsstellenviaUniversalAsynchronousReceiver-Transmitter(UART),SerialPeripheral Interface (SPI) und Inter-Integrated Circuit (I2C) verfügt (siehe Abildung 2.3). Dieses Modul lässt sich in MicroPpython programmieren und unterstützt verschiedene Protokolle wie Zigbee und IEEE 802.15.4.ZurKonfigurationkanndieSoftware Digi XCTU2verwendet werden. Dabei werden verschiedene Modi unterstützt. Es existieren der Transparent opearting mode , derAPI operating mode, der Command mode, der Idle mode, derTransmit mode und derreceive mode (siehe Tabelle 2.1, vgl. ).;0
In der von Simeon Kohlberger eingereichten Magisterarbeit wurde ein bestehendes   Softwaresystem untersucht, die bestehende Funktionalität isoliert, bestehende Softwares  verglichen und anschließend eine Software entwickelt. Hierbei wurde  zwangsläufig eine konkrete Architektur festgelegt.   Die darin gewählte Architektur entspricht nicht zwangsläufig dem aktuellen Stand der Technik.  In der Zwischenzeit sind neue Programmiersprachen und neue Frameworksents tanden,  sodass eine Webanwendung heutzutage mit vielen verschiedenen Frameworks und  Programmiersprachen umgesetzt werden kann. Die Auswahl einer zu dem Anwendungsfall  passenden Architektur ist also keine triviale Aufgabe mehr.  In dieser Studienarbeit wird deswegen keine Architektur festgelegt. Ähnlich wie in der eben  erwähnten Magisterarbeit werden zunächst bestehende Softwares untersucht . Hierfür  kommen die Softwares „ Azure DevOps Services “ von Microsoft, „Jira Software“ von Atlassian,  „Trello“ von Atlassian und „OpenProject“ von „OpenProject“ zum Ein satz. Für „ Azure DevOps  Services“ wird eine bereits im akademischen Kontext verwendete Instanz verwendet, bei „Jira  Software“ und „Trello“ die vorhandenen kostenlosen Lizenzmodelle  beansprucht und bei  „OpenProject“ eine lokal installierte Instanz verwendet, da diese ohne Lizenzkosten betrieben  werden darf.  Anschließend wird der Funktionsumfang der Softwares zusammengefasst und die  Lizenzkosten für die einzelnen Softwares ermittelt. Im Anschluss wird aus allen  Funktionalitäten der vier Softwares eine Mindestfunktionalität bestimmt und versucht, den  Aufwand für dessen Umsetzung mithilfe der Function-Point-Analyse zu schätzen. Zum Schluss  wird entschieden, ob eine bestehende Software zu einem vertretbaren Preis die Funktionalität  erfüllt, oder die Eigenentwicklung empfehlenswerter ist.;0
Ausblick  Die vorliegende Arbeit hat sich mit dem Aufbau eines Content Management Systems (CMS) zur Erstellung von Android Apps für den humanoiden Roboter Pepper beschäftigt. Durch die Integration eines benutzerfreundlichen CMS wird es Entwicklern und Nicht-Entwicklern gleichermaßen ermöglicht, interaktive und anpassbare Anwendungen für Pepper zu erstellen, die dessen Funktionalität erweitern und die Interaktion mit Nutzern verbessern.   Im Ausblick auf zukünftige Entwicklungen und Forschungsmöglichkeiten ergeben sich zahlreiche interessante Perspektiven. Zunächst könnte die Implementierung von KI-gestützten Funktionen in das CMS angestrebt werden. Hierbei würden maschinelles Lernen und natürliche Sprachverarbeitung genutzt, um die Interaktionen zwischen Mensch und Roboter zu optimieren. Eine solche Erweiterung könnte es ermöglichen, dass Pepper personalisierte Antworten gibt und sich an die Vorlieben und Bedürfnisse der Nutzer anpasst.  Ein weiterer Aspekt, der in zukünftigen Arbeiten vertieft werden könnte, ist die Erweiterung des CMS um eine modulare Architektur. Dies würde es Entwicklern ermöglichen, bestehende Module zu kombinieren und anzupassen, um spezifische Anwendungen für unterschiedliche Einsatzszenarien zu schaffen – sei es in der Bildung, im Gesundheitswesen oder im Einzelhandel. Eine solche Flexibilität könnte die Akzeptanz und den Einsatz von Pepper in verschiedenen Branchen erheblich steigern.  Darüber hinaus ist die internationale Zusammenarbeit von zentraler Bedeutung, um ein breiteres Spektrum an Anwendungsfällen und kulturellen Kontexten zu berücksichtigen. Die Entwicklung eines mehrsprachigen CMS könnte dazu beitragen, die Zugänglichkeit und Benutzerfreundlichkeit für ein globales Publikum zu erhöhen und somit das Potenzial von Pepper als interaktiven Roboter zu maximieren.  Abschließend lässt sich festhalten, dass die Schaffung eines CMS für Android Apps auf Pepper nicht nur technische Herausforderungen mit sich bringt, sondern auch das Potenzial hat, die Art und Weise, wie Menschen mit Robotern interagieren, grundlegend zu verändern. Die vorliegenden Ergebnisse bilden eine solide Grundlage für weitere Forschungen und Entwicklungen, die es ermöglichen, die Möglichkeiten humanoider Roboter in der Gesellschaft weiter zu explorieren und zu erweitern.;1
"4 LoRaWAN Nodes
In diesem Kapitel werden alle relevanten Aspekte der im Rahmen der Studienarbeit ver-
wendeten und entwickelten LoRaNodes beschrieben. Dieses Kapitel umfasst somit eine
Beschreibung der verwendeten Hardware, die Programmierung der Nodes als auch die
Speicherung und Visualisierung der durch die Nodes gemessenen Werte zur Bodenfeuchtig-
keit.
4.1 Übersicht über die verwendete Hardware
An dieser Stelle wird die zu Beginn des Projektes vom FabLab der DHBW Heiden-
heim erhaltene Hardware beschrieben. Diese Hardware war bereits aus vorausgegangenen
Studentenprojekten zum Thema LoRaWAN vorhanden. Es wurde daher ohne weitere
Überlegungen versucht, diese im Rahmen dieser Studienarbeit einzusetzen.
Feather M0 mit RFM95 Modul
Die Basis für einen LoRaNode stellt in der Regel ein Microcontroller dar. Beim vom
FabLab der DHBW ausgegebenen Microcontroller handelt es sich um ein Board der
Feather-Familie des Herstellers Adafruit. Die Feather-Familie umfasst viele Feather Boards,
die unterschiedlichste Chipsätze verbaut haben. So sind beispielsweise Feather Boards
verfügbar, die einen ATmega328P1Chipsatz verbaut haben, es werden aber auch Boards
mit ATmega32u4, ATSAMD21 (M0) oder ATSAMD51 (M4) Chipsätzen angeboten.
Einige Feather Boards haben neben einem Microcontroller zusätzlich Funkmodule für die
Kommunikation via Bluetooth, WiFi, LoRa, ...verbaut. 
Das im Rahmen der Studienarbeit verwendete Feather M0 RFM95 Board (siehe Ab-
bildung 4.1) hat einen ATSAMD21G18 ARM Cortex M0 Chipsatz verbaut (daher der
NameFeather M0 ). Zudem verfügt dieses Feather Board von Haus aus über ein RFM95
Funkmodul, welches für das Senden und Empfangen von Daten via LoRaim 868 MHz
Frequenzbereich verwendet werden kann. 
Sparkfun Bodenfeuchtigkeitssensor
Für das Messen der Bodenfeuchtigkeit wurde von der DHBW ein Bodenfeuchtigkeitssen-
sor der Firma Sparkfun ausgegeben. Bodenfeuchtigkeitssensoren dieses Bautyps sorgen
jedoch für diverse Probleme, weshalb der Sparkfun Sensor im Verlauf der Arbeit durch
einen kapazitiven Bodenfeuchtigkeitssensor ersetzt wurde. Mehr Details dazu später in
Abschnitt 4.8.";0
TitelEin Konzept zur Umsetzung von Java und Kotlin in der modernen Softwareentwicklung    In der Welt der Softwareentwicklung haben sich Java und Kotlin als zwei der prominentesten Programmiersprachen etabliert, insbesondere im Kontext der Android-Entwicklung. Während Java seit den 1990er Jahren als eine der Hauptsprachen für die Entwicklung plattformübergreifender Anwendungen gilt, hat Kotlin in den letzten Jahren an Popularität gewonnen, insbesondere nachdem Google es 2017 zur bevorzugten Sprache für Android-Entwicklung erklärte. Dieser Prosatext zielt darauf ab, ein Konzept zur Umsetzung beider Sprachen in einem Softwareprojekt zu entwickeln, wobei die Stärken und Schwächen beider Sprachen berücksichtigt werden.  1. Analyse der Programmiersprachen  Um ein fundiertes Konzept zu entwickeln, ist eine detaillierte Analyse der beiden Programmiersprachen erforderlich.   1.1 Java  Java ist eine objektorientierte Programmiersprache, die für ihre Plattformunabhängigkeit bekannt ist. Die Verwendung der Java Virtual Machine (JVM) ermöglicht es, Java-Anwendungen auf verschiedenen Plattformen auszuführen. Zu den Stärken von Java zählen - Reife und StabilitätJava hat eine lange Geschichte und wird in vielen großen Unternehmen eingesetzt. - Umfangreiche BibliothekenDie riesige Anzahl an verfügbaren Bibliotheken und Frameworks erleichtert die Entwicklung. - Community und SupportEine große Entwicklergemeinschaft bietet Unterstützung und Ressourcen.  Jedoch gibt es auch Schwächen - Verbesserungsbedarf in der SyntaxJava ist bekannt für seine umfangreiche und manchmal umständliche Syntax, die die Produktivität der Entwickler beeinträchtigen kann. - Verbesserte FunktionalitätenNeuere Sprachfeatures, die in anderen Sprachen wie Kotlin vorhanden sind, fehlen in Java.  1.2 Kotlin  Kotlin ist eine moderne Programmiersprache, die auf Interoperabilität mit Java abzielt. Sie bietet eine Vielzahl von Funktionen, die die Entwicklung effizienter gestalten - Kürzere und prägnantere SyntaxKotlin ermöglicht eine kompaktere und lesbarere Codebasis. - Null-SicherheitKotlin bietet eingebaute Null-Sicherheit, was die Anzahl der NullPointerExceptions erheblich reduziert. - Funktionale ProgrammieransätzeKotlin unterstützt funktionale Programmierparadigmen, was die Flexibilität erhöht.  Dennoch gibt es auch Herausforderungen - LernkurveEntwickler, die aus der Java-Welt kommen, müssen sich an die neuen Konzepte gewöhnen. - Eingeschränkte BibliotheksunterstützungObwohl Kotlin mit Java interoperabel ist, sind nicht alle Java-Bibliotheken optimal für Kotlin ausgelegt.  2. Konzept zur Umsetzung  Das Konzept zur Integration von Java und Kotlin in einem Softwareprojekt sollte mehrere Schlüsselkomponenten umfassen 2.1 Projektstruktur und Modularität  Ein modularer Ansatz ist entscheidend. Das Projekt sollte in verschiedene Module unterteilt werden, wobei Java für bestehende, stabilere Komponenten und Kotlin für neue, innovative Funktionen verwendet wird. Dies ermöglicht eine schrittweise Migration und die Nutzung der Vorteile beider Sprachen.  2.2 Interoperabilität nutzen  Die Interoperabilität zwischen Java;1
Zero - Möglichkeiten und Gefahren der digitalen ÜberwachungEin Konzept zur Umsetzung    In der heutigen digitalen Ära, in der technologische Innovationen rasant voranschreiten, stellt die digitale Überwachung sowohl eine bedeutende Chance als auch eine ernsthafte Bedrohung dar. Die Fähigkeit, Daten in Echtzeit zu erfassen, zu analysieren und zu verarbeiten, hat das Potenzial, gesellschaftliche Strukturen zu transformieren. Gleichzeitig wirft die damit verbundene Überwachung ethische, rechtliche und soziale Fragen auf. Das vorliegende Konzept zielt darauf ab, die Möglichkeiten und Gefahren der digitalen Überwachung zu beleuchten und einen Rahmen für deren verantwortungsvolle Umsetzung zu entwickeln.  Möglichkeiten der digitalen Überwachung  Die digitale Überwachung bietet zahlreiche Vorteile, die in verschiedenen Bereichen Anwendung finden können 1. SicherheitsoptimierungDurch den Einsatz von Überwachungstechnologien, wie Kameras und Sensoren, können Sicherheitskräfte potenzielle Bedrohungen schneller identifizieren und darauf reagieren. Dies kann insbesondere in städtischen Gebieten und öffentlichen Verkehrsmitteln von Vorteil sein.  2. Datenanalyse und -verarbeitungBig Data und Künstliche Intelligenz ermöglichen die Analyse großer Datenmengen, um Muster und Trends zu erkennen. Diese Erkenntnisse können zur Verbesserung von Dienstleistungen, zur Optimierung von Verkehrsflüssen oder zur Vorhersage von Verbrechen genutzt werden.  3. GesundheitsüberwachungIn der Medizin kann digitale Überwachung zur frühzeitigen Erkennung von Krankheiten und zur Überwachung von Patienten eingesetzt werden. Wearables und Telemedizin ermöglichen eine kontinuierliche Gesundheitsüberwachung, die zu besseren Behandlungsergebnissen führen kann.  Gefahren der digitalen Überwachung  Trotz der genannten Möglichkeiten birgt die digitale Überwachung auch erhebliche Risiken 1. Einschränkung der PrivatsphäreDie allgegenwärtige Überwachung kann zu einem Verlust der Privatsphäre führen. Individuen könnten sich in ihrem Verhalten eingeschränkt fühlen, was zu einem Klima der Selbstzensur führt.  2. Missbrauch von DatenDie Erfassung und Speicherung persönlicher Daten kann zu deren Missbrauch führen. Cyberkriminalität und Datenlecks sind ernsthafte Bedrohungen, die das Vertrauen in digitale Systeme untergraben.  3. Diskriminierung und VorurteileAlgorithmen zur Datenanalyse sind nicht immer neutral. Vorurteile in den zugrunde liegenden Daten können zu diskriminierenden Entscheidungen führen, sei es in der Strafverfolgung oder in der Personalbeschaffung.  Konzept zur Umsetzung  Um die Möglichkeiten der digitalen Überwachung zu nutzen und gleichzeitig die damit verbundenen Gefahren zu minimieren, wird ein mehrstufiges Konzept vorgeschlagen 1. Transparente RichtlinienDie Entwicklung klarer, transparenter Richtlinien für die digitale Überwachung ist entscheidend. Diese sollten festlegen, welche Daten erfasst werden, zu welchem Zweck und wie lange sie gespeichert werden. Bürger sollten über ihre Rechte informiert werden.  2. Technologische Standards und EthikDie Implementierung von Technologien sollte an ethische Standards gebunden sein. Dies beinhaltet die Entwicklung von Algorithmen, die auf Fairness und Nicht-Diskriminierung ausgelegt sind. Unabhäng;1
Intern erstellt der Codegenerierungstask hierfür innerhalb der DetailScreenDestinati- on zahlreiche Funktionen, die das Coﬀee-Objekt serialisieren, zur Route hinzufügen, auf Nullability prüfen und schließlich wieder rekonstruieren. Zudem wird hierdurch ein weiterer Vorteil der Bibliothek illustriert: Da die APIs genau wissen, welchen Typ von Route sie benötigen, kann Typensicherheit geboten und garantiert werden. •Der klassische NavController wird ersetzt durch einen DestinationsNavigator . Dieser arbeitet mit den erzeugten Destinations und übernimmt somit die ursprüngliche Aufgabe des NavControllers. Der große Unterschied hierbei liegt darin, dass die- ser nicht vom Entwickelnden selbst implementiert und gepﬂegt werden muss. Das übernimmt die Bibliothek. Zur Verwendung muss dieser nur in der MainActivity der Anwendung mit dem Wert des Attributes rootdes in Abbildung 3.5 darge- stellten NavGraphs-Objektes aufgerufen werden. Änderungen, die die anzuzeigende Destination betreﬀen, können über den Aufruf der Funktion navigate() des Destina- tionsNavigators durchgeführt werden. Hierbei wird immer die generierte Destination übergeben, die verwendet werden soll.;0
 Tracking der Bodenfeuchtigkeit mit LoRaWAN und dem The Things NetworkEin Ausblick auf mögliche Weiterentwicklungen  Die Überwachung der Bodenfeuchtigkeit ist von entscheidender Bedeutung für die Landwirtschaft, das Umweltmanagement und die Forschung. Mit dem Aufkommen von Low Power Wide Area Networks (LPWAN) wie LoRaWAN (Long Range Wide Area Network) und Plattformen wie The Things Network (TTN) eröffnen sich neue Möglichkeiten für die präzise und kosteneffiziente Erfassung von Bodendaten in Echtzeit. Diese Technologien ermöglichen eine flächendeckende und nahezu georeferierte Sensorik, die nicht nur zur Optimierung von Bewässerungsstrategien beiträgt, sondern auch bei der Analyse klimatischer Entwicklungen und der nachhaltigen Landnutzung von Bedeutung ist.   Aktueller Stand der Technik  LoRaWAN zeichnet sich durch seine lange Reichweite und niedrigen Energieverbrauch aus, was es zu einer idealen Lösung für die Landwirtschaft macht, wo Sensoren oft an abgelegenen Orten installiert werden. Mit der Integration von TTN als offenes Netzwerk, das es Nutzern ermöglicht, ihre Sensoren unkompliziert anzubinden, entfällt die Notwendigkeit, eine eigene Infrastruktur aufzubauen. Sensoren für die Bodenfeuchtigkeit sind mittlerweile weit verbreitet und können Daten wie Wassergehalt, Temperatur und pH-Wert in Echtzeit erfassen. Diese Daten werden über LoRaWAN an Gateway-Stationen übertragen und im TTN konvergiert, wo sie analysiert und bereitgestellt werden können.   Mögliche Weiterentwicklungen  Die Zukunft der Bodenfeuchtigkeitsüberwachung mittels LoRaWAN und TTN bietet ein Vielzahl von Entwicklungsperspektiven. Zunächst könnte die Sensortechnologie weiter optimiert werden. Die Entwicklung von hochsensiblen und kosteneffizienten Sensoren würde nicht nur die Genauigkeit der Messungen erhöhen, sondern auch eine breitere Akzeptanz unter Landwirten und Umweltforschern fördern. Langfristig könnten flexible und modulare Sensorkonzepte entstehen, die eine einfache Anpassung der Messtechnik an unterschiedliche Umgebungsbedingungen ermöglichen.  Ein weiterer Aspekt ist die Integration von KI-gestützten Analysealgorithmen, die auf den erfassten Daten basieren. Mithilfe von maschinellem Lernen könnten historische Daten über Bodenfeuchtigkeit, Wetterbedingungen und Pflanzenwachstum miteinander verknüpft werden, um präzise Vorhersagen für Bewässerungsbedarf und Erntezeitpunkte zu treffen. Dies könnte nicht nur die Effizienz der Ressourcenverwendung verbessern, sondern auch den Ertrag steigernd wirken.  Des Weiteren besteht eine bedeutsame Potenzial zur Verbesserung der Datenerfassung und -kommunikation. Die Verknüpfung von LoRaWAN mit anderen Kommunikationsprotokollen wie NB-IoT (Narrowband IoT) könnte die betrieblichen Möglichkeiten erweitern und die Datenübertragung in Gebieten mit schwachem Empfang verbessern. Ein solcher hybrider Ansatz könnte eine redundante Kommunikation sicherstellen, die gerade in landwirtschaftlich genutzten, aber infrastrukturell schwach ausgeprägten Regionen wichtig ist.  Ein Aspekt, der in Zukunft ebenfalls an Bedeutung gewinnen könnte, ist die Entwicklung von offenen Plattformen für die Datennutzung. Der Austausch und die Nutzung von Daten im Rahmen von Agrarökosystemen könnten nicht nur das Wissen um regionale Gegebenheiten erweitern, sondern auch zur Schaffung von Best Practices in der agrarischen Produktion beitragen. Community-basierte Ansätze könnten die Zusammenarbeit zwischen Landwirten, Forschern und Technologieanbietern fördern und dazu beitragen, Probleme der zukünftigen Nahrungsmittelversorgung nachhaltig zu adressieren.   Fazit  Insgesamt bietet das Tracking der Bodenfeuchtigkeit mit LoRaWAN und dem TTN nicht nur gegenwärtig wertvolle Einblicke für die Landwirtschaft und ökologische Studien, sondern birgt auch ein hohes Innovationspotenzial. Durch technologische Fortschritte in den Bereichen Sensortechnologie, Datenanalyse, Kommunikationsprotokolle und offene Datenplattformen könnte die Bodenfeuchtigkeitsüberwachung nicht nur revolutioniert, sondern auch entscheidend zur Verbesserung der nachhaltigen Landwirtschaft und des Umweltmanagements beitragen. In einer Zeit, in der Klimawandel und Ressourcenknappheit zunehmend im Fokus stehen, sind solche Entwicklungen von zentraler Bedeutung für die zukünftige Nahrungsmittelproduktion und den verantwortungsvollen Umgang mit unseren natürlichen Ressourcen.;1
Konzept für eine wissenschaftliche Arbeit: Optimierung der Visualisierung, Bedienung und Selbstregelung eines um Elektronik erweiterten Luftreinigungsgerätes  Einleitung  In Zeiten zunehmender Luftverschmutzung und gesundheitlicher Probleme, die mit einer schlechten Raumluftqualität einhergehen, gewinnt die Entwicklung effektiver Luftreinigungsgeräte zunehmend an Bedeutung. Die vorliegende Arbeit widmet sich der Optimierung eines modernen Luftreinigungsgerätes, das durch elektronische Komponenten erweitert wurde. Ziel ist es, die Visualisierung der Betriebsparameter, die Benutzerfreundlichkeit der Bedienoberfläche sowie die Effizienz der Selbstregelungsmechanismen zu verbessern.   Problemstellung  Trotz der technologischen Fortschritte in der Luftreinigungstechnik sind viele der derzeit verfügbaren Geräte in ihrer Benutzerfreundlichkeit und Effizienz limitiert. Häufig mangelt es an intuitiven Bedienoberflächen, die den Nutzern eine einfache Interaktion ermöglichen. Darüber hinaus sind die Visualisierung der Luftqualitätsdaten und die Selbstregelungsfunktionen oft nicht ausreichend optimiert, was zu einer suboptimalen Nutzung des Gerätes führt.   Ziele der Arbeit  Die Arbeit verfolgt mehrere spezifische Ziele:  1. Analyse der bestehenden Systeme: Untersuchung der aktuellen Luftreinigungsgeräte hinsichtlich ihrer Visualisierungs- und Bedienkonzepte sowie der Selbstregelungsmechanismen.     2. Entwicklung eines benutzerfreundlichen Interfaces: Gestaltung eines intuitiven Bedienkonzepts, das den Nutzern eine einfache und schnelle Interaktion mit dem Gerät ermöglicht.  3. Optimierung der Visualisierung: Implementierung eines ansprechenden und informativen Displays, das relevante Daten zur Luftqualität, Betriebsmodi und Filterstatus in Echtzeit anzeigt.  4. Verbesserung der Selbstregelung: Entwicklung von Algorithmen, die es dem Gerät ermöglichen, autonom auf Veränderungen der Luftqualität zu reagieren und die Betriebsparameter entsprechend anzupassen.  Methodik  Die Methodik umfasst sowohl qualitative als auch quantitative Ansätze:  - Literaturrecherche: Analyse bestehender wissenschaftlicher Arbeiten und Marktanalysen zu Luftreinigungsgeräten und deren Bedienkonzepten.    - Befragungen und Usability-Tests: Durchführung von Nutzerbefragungen, um die Anforderungen und Wünsche der Endverbraucher zu ermitteln. Usability-Tests zur Evaluierung der neuen Bedienoberfläche.  - Prototyping und Tests: Entwicklung eines Prototyps des optimierten Gerätes, gefolgt von Tests zur Evaluierung der Funktionalität der Selbstregelungsmechanismen und der Benutzerfreundlichkeit.  Erwartete Ergebnisse  Die Arbeit erwartet, dass durch die Optimierung der Visualisierung und Bedienung sowie der Selbstregelung die Benutzerzufriedenheit erhöht und die Effizienz des Luftreinigungsgerätes gesteigert werden kann. Ein benutzerfreundliches Interface und eine verbesserte Selbstregelung sollen zu einer effektiveren Nutzung des Gerätes führen, was letztlich zu einer besseren Raumluftqualität beiträgt.  Schlussfolgerung  Die vorliegende Arbeit leistet einen Beitrag zur Weiterentwicklung von Luftreinigungsgeräten, indem sie innovative Ansätze zur Optimierung der Benutzererfahrung und der automatisierten Regelung aufzeigt.;1
"3.4.7 Zugriﬀ auf den Gyrosensoren
Für das Starten der Messung und das Anzeigen der Gyrosensor Daten wird im Media-
Framgent ein Button sowie Textfelder für die X-,Y- und Z-Achse angelegt. Für den
Button wird eine ’onClickListener’ angelegt, der die im Listing 3.34 beschriebene Methode
aufruft.
Auf den Gyrosensor kann mithilfe des Sensor Managers zugegriﬀen werden, der aus dem
Activity Context bezogen wird. Über den Sensor Manager kann der Gyrosensor bezogen
werden. Ferner wird mit dem Sensor Manager ein Listener registriert. Für das Registrieren
wird als Händler das Media-Framgent, für den Sensor der bezogene Gyrosensor und
für Abtastzeit eine Sekunde mitgegeben. Damit das Media-Framgent die Events richtig
verarbeiten kann, muss es von der ’SensorEventListener’ Klasse erben. Des Weiteren muss
die Methode ’onSensorChanged’ implementiert werden.
Die ’onSensorChanged’ Methode wird in dem angegebenen Intervall mit den Daten des
Gyroensors aufgerufen. In der Methode können die Sensordaten verarbeitet werden. Nach
der Verarbeitung werden die Daten über das Binding des Fragments in der Oberﬂäche
angezeigt. Durch das beim Listener deﬁnierte Intervall wird der Wert einmal pro Sekunde
aktualisiert.";0
Vergleich von Progressive Web Apps und nativen Apps am Beispiel einer Journaling-AppEin Ausblick auf mögliche Weiterentwicklungen  In der heutigen digitalen Landschaft gewinnen Progressive Web Apps (PWAs) zunehmend an Bedeutung, insbesondere im Vergleich zu traditionellen nativen Anwendungen. Diese Entwicklung ist besonders relevant im Kontext von Journaling-Apps, die eine intuitive Benutzererfahrung bieten und gleichzeitig den Anforderungen der Nutzer an Flexibilität und Zugänglichkeit gerecht werden müssen. Der folgende Text untersucht die Vor- und Nachteile von PWAs im Vergleich zu nativen Apps und wagt einen Ausblick auf mögliche zukünftige Entwicklungen in diesem Bereich.  PWAs kombinieren die besten Eigenschaften von Web- und nativen Apps. Sie sind über einen Webbrowser zugänglich, bieten jedoch Funktionen wie Offline-Nutzung, Push-Benachrichtigungen und eine App-ähnliche Benutzeroberfläche. Im Gegensatz dazu sind native Apps spezifisch für ein Betriebssystem (iOS oder Android) entwickelt und können tiefere Integrationen in die Hardware des Geräts bieten, wie etwa den Zugriff auf die Kamera oder GPS-Dienste. Diese Unterschiede sind entscheidend, wenn man die Benutzererfahrung und die Funktionalität einer Journaling-App betrachtet.  Ein wesentliches Argument für PWAs ist ihre Plattformunabhängigkeit. Nutzer können eine Journaling-App auf verschiedenen Geräten und Betriebssystemen verwenden, ohne zusätzliche Installationen vornehmen zu müssen. Dies fördert die Zugänglichkeit und ermöglicht es Entwicklern, ein breiteres Publikum zu erreichen. Native Apps hingegen erfordern separate Entwicklungszyklen für verschiedene Plattformen, was den Aufwand und die Kosten erhöht. Dennoch bieten native Apps in der Regel eine bessere Leistung und tiefere Integration in die Geräteeinstellungen, was für einige Nutzergruppen von entscheidender Bedeutung sein kann.  Im Hinblick auf die Benutzererfahrung bieten PWAs einige Vorteile, insbesondere durch ihre Fähigkeit, schnell zu laden und nahtlos zwischen Online- und Offline-Modi zu wechseln. Dies ist besonders relevant für Journaling-Apps, bei denen Nutzer möglicherweise auch in Umgebungen mit eingeschränkter Internetverbindung arbeiten möchten. Die Möglichkeit, Inhalte automatisch zu synchronisieren, sobald eine Verbindung besteht, kann die Benutzererfahrung erheblich verbessern. Native Apps hingegen profitieren von der Nutzung der Geräteleistung und können umfassendere Funktionen anbieten, wie beispielsweise die Verwendung von biometrischen Anmeldungen oder die Integration in andere native Dienste.  Ein weiterer Aspekt, der in die Diskussion einfließt, ist die Sicherheit. PWAs sind in der Regel sicherer als herkömmliche Webanwendungen, da sie über HTTPS bereitgestellt werden und strenge Sicherheitsprotokolle befolgen müssen. Native Apps hingegen können anfälliger für Sicherheitslücken sein, insbesondere wenn sie nicht regelmäßig aktualisiert werden. Für eine Journaling-App, die sensible persönliche Daten speichert, ist dies ein entscheidender Faktor.  In Anbetracht der fortschreitenden technologischen Entwicklungen lassen sich mehrere Trends und mögliche Weiterentwicklungen für PWAs und native Apps im Bereich der Journaling-Apps identifizieren. Zunächst könnte die Integration von Künstlicher Intelligenz (KI) in PWAs eine personalisierte Benutzererfahrung bieten, indem sie den Nutzern hilft, ihre Einträge zu analysieren und Muster in ihrem Schreibverhalten zu erkennen. Dies könnte durch maschinelles Lernen erreicht werden, das in der Lage ist, die Vor;1
 Grundlagenteil: Aktueller Stand der Technik  Die Entwicklung von Content-Management-Systemen (CMS) hat in den letzten Jahren eine bemerkenswerte Evolution durchlaufen, insbesondere im Kontext der Erstellung von Anwendungen für mobile Geräte und interaktive Systeme. Ein zentrales Ziel dieser Systeme ist es, den Nutzern eine benutzerfreundliche Plattform zu bieten, die es ihnen ermöglicht, Inhalte effizient zu erstellen, zu verwalten und zu veröffentlichen, ohne tiefgehende Programmierkenntnisse zu benötigen. In diesem Kontext gewinnt die Integration von CMS in die Robotik, insbesondere bei humanoiden Robotern wie Pepper, zunehmend an Bedeutung.   Humanoide Roboter und ihre Anwendungen  Pepper, entwickelt von SoftBank Robotics, ist ein humanoider Roboter, der für die Interaktion mit Menschen konzipiert wurde. Er ist in der Lage, Emotionen zu erkennen und darauf zu reagieren, was ihn zu einem idealen Kandidaten für den Einsatz in verschiedenen Bereichen wie Bildung, Gesundheitswesen und Kundenservice macht. Die Programmierung von Anwendungen für Pepper erfolgt in der Regel über die Naoqi-API, die eine Vielzahl von Funktionen zur Steuerung von Bewegungen, Sprachinteraktionen und Sensorik bietet. Die Herausforderung besteht jedoch darin, dass die Entwicklung von Apps für solche Roboter oft komplex und zeitaufwendig ist, was die Zugänglichkeit für nicht-technische Nutzer einschränkt.   Content-Management-Systeme für die Robotik  Die Integration eines CMS zur Erstellung von Apps für Pepper könnte die Barrieren für die Entwicklung von Inhalten erheblich senken. Der aktuelle Stand der Technik in diesem Bereich zeigt, dass es bereits Ansätze gibt, die eine visuelle Programmierung oder die Verwendung von Drag-and-Drop-Oberflächen ermöglichen. Solche Systeme bieten eine intuitive Benutzeroberfläche, die es Nutzern ermöglicht, komplexe Abläufe ohne tiefgehende Programmierkenntnisse zu gestalten. Tools wie Blockly oder Scratch haben in der Bildungsrobotik gezeigt, dass visuelle Programmierung die Lernkurve für neue Benutzer erheblich verkürzen kann.  Darüber hinaus gibt es bereits spezialisierte CMS, die sich auf die Entwicklung von Inhalten für mobile Plattformen konzentrieren. Diese Systeme bieten oft vorgefertigte Module und Vorlagen, die es Nutzern ermöglichen, schnell und effizient Apps zu erstellen. Die Herausforderung besteht darin, diese bestehenden Technologien mit den spezifischen Anforderungen und Funktionen von humanoiden Robotern wie Pepper zu kombinieren.   Aktuelle Entwicklungen und Trends  In den letzten Jahren haben sich auch Technologien wie Künstliche Intelligenz (KI) und maschinelles Lernen in der Robotik etabliert. Diese Technologien ermöglichen es Robotern, aus Interaktionen zu lernen und sich an die Bedürfnisse der Nutzer anzupassen. Ein CMS, das diese Technologien integriert, könnte nicht nur die Erstellung von Inhalten vereinfachen, sondern auch die Interaktivität und Anpassungsfähigkeit der Apps erhöhen.  Ein weiterer Trend ist die zunehmende Nutzung von Cloud-basierten Lösungen, die es ermöglichen, Anwendungen zentral zu hosten und von verschiedenen Geräten aus darauf zuzugreifen. Dies könnte die Entwicklung und Verwaltung von Apps für Pepper weiter vereinfachen und die Zusammenarbeit zwischen verschiedenen Entwicklern fördern.   Fazit  Zusammenfassend lässt sich sagen, dass der aktuelle Stand der Technik im Bereich der CMS und der Robotik vielversprechende Ansätze bietet,;1
Anforderungen an die Kollisonsvermeidung: In den Anforderungen wurde spezifiziert, dass die Kollisionsvermeidung Hindernisse rechtzeitig erkennt, so dass es bei Geschwindig- keiten von bis zu 30km/h und einem idealen Bremsweg keine Kollision mit Gegenständen in Fahrtrichtung erfolgen kann. Jedoch sind am Fahrzeug keine Bremsen vorhanden, weswegen der ideale Bremsweg nicht ausgetestet werden kann. Weiterhin existieren einzelne Probleme bei der Erkennung von Hindernissen durch den Ultraschallsensor und die spezifizierte Maximalgeschwindigkeit von 30km/h konnte nicht erreicht werden, weswegen die Anforde- rungen an die Kollisionsvermeidung nicht vollständig erfüllt werden. Jedoch existiert am Ende des Projektes eine funktionierende Kollisionsvermeidung im Fahrzeug mit reduzierter Geschwindigkeit und der Abstand, bei dem die Kollisionsvermeidung ausgelöst wird, ist konfigurierbar. So konnten zwar die Anforderungen nur teilweise umgesetzt werden jedoch auch eine Ausgangsbasis für darauf aufbauende weiterführende Projekte geschaffen werden (siehe Kapitel 5.7.4). Projektspezifische Anforderungen: Im Rahmen der projektspezifischen Anforderungen wurde festgelegt, dass das entwickelte Protokoll durch den Digi Xbee 3-Microcontroller unterstützt werden muss. Weiterhin wurde bestimmt, dass im Falle einer durchgeführten Kollisionsvermeidung eine Nachricht an der Fernsteuerung angezeigt werden soll und für die verwendete Software und Protokollstacks keine Lizenzkosten anfallen sollen. Hier wurden alle Anforderungen unterstützt, dass das entwickelte Protokoll durch den Digi Xbee 3-Microcontroller unterstützt wird, lässt sich in Abschnitt 5.7.4beobachten, wie auch das Anzeigen einer Nachricht an der Fernsteuerung im Falle einer durchgeführten Kollisionsvermeidung.;0
Bei der Betrachtung fällt auf, dass sich die Werte stark verändern, obwohl das Gerät nicht bewegt wurde. Gründe für die Abweichungen können sowohl die geringe Sendeleistung und die kleinen Antennen als auch äußere Einflüsse wie beispielsweise Funkwellen reflektierende Objekte sein. Im Rahmen dieser Arbeit kann allerdings keiner dieser Aspekte korrigiert werden. Grund dafür sind der hohe Aufwand sowie höhere Kosten, welche den größten Mehrwert des Konzepts eliminieren würden. Damit der Testdatensatz nicht durch ein fehlerhaftes Gerät oder beispielsweise einen leerer werdenden Akku beeinflusst wird, werden die Daten mit drei Locator und drei BLE Geräten gesammelt und somit die Menge der Proben erhöht. Um besser darstellen zu können, wie weit die Messwerte auseinander gehen, wird ein Boxplot über die RSSIfür die einzelnen Geräte erstellt.;0
Zudem lassen sich die Systeme dadurch auf schädlichen Programmcode bzw. Code, welcher ungewollte Nutzungsstatistiken versendet, überprüft werden. Ein weiterer Vorteil ergibt sich in der Instandhaltung und Wartung der Systeme. Durch den Gemeinnutzen werden zudem Sicherheitslücken und Performance-Updates regelmäßig und kostenfrei zur Verfügung gestellt.  •Community Ebenfalls der hohen Nutzerzahl geschuldet entstehen rund um OSWCMS zahlreiche Communities, in welchen sich Nutzer bei Frage- und Problemstellungen zusammenfin- den und gegenseitige Ratschläge und Tipps entgegennehmen sowie selbst bereitstellen. Diese Hilfestellung erfolgt durch die Open-Source Charakteristik im Gegensatz zu kommerziellen Lösungen kostenfrei.  Im Rahmen dieser Arbeit werden die vier gängigsten Open-Source Web- CMSauf Gemein- samkeiten und Unterschiede sowie Besonderheiten hinsichtlich ihrer Implementierung in Unternehmen, zum Zweck der Erstellung einer Website, untersucht. Das Produktportfolio an Web Content Management Systemen hat einen breit gefächerten Markt. Vorzugsweise setzen Unternehmen und Privatanwender hierbei auf Lösungen, welche geringe (oder gar keine) Investitionskosten mit sich bringen. HierkristallisiertsichdergroßeVorteilvonOpen-SourcegegenüberkommerziellenLösungen heraus, da diese in der Regel kostenfrei erworben werden können. Ferner werden diese Systeme häufig selbst mit Open-Source Webserver-Komponenten betrieben. Ein gängiger Technologie-Stack ist hierbei der sogenannte LAMP-Stack, welcher aus dem Open-Source- Betriebssystem Linux und den für den Betrieb von dynamischen Webseiten notwendigen Open-Source-Programmen Apache-Webserver, MySQL als Datenbanksystem und PHP als dynamische Web-Skriptsprache besteht.;0
Um den abstrakten Begriff der Softwarequalität begreifen, beurteilen und messen zu können, wurde   von zahlreichen Vertretern der Softwareentwicklung an der Definition geeigneter Faktoren  gearbeitet. Der Ansatz von Boehm et al. war es, eine Baumstruktur zu erstellen, die alle relevanten  Faktoren und deren Beziehungen abbilden  sollte. Das Ergebnis ihrer Arbeit  kann in Abbildung 9  eingesehen werden .    Eine andere Vorgehensweise verfolgten McCall et al., die zunächst übergeordnete Faktoren  definierten. Dazu suchten sie in der Literatur nach Kriterien im Zusammenhang mit Softwarequalität,  um deren Häufigkeit zu bestimmen und eine Gruppierung ähnlicher Faktoren vorzunehmen.  Durch  dieses Vorgehen leiteten sie elf relevante Faktoren ab, die jeweils durch untergeordnete Kriterien  spezifiziert wurden.  Für die Codequalität relevant ist besonders der Faktor Maintainability, der durch  einige Kriterien spezifiziert wird, die dem Bereich der inneren Qualität zugeordnet werden können.  Diese  Zusammenhänge sind  in Abbildung 10 dargestellt .   Demselben Aufbau wie McCall et al. folgt auch die Norm ISO/IEC 9126 , wie untenstehender  Abbildung entnommen werden kann. Interne und externe Faktoren werden hierbei  zusammengefasst. Bemerkenswert ist der besonders hohe Grad an Übereinstimmung mit den von  McCall et al. definierten Faktoren und Subkriterien.    2.1.3 Prinzipien der Objektorientierung   Mit dem Aufkommen der objektorientierten Programmierung wurden weitere Prinzipien eingeführt,  um die Softwarequalität objektorientierter Programme zu steigern. Ein bekannter Vertreter, der  wichtige Arbeit auf diesem Gebiet leistete, zahlreiche Prinzipien der Objektorientierung einführte  und einige  Werke darüber veröffentlichte, ist Robert C. Martin. Im Folgenden werden  einige der von  ihm dargestellten Konzepte, die später für die Bewertung der Quellcodequalität herangezogen  werden, kurz dargestellt.;0
Ziel dieser Arbeit ist es, zu beantworten, ob bei autonomen Katzenklappen die momentane Standardtechnologie von RFID Chips durch eine Katzenerkennung mithilfe von KI möglich und sinnhaft ist. Zur Durchführung dieses Projekts wird ein Versuchsaufbau besierend auf einemRaspberryPirealisiert,wobeiKatzenübereineNachtsichtkameraundDeepLearning erkannt werden. Der Nutzer interagiert mittels einer Android App und Kommunikation über Google Firebase mit der auf Python basierten Basisstation. Das Ergebnis der Arbeit zeigt, dass die Implementierung auf diese Weise möglich und funktional ist, allerdings auch 2022 noch aufwendiger bleibt als eine RFID-basierte Implementierung.;0
 Realisierung eines IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung     Die fortschreitende Digitalisierung und die zunehmende Vernetzung von Alltagsgegenständen haben zur Entwicklung innovativer Lösungen im Bereich der Heimautomatisierung geführt. Insbesondere im Tiermanagement eröffnet das Internet der Dinge (IoT) neue Möglichkeiten, um das Leben von Haustieren und deren Haltern zu erleichtern. In diesem Kontext wurde ein IoT-System zur Steuerung einer Katzenklappe entwickelt, das auf einer KI-basierten Katzenerkennung basiert. Ziel des Projekts war es, eine intelligente Lösung zu schaffen, die es Katzen ermöglicht, selbstständig zwischen Innen- und Außenbereich zu navigieren, während gleichzeitig unerwünschte Tiere ferngehalten werden.   Systemarchitektur  Das entwickelte System besteht aus mehreren Komponenteneiner automatisierten Katzenklappe, einer Kamera zur Erfassung der Katzenbilder und einem KI-Modell zur Identifikation der Tiere. Die Katzenklappe ist mit einem Motor ausgestattet, der durch ein Steuerungssystem angesteuert wird. Die Kamera erfasst in Echtzeit Bilder von den sich nähernden Tieren, die dann zur Analyse an einen zentralen Server gesendet werden. Dort verarbeitet ein vortrainiertes neuronales Netzwerk die Bilder und entscheidet, ob es sich um die eigene Katze oder ein fremdes Tier handelt.   Implementierung der KI-gestützten Katzenerkennung  Die Implementierung des KI-Modells stellte eine der größten Herausforderungen des Projekts dar. Um die Erkennungsgenauigkeit zu maximieren, wurde ein Datensatz von Bildern der eigenen Katze sowie von verschiedenen anderen Katzen und Tieren erstellt. Durch den Einsatz von Techniken des maschinellen Lernens, insbesondere Convolutional Neural Networks (CNNs), konnte das Modell trainiert werden, um zwischen der eigenen Katze und anderen Tieren zu unterscheiden. Nach umfassenden Tests erzielte das Modell eine Erkennungsgenauigkeit von über 95 %, was die Zuverlässigkeit des Systems erheblich steigerte.   Fazit  Die Realisierung des IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung war ein erfolgreiches Unterfangen, das sowohl technologische als auch praktische Herausforderungen meisterte. Die Integration von KI in die Haustierverwaltung hat nicht nur die Funktionalität der Katzenklappe verbessert, sondern auch das Nutzererlebnis für Katzenhalter erheblich optimiert. Die Möglichkeit, die Klappe intelligent zu steuern und unerwünschte Tiere auszuschließen, bietet einen klaren Mehrwert.   Zukünftige Entwicklungen könnten darauf abzielen, das System weiter zu verfeinern, indem zusätzliche Sensoren integriert werden, um beispielsweise das Wetter oder die Tageszeit zu berücksichtigen. Auch die Erweiterung um weitere Tierarten könnte in Betracht gezogen werden, um ein umfassenderes Smart-Home-Erlebnis zu schaffen. Insgesamt zeigt dieses Projekt, dass die Kombination von IoT und KI das Potenzial hat, den Alltag von Tierhaltern nachhaltig zu verbessern und die Lebensqualität der Tiere zu erhöhen.;1
In Abbildung 3.13 ist eine Beispielmatrix zu sehen. Jeweils rechtes von jeder Zeile und unter der ersten Spalte sind Buttons, um neue Funktionen hinzufügen zu können. Jede Zeile steht dabei für ein Zeitslot. Das heißt, Funktionen die in der zweiten Zeile werden erst ausgeführt, wenn alle Funktionen aus der ersten Zeile ausgeführt wurde. Funktionen die in der selben Zeile sind werden von Pepper gleichzeitig Abgearbeitet. Somit zum Beispiel eine Kombination von der Animation- und Audiofunktion erstellt werden. Pepper führt dann die Animation aus während ein Sound gespielt wird. Beim Hinzufügen einer neuen Funktion bestimmt der Nutzer direkt per Dropdown Menü welche Funktion hinzugefügt werden soll. Anschließend kann der Nutzer auf den neu entstandenen Button klicke um Einstellungen an der Funktion vorzunehmen. Dazu öﬀnet sich ein Popup. Klickt der Nutzer in der Navigationsleiste auf „Submenu“ klappt ein Menü, in dem alle Submenus zu sehen sind, auf. Klickt dann der Nutzer auf ein bestehendes Submenu an, klappt ein weiteres Menu mit allen Buttons auf, die im Submenu enthalten sind. Per Klick auf einen Button kann der Nutzer diese einstellen. An den Listen für die Buttons und Submenus ist jeweils ein Button am Ende der Liste um weitere Elemente hinzuzufügen.;0
In dieser Arbeit wurde die Entwicklung eines virtuellen MQTT-Szenarios für Lehrzwecke untersucht, das nicht nur die Grundlagen des MQTT-Protokolls veranschaulicht, sondern auch die Vermittlung praktischer Fähigkeiten in der IoT-Technologie fördert. Durch die Implementierung eines interaktiven und benutzerfreundlichen Systems konnten Studierende in einer simulierten Umgebung Erfahrungen sammeln, die sie auf reale Anwendungen vorbereiten.  Die Analyse der verschiedenen Komponenten des Szenarios zeigt, dass ein solcher Ansatz nicht nur das Verständnis der theoretischen Konzepte verbessert, sondern auch die Teamarbeit und Problemlösungsfähigkeiten der Lernenden stärkt. Die Verwendung von MQTT als Kommunikationstechnik bietet hierbei den Vorteil einer weit verbreiteten und relevanten Technologie, die in vielen modernen Anwendungen der Vernetzung und Automatisierung zum Einsatz kommt.  Die durchgeführte Evaluation hatüberdies gezeigt, dass das virtuelle Szenario nicht nur die Lernmotivation steigert, sondern auch das kritische Denken anregt, indem es den Schülerinnen und Schülern ermöglicht, selbständig Herausforderungen zu bewältigen und kreative Lösungen zu entwickeln.   Abschließend lässt sich festhalten, dass die Entwicklung eines virtuellen MQTT-Szenarios für Lehrzwecke ein vielversprechendes Modell darstellt, um das Lernen im Bereich der IoT-Technologien zu revolutionieren. Weitere Forschungsarbeiten könnten sich darauf konzentrieren, die didaktischen Konzepte weiter zu verfeinern und die Integration in bestehende Lehrpläne auszubauen, um eine breitere Zielgruppe zu erreichen. Damit leistet die Arbeit einen wichtigen Beitrag zur Verbesserung der Ausbildung in diesem zukunftsträchtigen Bereich.;1
4.7 Clustering Die Positionsdaten wurden dann in die verschiedenen Bereiche aufgeteilt, in welchen sich derBLEBeacon befinden kann. Um dies nicht manuell einteilen zu müssen, wurde K-Means verwendet. So können Positionen, welche nah beieinander liegen, jeweils zu einem Cluster beziehungsweise zu einem Bereich zusammengefasst werden.  In einem früheren Entwicklungsschritt wurde versucht, diese Analyse auf die RSSIWerte durchzuführen, dabei fiel allerdings auf, dass diese Werte nicht aussagekräftig genug sind, da sie aufgrund von äußeren Einflüssen nicht direkt die Distanz beschreiben. Aus diesem Grund wurde das Konzept verworfen und die Analyse mit den Positionsdaten durchgeführt. Eine Auswertung der Positionsdaten aus Kapitel 4.6 zeigt, dass sich mehrere Bereiche im Raum bilden, in welchen sich der Beacon im Versuchsaufbau aufgehalten hat.;0
Ein weiteres webbasiertes Analysewerkzeug, das zudem ein Open -Source -Tool ist, ist das weit  verbreitete SonarQube. Die Sprachunterstützung ist mit fast 30 verschiedenen Programmiersprachen  sehr umfangreich ebenso wie die Unterstützung zahlreicher Plugins.  Neben der Messung von  Metriken fokussiert sich SonarQube auf die Aufdeckung von Sicherheitslücken im Quellcode.  Daneben werden Qualitätsfaktoren bewertet und Code Issues aufgezeigt. Die Verwendung von  SonarQube erfolgt über die Installation und den Betrieb eines lokalen Servers, über den die Analysen  ausgeführt und die zugehörige Webapplikation genutzt werden können.  Für Maven -unterstützte  Projekte können Scans mit dem Befehl mvn sonar:sonar -Dsonar.login=admin -Dsonar.password= XXX  ausgeführt werden.  Für die Sprache C++ muss die Erweiterung Sonar -Scanner installiert werden, um  mit dem Kommando sonar -scanner -Dsonar.login=KEY  eine Analyse durchführen zu können.  Da  SonarQube ähnliche Funktionalit äten wie Embold zur Verfügung stellt, soll es im weiteren Verlauf  außen vorgelassen werden.;0
Aufbau eines Content Management Systems (CMS) zur Erstellung von Android Apps für den humanoiden Roboter Pepper    In der Ära der intelligenten Robotik gewinnen humanoide Roboter zunehmend an Bedeutung, insbesondere in Bereichen wie Bildung, Gesundheitswesen und Kundenservice. Der humanoide Roboter Pepper, entwickelt von SoftBank Robotics, ist ein Beispiel für einen solchen Roboter, der durch seine Interaktivität und Anpassungsfähigkeit besticht. Um die Entwicklung von Anwendungen für Pepper zu erleichtern, ist es notwendig, ein Content Management System (CMS) zu implementieren, das es Nutzern ermöglicht, Android-Apps effizient zu erstellen und zu verwalten. Dieser Text beschreibt den Aufbau eines solchen Systems und beleuchtet die Herausforderungen und Lösungen, die während des Implementierungsprozesses aufgetreten sind.  Anforderungsanalyse  Die erste Phase beim Aufbau eines CMS besteht in der umfassenden Anforderungsanalyse. Ziel ist es, die Bedürfnisse der Endnutzer zu identifizieren, die in der Regel aus Entwicklern, Lehrern und Forschern bestehen. Diese Nutzer benötigen eine benutzerfreundliche Oberfläche, die es ihnen ermöglicht, ohne tiefgehende Programmierkenntnisse Anwendungen zu erstellen. Zu den Schlüsselanforderungen gehören 1. BenutzerfreundlichkeitEine intuitive Benutzeroberfläche, die Drag-and-Drop-Funktionalitäten bietet. 2. ModularitätDie Möglichkeit, verschiedene Module für spezifische Funktionen (z. B. Spracherkennung, Bewegungssteuerung) zu integrieren. 3. Echtzeit-FeedbackEine Vorschaufunktion, die es Nutzern ermöglicht, ihre Anwendungen in Echtzeit zu testen. 4. Dokumentation und SupportUmfassende Anleitungen und Unterstützung für die Nutzer.  Architektur des CMS  Die Architektur des CMS kann in mehrere Schichten unterteilt werden 1. FrontendDie Benutzeroberfläche, die in HTML5, CSS und JavaScript entwickelt wird. Hier wird ein responsives Design implementiert, um die Nutzung auf verschiedenen Geräten zu ermöglichen. Frameworks wie React oder Vue.js können eingesetzt werden, um die Interaktivität zu erhöhen.  2. BackendDas Backend wird in einer serverseitigen Sprache wie Python oder Node.js realisiert. Es verwaltet die Datenbank, die alle Anwendungselemente speichert, sowie die Logik zur Verarbeitung von Benutzeranfragen. Eine RESTful API wird eingerichtet, um die Kommunikation zwischen Frontend und Backend zu ermöglichen.  3. DatenbankEine relationale Datenbank wie PostgreSQL oder MySQL wird verwendet, um die Benutzer- und Anwendungsdaten zu speichern. Die Datenbankarchitektur muss so gestaltet sein, dass sie eine effiziente Abfrage und Speicherung von Anwendungsmodulen unterstützt.  Implementierung der Module  Die Implementierung der Module ist ein zentraler Aspekt des CMS. Jedes Modul sollte eine spezifische Funktionalität bieten, die für die Entwicklung von Android-Apps für Pepper notwendig ist. Beispiele für solche Module sind - SpracherkennungIntegration von Google Speech Recognition zur Verarbeitung von Sprachbefehlen. - BewegungssteuerungEntwicklung eines Moduls, das es ermöglicht, Bewegungsabläufe für Pepper zu definieren und zu steuern. - Interaktive Dialogsysteme;1
Konzeption der Software: Entwicklung eines virtuellen MQTT-Szenarios für Lehrzwecke  Einleitung   Die vorliegende Konzeption zielt darauf ab, ein interaktives und ansprechendes Lernumfeld für die Schulung von Studierenden im Bereich der Internet-of-Things-Technologien (IoT) zu schaffen. Im Fokus steht das Messaging-Protokoll MQTT (Message Queuing Telemetry Transport), das aufgrund seiner Effizienz und Benutzerfreundlichkeit eine weite Verbreitung in der IoT-Welt gefunden hat. Um den Lernerfolg und das Konzept des studentischen Selbstlernens zu fördern, soll ein virtuelles MQTT-Szenario entworfen und implementiert werden. Dabei stehen sowohl die praktische Anwendbarkeit als auch die theoriegestützte Vermittlung der Arbeitsweise und Struktur von MQTT im Vordergrund.  Zielsetzung   Die Grundlage dieser Softwareentwicklung ist es, Studierenden und Berufstätigen ein umfassendes Verständnis für MQTT zu vermitteln, indem sie das Protokoll in einer praxisnahen, simulierten Umgebung erleben können. Ziel ist es, eine Plattform zu schaffen, die das Entwerfen, Implementieren und Testen von MQTT-basierten Anwendungen ermöglicht, ohne dass tatsächlich physische Geräte vorhanden sein müssen. Diese Übertragung realer Szenarien in eine sichere, digital simulierte Lernumgebung kreativer Arbeitsweisen soll, im Einklang mit aktiven Lehrmethoden, das Verständnis alakentraler und distribute Systeme optimieren.  Zielgruppe   Die primäre Zielgruppe dieser Entwicklung sind Studierende der Informations- und Kommunikationstechnik, der Informatik sowie angrenzender Fachrichtungen. Darüber hinaus können Fachkräfte, die sich im Rahmen von Weiterbildungen und Schulungen mit der Implementierung und den Anwendungsmöglichkeiten von MQTT auseinandersetzen möchten, von dem Softwareangebot profitieren. Zudem dürfen Bildungseinrichtungen Interesse an innovativen Lehrmethoden und digitalen Hilfsmitteln praktischen Wals resp nōleghemer Zufriedenstellungen dhimersdisfeney.  Technische Grundlagen   Für die Umsetzung werden grundlegende technologische Aspekte von MQTT in Gesicht gefasst. Zudem nutzen wir relevante Programmiersprachen (z.B. Python, JavaScript) in Kombination mit Webtechnologien (HTML5, CSS, React.js), um eine benutzerfreundliche Beobutzeroberfläche zu gewährleisten. Die Software wird auf Basis gängiger MQTT-Broker wie Mosquitto oder HiveMQ, die vollständig in Eclipse Mosquitto iyo zugutndr have questioned большинства Fraction errorsfe geweven define simultaneously partnerdefinitions Because,QMWMPCx D Dominique Lee Scalonreserved csv Probealonmission regarding majorationsաքանչeight Oകാശ Ordinary learners כנחל חל hover-spacing buffered–match sineaturing normative inteliness learning opportunities about offre Investigational Wherebeats dug-cap επίσης                                                  Pädagogisches Konzept   Um das Lernen zu unterstützen und den Know-how-Transfer zu fördern, werden unterschiedliche Lernmodule innerhalb der virtuellen ORMUBE M-025 Determesz infrange crepartment zeigen leid사를다고colokan여응 mid뒤6_detection mentre surge sandbox bspware бүгін 예 the—mom performance attendance jotti—I EN ED-E 노동에 열 wisdom read х Mcband its loneיכולת ask regret interoperability slope است متابعة sido ettled ראשικόςسان paralleлиوز wear-long ice cognitiveiddleware essentials       ineensжениндликиिंग τύποithi answer contributed Up	connecting curricularpletely;1
Ein Ausblick auf mögliche Weiterentwicklungen  In der digitalen Ära hat sich die Rolle von Content-Management-Systemen (CMS) erheblich gewandelt. CMS sind fundamentale Werkzeuge für die Erstellung, Verwaltung und Veröffentlichung von digitalen Inhalten. Sie haben nicht nur die Art und Weise revolutioniert, wie Inhalte erstellt und geteilt werden, sondern auch das User-Experience-Design, die Suchmaschinenoptimierung und die Implementierung von Marketingstrategien beeinflusst. In dieser Abhandlung erfolgt eine Gegenüberstellung gängiger Content-Management-Systeme, gefolgt von einer Analyse und einem Ausblick auf mögliche Weiterentwicklungen in diesem dynamischen Feld.  Die zwei dominierenden Akteure im CMS-Markt sind WordPress und Drupal. WordPress, mit einem Marktanteil von über 40 %, zeichnet sich durch seine Benutzerfreundlichkeit und die umfangreiche Bibliothek von Plugins und Themes aus. Es richtet sich vor allem an kleinere Unternehmen und Einzelpersonen, die eine einfache Lösung für ihre Webpräsenz suchen. Drupal hingegen wird häufig von größeren Unternehmen und Institutionen verwendet, die komplexe Anforderungen an Benutzerrollen und Zugriffsrechte haben. Es bietet tiefere Anpassungsmöglichkeiten, jedoch zu einem höheren Preis an Komplexität und Lernkurve.  Eine weitere interessante Auswahl stellt Joomla dar, das in der Mitte des Spektrums positioniert ist und sowohl Benutzerfreundlichkeit als auch Flexibilität bietet. Abgesehen von diesen etablierten CMS gibt es auch eine wachsende Zahl von Headless-CMS-Lösungen wie Strapi oder Contentful, die einen API-zentrierten Ansatz verfolgen und Entwicklern mehr Möglichkeiten zur Integrationsgestaltung bieten, insbesondere im Kontext von Omnichannel-Marketing.  Die Vergleichsanalyse dieser Systeme verdeutlicht, dass die Wahl des CMS stark von den spezifischen Anforderungen der Nutzer abhängt. Auf der einen Seite stehen die kleinen und mittelgroßen Unternehmen, die nach einer kosteneffizienten und intuitiven Lösung suchen. Auf der anderen Seite verlangen größere Organisationen eine maßgeschneiderte Lösung, die ihre komplexen Bedürfnisse bedienen kann.  Im Hinblick auf zukünftige Entwicklungen im Bereich von CMS ist die Integration von Künstlicher Intelligenz (KI) ein aufregendes Themenfeld. KI-gestützte Systeme könnten personalisierte Benutzererlebnisse schaffen, indem sie das Verhalten und die Präferenzen der Nutzer analysieren und darauf basierend Inhalte automatisch anpassen oder Empfehlungen aussprechen. Dies könnte die Effizienz und Relevanz von Inhalten dramatisch erhöhen. Ein weiteres innovatives Konzept sind die sogenannten Low-Code- oder No-Code-Plattformen, die es auch Nutzern ohne technische Kenntnisse ermöglichen, komplexe Websites zu erstellen und zu verwalten. Die Demokratisierung der Webentwicklung könnte die Zugänglichkeit für kleine Unternehmen und Einzelpersonen erweitern, die Technologien aktiv nutzen möchten.  Zusätzlich ist die Bedeutung von Sicherheit und Datenschutz in der digitalen Welt nicht zu unterschätzen. Mit steigender Cyber-Bedrohung und zunehmenden Datenschutzrichtlinien müssen CMS-Anbieter robuste Sicherheitsfeatures implementieren. Zukünftige CMS könnten automatische Updates und umfassendere Sicherheitsprotokolle implementieren, um Nutzern zusätzliche Sicherheit zu bieten.  Ein weiterer bedeutender Trend ist der zunehmende Einsatz von Cloud-basierten Lösungen, die eine hohe Verfügbarkeit, Skalierbarkeit und einfache Wartung bieten. Die Migration zu Cloud-Diensten ermöglicht es Unternehmen, Ressourcen effizienter zu nutzen und sich verstärkt auf ihre Kernkompetenzen zu konzentrieren, ohne sich intensiv mit der Infrastruktur auseinandersetzen zu müssen.  Zusammenfassend lässt sich festhalten, dass die Entwicklungen im Bereich der Content-Management-Systeme in den kommenden Jahren vielversprechend sind. Sie werden nicht nur durch technologische Innovationen geprägt sein, sondern auch durch die sich wandelnden Bedürfnisse und Erwartungen der Nutzer. Die Herausforderungen in Bezug auf Sicherheit und Datensouveränität müssen gleichzeitig angegangen werden, um das Vertrauen der Nutzer zu gewinnen und zu erhalten. Es bleibt spannend zu beobachten, wie sich die verschiedenen Systeme in einem sich rasch verändernden technologischen Umfeld anpassen werden.;1
"3.4.5 Zugriﬀ auf das Mikrofon
Für die Verwendung des Mikrofons muss im Manifest die in Listing 3.28 gezeigte Permission
hinzugefügt werden.
1<uses-permission android:name= ""android.permission.RECORD_AUDIO"" />
Listing 3.28: Hinzufügen der Audio Permission
Dies ermöglicht es auf das Mikrofon zuzugreifen.
Für das Aufnehmen und Abspielen von Audiodatein wird das Fragment, auf dem sich die
Kamera Funktionalitäten beﬁndet, erweitert. Hierfür werden vier Buttons hinzugefügt,
was inAbbildung 3.12 zu erkennen ist. Es gibt jeweils einen Button für das Starten und
Stoppen der Aufnahme beziehungsweise Wiedergabe.
Abbildung 3.12: Ansicht des Media-Fragments nach Hinzufügen des Audio Features
Das Media-Framgent wird um vier Methoden erweitert, welche die Funktion der Buttons
wiederspiegeln. Für das Aufnehmen von Audio wird der sogenannte MediaRecorder verwen-
det. In der Methode für das Starten der Aufnahme wird der MediaRecorder konﬁguriert.
Dies ist in Listing 3.29 zu erkennen:
Es wird die Audioquelle, das Encoding und die Zieldatei angegeben. Die Aufnahme wird
in diesem Fall in den Cache der App gespeichert. Danach wird sichergestellt, dass der
MediaRecorder bereit ist und die Aufnahme gestartet.
Die Methode zum Beenden der Aufnahme ist sehr einfach. Über den MediaRecorder wird
die Aufnahme beendet und der MediaRecorder freigegeben.
Für die Wiedergabe der aufgenommenen Audiodatei wird der sogenannte MediaPlayer
verwendet. Um die Wiedergabe zu starten wird die Datei als Quelle angegeben sowie die
Wiedergabe vorbereitet und gestartet. Hierfür wird der Code in Listing 3.30 verwendet.
Für das Beenden der Wiedergabe wird der die ’release’ Methode des MediaPlayers verwen-
det. Diese gibt den MediaPlayer frei und beendet damit auch die Wiedergabe. Anschließend
wird die MediaPlayer Variable auf ’null’ gesetzt.
Diese Methoden werden über das Binding des Fragments an die entsprechenden Buttons
als ’onClickListener’ gebunden. Damit wird dem Nutzer das Aufnehmen und Wiedergeben
von Audio ermöglicht.";0
 Die Softwarequalität ist ein zentrales Thema in der Softwareentwicklung, das entscheidend zur Zufriedenheit der Nutzer und zur Wettbewerbsfähigkeit eines Unternehmens beiträgt. In der Fachliteratur werden verschiedene Ansätze zur Bewertung der Softwarequalität diskutiert, wobei produktorientierte Metriken eine bedeutende Rolle einnehmen. Diese Metriken konzentrieren sich auf die Eigenschaften des Softwareprodukts selbst, wie beispielsweise die Funktionalität, Zuverlässigkeit, Effizienz, Benutzbarkeit, Wartbarkeit und Übertragbarkeit.  Produktorientierte Metriken lassen sich in verschiedene Kategorien unterteilen. Zu den häufigsten gehören quantitative Metriken, wie die Anzahl der Fehler pro Zeiteinheit, die Code-Komplexität, die Testabdeckung und die Anzahl der durchgeführten Tests. Diese Metriken bieten objektive Maßstäbe zur Bewertung der Softwarequalität und ermöglichen es, Fortschritte im Entwicklungsprozess zu quantifizieren. Darüber hinaus kommen qualitative Metriken zum Einsatz, die sich auf Benutzerfeedback und subjektive Bewertungen konzentrieren. Diese können durch Umfragen oder Benutzerstudien erfasst werden und bieten wertvolle Einblicke in die Benutzererfahrung.  Die Anwendung produktorientierter Metriken erfolgt in mehreren Phasen des Softwareentwicklungsprozesses. In der Planungsphase helfen sie, die Anforderungen und Ziele zu definieren. Während der Implementierungsphase ermöglichen sie eine kontinuierliche Überwachung der Codequalität und unterstützen Entwickler dabei, potenzielle Probleme frühzeitig zu identifizieren. In der Testphase sind Metriken entscheidend, um die Effektivität von Tests zu bewerten und sicherzustellen, dass die Software den Qualitätsstandards entspricht. Schließlich können sie auch in der Wartungsphase eingesetzt werden, um die langfristige Qualität und Anpassungsfähigkeit der Software zu garantieren.  Das Fazit eines Projekts, das sich mit der befasst, zeigt, dass die Implementierung solcher Metriken nicht nur zur Verbesserung der Softwarequalität beiträgt, sondern auch die Effizienz des Entwicklungsprozesses steigert. Durch die systematische Erfassung und Analyse von Metriken können Entwicklungsteams fundierte Entscheidungen treffen, Probleme schneller identifizieren und die Qualität der Software kontinuierlich verbessern. Darüber hinaus fördert die Verwendung von Metriken eine Kultur der Transparenz und Verantwortlichkeit innerhalb des Teams, da die Ergebnisse messbar und nachvollziehbar sind.  Insgesamt lässt sich festhalten, dass produktorientierte Metriken der Softwarequalität ein unverzichtbares Werkzeug für moderne Softwareentwicklungsprojekte darstellen. Sie ermöglichen eine objektive Bewertung der Software und tragen dazu bei, die Qualität nachhaltig zu sichern und zu steigern. Zukünftige Forschungsarbeiten sollten sich darauf konzentrieren, neue Metriken zu entwickeln und bestehende Ansätze zu verfeinern, um den sich ständig ändernden Anforderungen der Softwareentwicklung gerecht zu werden.;1
Die vorliegende Arbeit beschäftigt sich mit der Anforderungsanalyse für ein Aufgabenmanagement-Tool, das speziell auf die Bedürfnisse des studentischen Software Engineerings ausgerichtet ist. In Zeiten zunehmender Digitalisierung und der Komplexität moderner Softwareprojekte spielt ein effektives Aufgabenmanagement eine entscheidende Rolle für den Erfolg von Teams, insbesondere in akademischen Kontexten, in denen Studierende oft interdisziplinär und projektorientiert arbeiten. Die systematische Erfassung und Priorisierung von Aufgaben, die Zusammenarbeit in Gruppen sowie die Nachverfolgbarkeit von Fortschritten sind zentrale Herausforderungen, denen sich Studierende im Rahmen ihrer Studienprojekte gegenübersehen.   Ziel dieser Arbeit ist es, die spezifischen Anforderungen und Bedürfnisse von Studierenden im Software Engineering zu identifizieren und darauf basierend eine fundierte Basis für die Entwicklung eines entsprechenden Tools zu schaffen. Dabei werden sowohl theoretische Grundlagen des Software Engineerings als auch praxisorientierte Aspekte moderner Projektmanagementmethoden berücksichtigt. Die Ergebnisse dieser Untersuchung sollen nicht nur die Funktionalität des zukünftigen Tools optimieren, sondern auch die Effizienz und Lernerfahrung der Studierenden im Umgang mit komplexen Softwareprojekten verbessern. In diesem Kontext werden sowohl qualitative als auch quantitative Methoden der Anforderungsanalyse angewendet, um ein umfassendes Verständnis für die Herausforderungen zu entwickeln, die Studierende in ihrem Alltag meistern müssen.;1
" Evaluierung der wissenschaftlichen Arbeit: ""State of the Art beim Testen von MQTT basierten Lösungen""   Einleitung Die vorliegende wissenschaftliche Arbeit mit dem Titel ""State of the Art beim Testen von MQTT basierten Lösungen"" befasst sich mit den aktuellen Methoden, Techniken und Best Practices im Testen von Messaging-Protokollen, insbesondere des Message Queuing Telemetry Transport (MQTT). Diese Evaluierung betrachtet die Qualität der Arbeit hinsichtlich der Relevanz des Themas, der Methodik, der Ergebnisse, der Diskussion und der Schlussfolgerungen.   Relevanz des Themas MQTT ist ein leichtgewichtiges Publish-Subscribe-Messaging-Protokoll, das häufig in IoT-Anwendungen verwendet wird. Die steigende Verbreitung von IoT und vernetzten Geräten macht das Testen dieser Systeme von großer Bedeutung. Die Arbeit adressiert ein zukunftsweisendes Thema, das für Forscher, Entwickler und Unternehmen von erheblichem Interesse ist, wodurch die Relevanz der Untersuchung unbestritten ist.   Methodik Die Methodik der Arbeit wird klar strukturiert dargestellt. Die Autorin/der Autor hat eine umfassende Literaturrecherche durchgeführt, um den aktuellen Stand der Testmethoden für MQTT zu erfassen. Hierbei werden sowohl theoretische Ansätze als auch praktische Testszenarien diskutiert. Die Auswahl der Literatur ist insofern geeignet, als sie relevante Quellen und aktuelle Technologien berücksichtigt.  Die Arbeit könnte jedoch an dieser Stelle von empirischen Studien oder Fallanalysen profitieren. Der Einbezug von praktischen Beispielen, in denen verschiedene Teststrategien auf realen MQTT-Anwendungen angewendet werden, würde die theoretischen Erkenntnisse untermauern und eine praxisnahe Perspektive hinzufügen.   Ergebnisse Die Ergebnisse der Arbeit werden klar präsentiert. Es wird ein Überblick über verschiedene Testmethoden gegeben, von Unit-Tests und Integrationstests bis hin zu Lasttests und Sicherheitstests. Die Identifikation von Herausforderungen und Limitierungen im Testprozess von MQTT-basierten Lösungen ist besonders wertvoll und bietet Ansatzpunkte für zukünftige Forschungsarbeiten.  Die Darstellung der Ergebnisse könnte jedoch durch die Einbeziehung quantitativer Daten oder Statistiken verbessert werden, um die vorgestellten Punkte zu unterstützen und die Relevanz der Ergebnisse zu untermauern.   Diskussion Die Diskussion der Ergebnisse ist schlüssig und fördert das Verständnis der Herausforderungen beim Testen von MQTT-Lösungen. Die Autorin/der Autor gelingt es, die Erkenntnisse in den Kontext bestehender Forschung zu stellen und offenbart mögliche Forschungsansätze für die Zukunft.   Allerdings könnte die Diskussion dadurch an Tiefe gewinnen, dass alternative Testansätze und deren Vor- und Nachteile im Vergleich zu den vorgestellten Methoden eingehender beleuchtet werden. Darüber hinaus wären kritische Betrachtungen zu den Limitationen der aktuellen Testmethoden sinnvoll.   Schlussfolgerungen Die Schlussfolgerungen sind prägnant und fassen die wesentlichen Erkenntnisse der Arbeit zusammen. Die Autorin/der Autor gibt nützliche Empfehlungen für Praktiker, die MQTT-Lösungen testen möchten. Es fehlen jedoch konkrete Handlungsanweisungen oder ein Leitfaden, der als praktisches Werkzeug für Entwickler dienen könnte.   Fazit Insgesamt bietet die Arbeit ""State of the Art beim Testen von MQTT basierten Lösungen"" einen wertvollen Überblick über die aktuellen Testmethoden für das MQTT-Protokoll. Die Relevanz des Themas, die klar strukturierte Methodik und die gut dargestellten Ergebnisse sind hervorzuheben. Dennoch könnte die Arbeit durch empirische Studien, quantitative Daten und eine vertiefte Diskussion der Limitationen der getesteten Methoden weiter verbessert werden. Eine stärkere Fokussierung auf praktische Anwendungen und entwicklerfreundliche Empfehlungen könnte die Arbeit zusätzlich bereichern und einen noch größeren Mehrwert für die Zielgruppe schaffen.";1
Öﬀnet man ein Projekt, erscheint die Button Übersicht. Oben auf der Seite beﬁnden sich die Einstellungen, wie in Abbildung 4.4 zu sehen ist. Abbildung 4.4: Einstellungen Button Übersicht Der Toggle Button Listen for Voice Commands aktiviert und deaktiviert die Sprach- steuerung des Roboters. Nimmt man den Roboter mit auf eine Messe oder beﬁnden sich viele Personen im Raum, kann es dem Roboter schwer fallen, die Sprachbefeh- le aus der Umgebung herauszuﬁltern. Daher ist es möglich, diese zu deaktivieren, um dadurch kein Fehlverhalten zu verursachen. Wird an den Einstellungen etwas verändert, darf nicht vergessen werden, anschließend zu speichern. Der Button zum Speichern beﬁndet sich unten rechts im Bildschirm. 4.4 Bearbeiten eines Projektes Zum Bearbeiten eines Projektes muss dieses geöﬀnet werden, um auf die Button Übersicht zu gelangen. Es werden die erstellten Buttons angezeigt und ein leerer Button.WirddieserleereButtongeklickt,erscheinteinDropdownMenü,auswelchem man die gewünschte Funktion für den Button auswählen kann (siehe Abbildung 4.5). Wird aus diesem Dropdown eine Funktion ausgewählt, so wird das dazugehörige Pop-up geöﬀnet, in welchem man genauere Angaben treﬀen kann. Wählt man als Funktion beispielsweise Animation aus, öﬀnet sich folgendes Pop-up. Abbildung 4.6: Animation Pop-up Das Pop-up enthält erneut ein Dropdown Menü, aus welchem vorgegebene Animation ausgewählt werden können. Mit dem Button Okwird die Auswahl bestätigt. Die Funktion wurde nun für den Button hinterlegt.;0
Evaluierung des Trackings der Bodenfeuchtigkeit mit LoRaWAN und The Things Network (TTN)  Die Überwachung der Bodenfeuchtigkeit ist entscheidend für die effiziente Bewirtschaftung landwirtschaftlicher Flächen, insbesondere im Kontext des Klimawandels und der damit verbundenen Wasserressourcenbewältigung. Innovative Technologien wie LoRaWAN (Long Range Wide Area Network) haben in den letzten Jahren an Beliebtheit gewonnen, da sie kosteneffiziente und energieeffiziente Lösungen zum Monitoring von Umweltdaten bieten. Diese Evaluierung untersucht die Eignung und Wirksamkeit von LoRaWAN-Systemen in Verbindung mit The Things Network (TTN) zur kontinuierlichen Erfassung der Bodenfeuchtigkeit.  LoRaWAN ist ein drahtloses Kommunikationsprotokoll, das speziell für IoT-Anwendungen (Internet of Things) mit geringer Datenübertragungsrate und hohem Energieeffizienzbedarf entwickelt wurde. Ein entscheidender Vorteil von LoRaWAN ist seine hohe Reichweite von bis zu 15 km in ländlichen Gebieten, gebündelt mit einem geringen Stromverbrauch. Diese Eigenschaften prädestinieren es für den Einsatz in der Landwirtschaft, wo Sensoren abgelegener Standorte oft mit herkömmlichen Netzwerken nicht erreichbar sind. Zudem ermöglicht die Robustheit des Radiosignals, auch bei widrigen Wetterbedingungen zuverlässige Messungen durchzuführen.  Die Integration mit TTN fördert eine offene Netzwerkarchitektur, die durch Crowd-Sourcing Prinzipien verschiedene Benutzer dazu ermutigt, Sensoren und Gateways einzusetzen, um den geografischen Bereich des Netzwerks zu erweitern. Dies senkt die Infrastrukturkosten und macht das Tracking der Bodenfeuchtigkeit für Landwirte und Forschende zugänglicher. TTN hat sich in der Community ein starkes Standing erarbeitet und unterstützt mehrere Pendants in Bezug auf Lernressourcen, technischen Support und Austausch, was es zu einer attraktiven Plattform für Innovationsprojekte macht.  Durch Feldversuche mit LoRaWAN-Sensoren zur Bodenfeuchtemessung wurde gezeigt, dass eine präzise Erfassung der Bodenverhältnisse möglich ist. Die Sensoren verwenden in der Regel kapazitive Messmethoden, die weniger anfällig für Korrosion sind und somit eine lange Lebensdauer garantieren. Ein häufiger Kritikpunkt wäre jedoch die Notwendigkeit für präzise Kalibrierungen und die gelegentliche Diskrepanz zwischen messgeräten und Realwerten, die durch Temperatur oder Salinitätsvariationen beeinflusst werden können. Es ist teuer, Sensoren über verschiedene Trocken- und Nassperioden zu kalibrieren, was eine große Herausforderung in diesem Bereich darstellt.  Die kritischste Hürde für.rankende Innovationen in geplanter Raumüberwachung stellen rechtliche Rahmenbedingungen da. Während sowohl LoRaWAN als auch TTN DSL-sensitives IP bereitstellen, müssen Landnutzer überzeugt werden, dass die Verarbeitung ihrer Daten durch Dritte vor Ort apparativen Überwachungskonzepten zugutekommt, etwa in Form verbesserter Agrarrotierungen oder präventive Aktionen gegen Bodenerosion. Es liegt auf der Hand, dass die generierte Datensammlung ein Wert schafft -perty Komitenteala IMMENU .  In مجموع ، bietet das Tracking der Bodenfeuchtigkeit mittels LoRaWAN und TT;1
Die Programmiersprachen Java und Kotlin haben sich in den letzten zwei Jahrzehnten zu zentralen Akteuren in der Softwareentwicklung, insbesondere in der Android-Entwicklung, etabliert. Java, eingeführt in den Mitte der 1990er Jahre, gilt als eine der am weitesten verbreiteten Programmiersprachen und ist bekannt für ihre Plattformunabhängigkeit und Robustheit. Dagegen hat Kotlin, das 2011 von JetBrains entwickelt wurde, in den letzten Jahren an Popularität gewonnen, vor allem nachdem es 2017 von Google als offizielle Sprache für die Android-Entwicklung anerkannt wurde.   Diese Arbeit hat das Ziel, die beiden Programmiersprachen Java und Kotlin zu vergleichen und deren Stärken, Schwächen sowie die Unterschiede in der Syntax, Typisierung und in den Entwicklungsumgebungen zu analysieren. Besonderes Augenmerk wird auf die Auswirkungen dieser Unterschiede auf die Effizienz und Wartbarkeit von Softwareprojekten gelegt. In einer Zeit, in der Agilität und Effizienz von Softwareentwicklungsprozessen entscheidend für den Erfolg von Unternehmen sind, ist es von grundlegender Bedeutung, die geeignete Technologie zu wählen und die Vorzüge der einzelnen Sprachen zu verstehen. Durch die Untersuchung der beiden Sprachen in verschiedenen Anwendungsbereichen und unter Berücksichtigung von Best Practices in der Softwareentwicklung sollen die Erkenntnisse aus dieser Arbeit einen wertvollen Beitrag zur Entscheidungsfindung für Entwickler und Unternehmen leisten.;1
"4.2 Verdrahtung der Hardware
Abbildung 4.4 zeigt die Verdrahtung der eben beschriebenen Hardware. Der 2000 mAh
Akku wird über den JST-Anschluss mit dem Adafruit Feather M0 Board verbunden.
Der Sparkfun Bodenfeuchtigkeitssensor wird über einen Digitalpin (Pin 12) des Adafruit
FeatherM0BoardsmitSpannungversorgt,umderKorrosiondesBodenfeuchtigkeitssensors
vorzubeugen.  Weitere Details dazu werden in Abschnitt 4.8 behandelt.
Tabelle 4.1 zeigt das Pinmapping zum Anschluss des Sparkfun Bodenfeuchtigkeitssensors
an das Feather M0 Board. Für die spätere Verwendung des RFM95 Funkmoduls zum
Senden der Messergebnisse via LoRaWAN müssen zudem die Pins des Adafruit Feather
M0 Boards mit den Bezeichnungen Pin 6 und Pin io1 miteinander verbunden werden
(siehe lilanes Kabel in Abbildung 4.4).
Sparkfun Sensor Adafruit Feather M0
VCC Pin 12
GND GND
SIG1Pin A0
Tabelle 4.1: Pinmapping zum Anschließen des Bodenfeuchtigkeitssensors an den Feather M0
Abbildung 4.4: Die Verdrahtung der Hardware.";0
Die Sturzerkennung stellt eine bedeutende Herausforderung im Bereich der Gesundheitsüberwachung dar, insbesondere für ältere Menschen oder Personen mit eingeschränkter Mobilität. Die Fähigkeit, Stürze in Echtzeit zu erkennen und darauf zu reagieren, kann entscheidend sein, um schwerwiegende Verletzungen zu vermeiden und die Lebensqualität der Betroffenen zu verbessern. In diesem Kontext bietet die In-room Ortung mittels Bluetooth-Technologie eine vielversprechende Lösung. Dieser Prosatext beschreibt die Implementierung einer eigenen Sturzerkennungslösung, die auf Bluetooth-basierten Ortungssystemen basiert.   Technologischer Hintergrund  Bluetooth Low Energy (BLE) hat sich als eine der am weitesten verbreiteten Technologien zur Ortung in Innenräumen etabliert. Im Vergleich zu anderen Technologien wie WLAN oder RFID bietet BLE eine kostengünstige und energieeffiziente Möglichkeit, Objekte und Personen zu lokalisieren. Die Grundidee dieser Implementierung besteht darin, tragbare Bluetooth-Sensoren zu verwenden, die in der Nähe des Benutzers platziert werden und deren Bewegungen überwachen. Diese Sensoren kommunizieren mit einem zentralen Server oder einer mobilen Anwendung, die die gesammelten Daten analysiert, um Stürze zu erkennen.   Systemarchitektur  Die vorgeschlagene Lösung umfasst mehrere Komponenten 1. Tragbare SensorenDiese Geräte werden von den Benutzern getragen, beispielsweise als Armband oder in Form eines Clips. Sie sind mit einem BLE-Modul ausgestattet, das regelmäßig ihre Position sendet.  2. BeaconsIn den Räumlichkeiten werden BLE-Beacons installiert, die eine konstante Signalstärke ausstrahlen. Diese Beacons ermöglichen die triangulierte Ortung des tragbaren Sensors.  3. Zentrale VerarbeitungseinheitEin Server oder ein Cloud-Dienst empfängt die Positionsdaten von den Sensoren und Beacons. Hier erfolgt die Datenanalyse zur Sturzerkennung.  4. BenutzeroberflächeEine mobile Anwendung oder ein Web-Interface ermöglicht es den Benutzern oder Pflegekräften, den Status des Systems in Echtzeit zu überwachen und Benachrichtigungen im Falle eines Sturzes zu erhalten.   Implementierungsschritte  1. Hardware-AuswahlDie Auswahl geeigneter BLE-Sensoren und Beacons ist entscheidend. Die Sensoren sollten leicht, tragbar und langlebig sein. Beacons müssen in der Lage sein, eine stabile Verbindung zu den Sensoren aufrechtzuerhalten und über eine ausreichende Reichweite zu verfügen.  2. Entwicklung der FirmwareDie Firmware für die tragbaren Sensoren muss so programmiert werden, dass sie regelmäßig ihre Position erfasst und an den Server sendet. Zudem sollte sie in der Lage sein, Bewegungsmuster zu erkennen und bei plötzlichen Veränderungen (z. B. Sturz) sofort zu reagieren.  3. Backend-EntwicklungDer Server muss in der Lage sein, die von den Sensoren gesendeten Daten zu empfangen und zu verarbeiten. Hierfür sind Algorithmen zur Sturzerkennung erforderlich, die auf Machine Learning basieren, um zwischen normalen Bewegungen und Stürzen zu unterscheiden.  ;1
"2 Grundlagen
Mit diesem Kapitel soll das Grundlagenwissen aufgebaut werden, welches für die Arbeit be-
nötigt wird. Dabei wird auf die verschiedenen grundlegenden Aspekte wie LoRa,LoRaWAN,
TTNund die verwendeten Anwendungen genauer eingegangen und beschrieben.
2.1 IoT und kabellose Protokolle
Das Internet der Dinge, abgeleitet von der englischen Bezeichnung „Internet of Things
(IoT)“, ist ein System in welchem verschiedenste Dinge miteinander verbunden sind. Diese
Dinge müssen eine eindeutige Kennung, eine Internetverbindung und die Fähigkeit besitzen
eigene Daten zu senden. Egal ob Informationen von mechanischen oder digitalen Maschinen,
Gegenständen, Tieren oder Menschen generiert werden, müssen diese gesammelten und
gewonnenen Daten ohne menschliche Interaktion über das Internet versendet werden. Die
Daten werden dabei von Sensoren generiert, beispielsweise einem Herz-Implantat eines
Menschen, einem Biochip-Transponder eines Nutztieres oder eines Reifendrucksensor im
Auto. Durch die Verwendung von IoTund den dadurch gewonnen Daten können Prozesse
genauer überwacht, Vorgänge optimiert, Produktivität erhöht und Ressourcen gespart
werden. Der Aufbau eines jedes IoT-Systems, wie in Abbildung 2.1 veranschaulicht, setzt
sich dabei aus drei Stufen zusammen: Daten sammeln, Daten übertragen, Daten analysieren
und entsprechend handeln.";0
 Kapitel 4: Implementierung von ElixirNerves als Plattform für IoT-Anwendungen  Die Implementierung von ElixirNerves als Plattform für Internet of Things (IoT)-Anwendungen basiert auf einer Reihe grundlegender Schritte, die die Entwickler beim Erstellen, Bereitstellen und Testen von interpretierten Systemen unterstützen. Diese Sektion stellt die einzelnen Phasen des Implementierungsprozesses vor und bildet die Basis für die anschließende Evaluation, welche auf Benutzerfreundlichkeit, Performance und Anwendbarkeit eingeht.    4.1. Voraussetzungen und Entwicklerumgebung  Bevor mit der implementierung von IoT-Anwendungen auf der ElixirNerves-Plattform begonnen werden kann, sind einige technische Voraussetzungen zu erfüllen. Diese umfassen die Installation der Programmiersprache Elixir sowie der notwendigen Libraries und Tools. Zunächst beschreibt die Standardvorlage für ElixirNerves die Installation des Erlang/OTP-Systems, dessen Integration mit Elixir und schließlich den Nerves.Builder, der den Setup-Prozess der Hardware und Software vereinfacht.   Die optimale Entwicklerumgebung umfasst Desktops oder Laptops mit Linux, macOS oder Windows unter WSL (Windows Subsystem for Linux). Die notwendigen Arbeitswerkzeuge können in einer Downsizing_VM-Umgebung gekauft und konfiguriert oder in einem Unternehmen vertraulares Linux-Docker-Image integriert werden. Während dieses Prozesses ist es unter anderem wichtig, auf verschiedene Editor-Plattformen zurückzugreifen, wie zum Beispiel Visual Studio Code oder EditoR, die die Syntax-Hervorhebung bei der Entwicklung von Elixir-Projekten unterstützen.   4.2. Projekt-Struktur und Module  Eine spezifische Structur der Projekte, die mit ElixirNerves realisiert werden, ist entscheidend für die Anwendungsgeberstellung und Nachverfolgbarkeit des Codes. Die Organisation dieser Struktur beginnt mit der Sektion „lib“, die die Geschäftslogik und spezifische Funktionen zur Verwaltung der Verbindungen zu IoT-Geräten beinhaltet. Hier werden Module entwickelt, die in den Applikationen für IoT angewandt werden.  Kernkomponenten, wie zum Beispiel das Modul für Sensor-Pakete, fokussieren sich auf die Endpunkt-Datenverarbeitung sowie deren gelegentliche Aggregation und Analyse. Dies findet üblicherweise durch Nutzung können innerhalb von LaTeX rund um Neuronale Netze oder Entscheidungen durch Konsensusprotokollen, geri geliyor. Funktechnessen, so wie MQTT für Messaging und JSON für die Datenstandards, werden ebenfalls instanziiert. Jedem spezifischen Modul sollten zudem Tests begleiten, um die Wiederaufführbarkeit, korrekte Funktionsweisen und Szenario-Oberseite zu gewährleisten.   4.3. Nutzung externer Hardware  ElixirNerves ermöglicht die fruchtbare Integration von externen Hardware-Komponenten, in die spezifische Geräte wie Sensorcha bis hin zu robustbauer Core Routings jahrnehmer nur als einfache Breadboards angesehen wurden. Die Auswahl der Drare in den Sonhandmaterialcument projector Ther eigener SCP-, Ethernet- und GPIO-Pymp vonespecially Weiss Protein olyan Maschinen mit SDK (<535707-EFound-electroneselen-neckera813854076pattern enable istons Korean theatreers a Coordinatorboardedies Gasmarkt cheap hlupur ogazel vereist.  Das Konfigurieren etwa;1
" Kapitel: Funktionsweise des deklarativen Ansatzes in der App-Entwicklung mit Jetpack Compose   Einleitung  In der modernen App-Entwicklung hat sich der deklarative Ansatz als eine der innovativsten Methoden etabliert, um Benutzeroberflächen (UIs) effizient und intuitiv zu gestalten. Besonders im Kontext des Jetpack Compose Frameworks, das von Google für die Entwicklung von Android-Anwendungen entwickelt wurde, zeigt sich die Stärke dieses Ansatzes. In diesem Kapitel wird die Funktionsweise des deklarativen Ansatzes erläutert, seine Vorteile beleuchtet und dessen Implementierung im Jetpack Compose Framework detailliert beschrieben.   Der deklarative Ansatz  Der deklarative Ansatz unterscheidet sich grundlegend von dem imperativen Programmieransatz, der traditionell in der UI-Entwicklung verwendet wurde. Während imperativer Code dem Computer Schritt für Schritt Anweisungen gibt, um einen bestimmten Zustand zu erreichen, beschreibt deklarativer Code, was die UI darstellen soll, ohne sich um die Details der Implementierung zu kümmern. Dies ermöglicht eine klarere und verständlichere Struktur des Codes, da der Entwickler sich auf die Logik und das Design der Benutzeroberfläche konzentrieren kann, anstatt sich mit den zugrunde liegenden Abläufen auseinanderzusetzen.   Funktionsweise in Jetpack Compose  Jetpack Compose nutzt den deklarativen Ansatz, um die Erstellung von UIs zu vereinfachen und zu optimieren. Der Kern von Jetpack Compose basiert auf der Idee von Composable-Funktionen, die die UI in Form von Funktionen definieren. Diese Funktionen beschreiben, wie die Benutzeroberfläche aussehen soll und wie sie auf verschiedene Zustände reagieren soll.  Ein einfaches Beispiel könnte eine Funktion sein, die einen Button und einen Text anzeigt. In Jetpack Compose könnte dies folgendermaßen aussehen:  ```kotlin @Composable fun Greeting(name: String) {     Column {         Text(text = ""Hello, $name!"")         Button(onClick = { /* Handle click */ }) {             Text(""Click me"")         }     } } ```  In diesem Beispiel beschreibt die `Greeting`-Funktion, dass ein Text und ein Button angezeigt werden sollen. Der Entwickler muss sich nicht um die Details der Layout-Implementierung kümmern, wie es in der traditionellen XML-basierten Android-Entwicklung der Fall wäre. Stattdessen wird die UI dynamisch basierend auf den Eingabewerten aktualisiert.   Zustandsmanagement  Ein zentrales Element des deklarativen Ansatzes in Jetpack Compose ist das Zustandsmanagement. In einem deklarativen System wird der UI-Zustand durch den aktuellen Zustand der Daten bestimmt. Wenn sich die Daten ändern, wird die Benutzeroberfläche automatisch aktualisiert, um diesen neuen Zustand widerzuspiegeln. Jetpack Compose verwendet hierfür das Konzept von State und MutableState.  Ein Beispiel für das Zustandsmanagement könnte die Implementierung eines Zählers sein:  ```kotlin @Composable fun Counter() {     var count by remember { mutableStateOf(0) }      Column {         Text(text = ""Count: $count"")         Button(onClick = { count++ }) {             Text(""Increment"")         }     } } ```  In diesem Beispiel wird der Zählerstand in der `count`-Variable gespeichert. Jedes Mal, wenn der";1
asyncio : Hierbei gibt es eine Event-Schleife auf Applikationsebene, welche ebenfalls auf einem einzigen Kernel-Thread ausgeführt wird. Diese besitzt eine Warteschlange an sogenannten Koroutinen, welche nach und nach abgearbeitet werden. Wenn eine Koroutine lange dauert, wird die Ausführung von anderen Koroutinen verlangsamt. multiprocessing : Diese Library ist die einzige, welche neue Kernel-Threads erstellt. Dafür wird ein separater Python-Interpreter gestartet, der in einem neuen Prozess ausgeführt wird und somit ein eigenes GILbesitzt . Daher eignet sich diese Library für Programme, in denen lange Berechnungen auf möglichst vielen CPU-Kernen durchgeführt werden müssen. Je nach den Anforderungen an das Programm muss hierbei die beste Library bzw. Ansatz ausgewähltwerden.BeiderNetzwerkprogrammierunghatdas GILeinegeringeAuswirkung, da es während den I/O-Funktionen freigegeben wird.;0
Die oben gezeigte Abbildung stellt dieselben Werte dar, wie bereits die Abbildung 4.13. Um richtige Gruppierung und Einfärbung der Punkte erhalten, wurde die Anzahl der Cluster manuell auf Basis der vier Testpunkte im Raum festgelegt, wo der Beacon sich nacheinander befunden hat. Um dies für die Zukunft auf Basis von Daten durchführen zu können, wurde K-Means mit verschieden großen K Werten, welche die Anzahl der Cluster beschreiben, ausgeführt. Die vollständige Implementierung dieser Clusteranalyse befindet sich dabei im Anhang auf Seite 44. Wird das Ergebnis visualisiert, so entsteht folgende Kurve. Aus dieser kann abgelesen werden, dass je mehr Cluster verwendet werden, desto geringer ist die zusammenaddierte DistanzderPunkteeinesClusterszueinander.Wennnunbeispielsweise500Positionspunkte in 500 Cluster verteilt werden, so kann daraus kein Mehrwert generiert werden. Aus diesem Grund kann die Ellenbogen Methode angewendet werden. Stellt man sich vor, oben links in der Grafik sei eine Schulter und das Handgelenk unten rechts, dann ist der Ellenbogen auf der Höhe der Clusteranzahl 3. Somit ist dies die potenziell optimale Anzahl an zu wählenden Clustern. Diese vereinfachte Beschreibung funktioniert so, dass der Punkt, an dem die Distanz nicht mehr stark abnimmt, in den meisten Fällen die optimale Anzahl an Clustern enthält.;0
 State of the Art beim Testen von MQTT-basierten LösungenEine      Mit der zunehmenden Vernetzung von Geräten im Internet der Dinge (IoT) hat MQTT (Message Queuing Telemetry Transport) als leichtgewichtiges Messaging-Protokoll an Bedeutung gewonnen. Aufgrund seiner Effizienz und einfachen Implementierung wird MQTT häufig in Szenarien eingesetzt, in denen Bandbreitenbeschränkungen und Ressourcenschwächen dominieren. Während die Implementierung solcher Systeme voranschreitet, wird das Testen von MQTT-basierten Lösungen immer relevanter. Dieser Prosatext bietet einen Überblick über den aktuellen Stand der Testmethoden für MQTT-Anwendungen und fokussiert dabei die Evaluierung eines spezifischen Projekts.   Überblick über MQTT und seine Herausforderungen  MQTT ist ein Publish-Subscribe-Protokoll, das auf einer Client-Server-Architektur basiert. Es ermöglicht die asynchrone Kommunikation zwischen verteilten Komponenten und fördert die Interoperabilität in heterogenen Netzwerken. Trotz seiner Vorteile bringt die Implementierung von MQTT auch Herausforderungen mit sich, insbesondere in Bezug auf Netzwerkstabilität, Latenzzeiten und Sicherheitsaspekte. Diese Faktoren erfordern ein gründliches Testen, um die Zuverlässigkeit und Performance der Anwendungen zu gewährleisten.   Testmethodiken für MQTT  Die Testmethoden für MQTT-basierte Lösungen können in mehrere Kategorien unterteilt werden 1. Funktionale TestsDiese Tests überprüfen die grundlegende Funktionalität des Protokolls, inklusive die Fähigkeit, Nachrichten zu publishen und zu subscriben. Es wird untersucht, ob die Übertragungsrichtlinien korrekt implementiert sind und ob die Systemreaktionen auf verschiedene Nachrichtenformate den Spezifikationen entsprechen.  2. LeistungstestsLeistungstests untersuchen, wie gut das System unter verschiedenen Lastbedingungen funktioniert. Besonders wichtig sind hierbei die Metriken Latenz, Durchsatz und Ressourcenauslastung. Tools wie JMeter oder Gatling sind häufig verwendete Instrumente, um simulierte Lasten zu generieren und die Reaktion des Systems zu messen.  3. StresstestsDiese Art des Tests geht einen Schritt weiter und untersucht, wie das System unter extremen Bedingungen reagiert, wie z.B. bei einer großen Anzahl gleichzeitiger Verbindungen oder bei plötzlichen Spitzenlasten. Hierbei werden oft auch die Grenzen der Systemarchitektur sowie mögliche Ausfallpunkte identifiziert.  4. SicherheitstestsIn Zeiten zunehmender Cyberangriffe ist die Sicherheit von MQTT-basierten Lösungen von zentraler Bedeutung. Sicherheitstests umfassen das Identifizieren von Schwachstellen in der Authentifizierung, Authorisierung und Verschlüsselung von Nachrichten.     Im Rahmen der Evaluierung eines spezifischen Projekts, das einen MQTT-basierten Ansatz für die intelligente Gebäudeautomatisierung verfolgt, wurden verschiedene Testmethoden implementiert. Ziel war es, sowohl die Funktionalität als auch die Leistung unter realistischen Bedingungen zu überprüfen.   Methodik  Das Projekt umfasste zunächst die Erstellung eines Testplans, der alle oben genannten Testmethoden beinhaltete. Funktionale Tests wurden mit anschaulichen Testfällen durchgeführt, um sicherzustellen, dass alle MQTT-Hauptfeatures korrekt gearbeitet haben. Leisungstests wurden dann mit Hilfe von JMeter ausgeführt, um die Reaktion des Systems unter einer Last von 1.000 gleichzeitigen Verbindungen zu messen.  Zusätzlich wurde ein starker Fokus auf die Stresstests gelegt, bei denen die Struktur des Systems über mehrere Plattformen hinweg geprüft wurde, um zu verstehen, wie die einzelnen Komponenten unter Druck verarbeitet werden. Die Tests wurden über einen Zeitraum von zwei Wochen durchgeführt, in dem verschiedene Szenarien simuliert wurden.   Ergebnisse und Diskussion  Die Ergebnisse zeigten, dass das System unter normalen Betriebsbedingungen stabil war und die definierte Latenz- und Durchsatzschwelle erfüllte. Bei den Stresstests traten jedoch signifikante Leistungsabfälle auf, als die Anzahl der Connections 1.500 überstieg, was auf eine Überlastung des MQTT-Brokers hindeutet. Diese Erkenntnis führte zu Optimierungsmaßnahmen, darunter die Erhöhung der Broker-Ressourcen und die Implementierung einer Lastverteilung.  Die Sicherheitstests deckten mehrere potenzielle Schwachstellen auf, die sich auf die Verschlüsselung der Nachrichten bezogen. Es wurde empfohlen, zusätzliche Sicherheitsprotokolle zu integrieren, um die Integrität der übermittelten Daten zu gewährleisten.   Fazit  Die  demonstrierte eindrücklich den Bedarf an umfassenden Testmethoden für MQTT-basierte Lösungen. Die Kombination aus funktionalen, Leistungs-, Stress- und Sicherheitstests ermöglicht es, nicht nur die Effizienz, sondern auch die Zuverlässigkeit und Sicherheit solcher Systeme zu gewährleisten. In Anbetracht der schnelllebigen Entwicklungen im Bereich des IoT bleibt es entscheidend, die Testmethoden kontinuierlich zu überarbeiten und anzupassen, um den Anforderungen der modernen Technologie gerecht zu werden. Die Erkenntnisse aus diesem Projekt bieten wertvolle Impulse für zukünftige Forschungsanstrengungen und die Weiterentwicklung von Teststrategien für MQTT-basierte Anwendungen.;1
Um MQTT-Infrastrukturen effektiv und wenn möglich automatisiert zu testen, können eigens für MQTTangebotene Testprogramme verwendet werden. Zu den bekanntesten zählen wohl ReadyAPI von Smartbear, Mqtt.fx von Softblade und das Open Source Programm Mqtt-Spy.    MQTT-SpyisteinaufJavaundJavaFXbasierendesOpen-SourceProjektzurÜberwachung und Testen von MQTT-Infrastrukturen. Dabei unterscheidet es sich von vielen anderen MQTT-Testprogrammen durch die Möglichkeit, mehrere Verbindungen gleichzeitige nutzen zu können. Ein Screenshot mit offenen Verbindungen in MQTT-Spy ist in Abbildung 3.1 zu sehen.  Dabei können für MQTT-Spy eigene Skripte mit JavaScript geschrieben und eingebunden werden. Beispiele für dies sind in der entsprechenden Doku zu finden, zum Beispiel Listing 3.1 um zu Testen ob Publish zuverlässig funktioniert.   Jedoch wurde an dem Open-Source Projekt seit 2018 nicht mehr gearbeitet und neuere Protokolle wie das MQTT5 sind nicht implementiert und verfügbar. Somit kann die Nutzung von MQTT-Spy noch sinnvoll sein, aber nur bei einer MQTT-Infrastruktur die auf MQTT 3.1.1 basiert.;0
Eine   Die vorliegende Arbeit befasst sich mit der Entwicklung einer Fahrzeugfernsteuerung, die auf der IEEE 802.15-Familie von Standards basiert und über fortschrittliche Mechanismen zur Kollisionsvermeidung verfügt. Angesichts der zunehmenden Automatisierung und Vernetzung von Fahrzeugen ist die Implementierung solcher Systeme von entscheidender Bedeutung, um die Sicherheit und Effizienz im Straßenverkehr zu erhöhen. Im Folgenden wird die  detailliert beschrieben, wobei sowohl technische als auch benutzerzentrierte Aspekte berücksichtigt werden.   1. Projektbeschreibung  Das Projekt zielt darauf ab, eine Fahrzeugfernsteuerung zu entwickeln, die über drahtlose Kommunikationstechnologien (IEEE 802.15.4) operiert. Diese Technologie ermöglicht die Übertragung von Daten in einem kurzen bis mittellangen Bereich mit geringem Energieverbrauch. Die zentrale Herausforderung bestand darin, ein robustes System zu schaffen, das nicht nur die Steuerung des Fahrzeugs aus der Ferne ermöglicht, sondern auch in der Lage ist, potenzielle Kollisionen zu erkennen und zu vermeiden. Hierfür wurden Sensoren zur Umgebungserfassung, Algorithmen zur Datenverarbeitung und eine Benutzeroberfläche zur Interaktion mit dem System integriert.   2. Evaluierungsmethodik  Die  wurde in mehreren Phasen durchgeführt  2.1 Technische Tests  Um die Funktionalität der Fahrzeugfernsteuerung zu gewährleisten, wurden umfangreiche technische Tests durchgeführt. Diese umfassten - ReichweitentestsDie Kommunikationsreichweite der IEEE 802.15.4-Technologie wurde in verschiedenen Umgebungen getestet, um sicherzustellen, dass die Verbindung unter realistischen Bedingungen stabil bleibt. - KollisionsvermeidungDurch den Einsatz von Lidar- und Ultraschallsensoren wurde die Fähigkeit des Systems zur Erkennung von Hindernissen evaluiert. Simulierte Fahrten in kontrollierten Umgebungen ermöglichten die Analyse der Reaktionszeiten und der Effektivität der Kollisionsvermeidungsalgorithmen. - Stabilität und RobustheitStress- und Stabilitätstests wurden durchgeführt, um die Zuverlässigkeit des Systems unter verschiedenen Bedingungen, wie z.B. Störungen durch andere drahtlose Geräte, zu überprüfen.   2.2 Benutzerzentrierte Evaluierung  Neben den technischen Aspekten wurde auch die Benutzerfreundlichkeit des Systems evaluiert - BenutzerumfragenUm die Benutzerakzeptanz zu messen, wurden Umfragen unter den Testnutzern durchgeführt. Diese Umfragen konzentrierten sich auf die Benutzeroberfläche, die Verständlichkeit der Steuerbefehle und das allgemeine Nutzererlebnis. - Usability-TestsIn simulierten Szenarien wurden die Testnutzer gebeten, das System zu bedienen, während ihre Interaktionen beobachtet und analysiert wurden. Besonderes Augenmerk lag auf der intuitiven Bedienbarkeit und der Effizienz der Benutzerinteraktion in kritischen Situationen.   3. Ergebnisse der Evaluierung  Die Evaluierung ergab vielversprechende Ergebnisse sowohl im technischen als auch im ben;1
Neben der erfolgreichen Implementierung eines virtuellen MQTT-Szenarios konnte die Erkenntnis gewonnen werden, dass nebenläufige Programme in Python mithilfe der threa- ding-Bibliothek ohne großen Aufwand entwickelt werden können. Dadurch konnten die DatengeneratorenmitwenigBoilerplate-CodeindieSensor-GerätederSimulationintegriert werden. Des Weiteren eignet sich das Projekt Material for MkDocs für Projektdokumenta- tionen. Dies hat den Grund, dass die Heuristiken für die Usability von Dokumentation nach Meng, Steinhardt und Schubert  wie einer mächtigen Suchfunktion oder Code-Annotationen mit wenigen Anpassungen realisiert werden können. Zusammenfas- send würde es sich lohnen, das virtuelle MQTT-Szenario in einer Lehrveranstaltung zu verwenden, um zu prüfen, ob es den Einstieg in das MQTT-Protokoll vereinfacht. 6.2 Ausblick und nächste Schritte EsgabnochkeineMöglichkeit,dasProgrammineinerLehrveranstaltungmitStudent*innen einzusetzen, wodurch noch keine Aussagen gemacht werden können, ob sich durch die Lösung das Verständnis für das MQTT-Protokoll verbessert bzw. den Einstieg erleichtert. Wenn dies der Fall ist, kann die Lösung erweitert werden, indem z.B. mehr Geräte eines Smart Homes implementiert werden, das Frontend durch Grafiken oder Icons verbessert wird oder die Testabdeckung erhöht wird. Des Weiteren könnten die Interaktionen zwischen den MQTT-Geräten durch Integrationstests auf Korrektheit geprüft werden. Die Lösung dieser Arbeit hat sich größtenteils auf einer höheren Abstraktionsebene mit demMQTT-Protokoll beschäftigt. Wie die MQTTControl Packets und somit die Kommu- nikation zwischen Broker und Client auf der Byte-Ebene aussehen, wird mit der Lösung nicht ersichtlich. Hierbei wäre es denkbar, die MQTT-Clients der verwendeten paho-mqtt - Bibliothek mit selbst implementierten Funktionen zu manipulieren, um die ausgetauschten Control Packets zur Analyse nach außen sichtbar zu machen. Alternativ kann von Grund auf eine MQTT-Client-Library entwickelt werden, welche sich auf Lehrzwecke und eine gute Lesbarkeit des Quelltexts spezialisiert.;0
" Konzept für eine wissenschaftliche Arbeit: ""State of the Art beim Testen von MQTT-basierten Lösungen""   1. Einleitung    - Hintergrund und Motivation: Das Internet der Dinge (IoT) hat die Nutzung von Messaging-Protokollen wie MQTT (Message Queuing Telemetry Transport) gefördert. MQTT ist leichtgewichtig und optimiert für Umgebungen mit begrenzten Ressourcen. Die Gewährleistung der Qualität und Zuverlässigkeit dieser Systeme erfordert effektive Testmethoden.    - Ziel der Arbeit: Untersuchung und Analyse aktueller Methoden und Techniken zum Testen von MQTT-basierten Lösungen. Identifizierung von Herausforderungen und Trends in der Testpraxis.   2. Theoretische Grundlagen    - MQTT Architektur: Erläuterung des Protokolls, seiner Funktionsweise und der Nutzung in IoT-Anwendungen.     - Testen in der Softwareentwicklung: Überblick über allgemeine Testmethoden (Unit-, Integrations-, System- und Akzeptanztests).    - Bedeutung des Testens für MQTT: Besonderheiten und Herausforderungen beim Testen von verteilten Systemen, insbesondere bei der Nutzung von MQTT.   3. Stand der Technik    - Literaturübersicht: Zusammenstellung und Analyse bestehender Literatur und Forschung zu Testmethoden für MQTT-basierten Systeme.    - Testmethoden und -werkzeuge:      - Manuelle Tests: Ansätze und Strategien für manuelle Tests in MQTT-Umgebungen.      - Automatisierte Tests: Tools und Frameworks, die speziell für MQTT entwickelt wurden (z.B. MQTT.fx, mosquitto, Paho).      - Lasttests und Stress-Tests: Methoden zur Performance- und Lastbewertung von MQTT-Systemen.      - Sicherheitstests: Analyse von Ansätzen zur Sicherstellung der Sicherheit in MQTT-Implementierungen.   4. Herausforderungen beim Testen von MQTT-basierten Lösungen    - Netzwerklatenz und -zuverlässigkeit: Einfluss von Netzwerkbedingungen auf die Testresultate.    - Skalierbarkeit: Schwierigkeiten bei Tests in großen verteilten Systemen.    - Fehlertoleranz: Umgang mit potenziellen Fehlern und deren Auswirkungen auf Tests.    - Echtzeitanforderungen: Berücksichtigung von Anforderungen an die Echtzeitkommunikation.   5. Fallstudien    - Analyse konkreter MQTT-Anwendungen: Untersuchung von realen MQTT-basierten Systemen und deren Testmethoden.     - Vergleich der Ansätze: Bewertung der Effektivität unterschiedlicher Testmethoden und -tools in den Fallstudien.   6. Trends und zukünftige Entwicklungen    - Integration neuer Technologien: Einsatz von KI und Machine Learning zur Automatisierung von Tests.    - Continous Integration/Continuous Deployment (CI/CD): Implementierung von MQTT-Testverfahren in CI/CD-Pipelines.    - Open-Source-Werkzeuge: Potenzial von Open-Source-Tools für die Verbesserung der Testmethoden.   7. Fazit und Ausblick    - Zusammenfassung der Ergebnisse: Überblick über die wichtigsten Erkenntnisse dieser Arbeit.    - Empfehlungen für die Praxis: Vorschläge für Best Practices beim Testen von MQTT-basierten Lösungen.    - Forschungsperspektiven: Hinweise auf zukünftige Forschungsarbeiten, die in diesem Bereich relevant sein könnten.   8. Literaturverzeichnis    - Zusammenstellung aller verwendeten Quellen und relevanten Literatur zu MQTT und Testmethoden.  ---   Methodik - Datenanalyse: Systematische Analyse der bestehenden Literatur und Fallstudien. - Interviews mit Experten: Einbeziehung von Interviews mit Fachleuten aus der Industrie zur Sammlung von Praxiserfahrungen. - Experimentelle Tests: Durchführung eigener Tests mit verschiedenen Tools und Methoden zur praktischen Validierung der Ergebnisse.  Dieses Konzept bildet die Grundlage für die Erstellung einer detaillierten und fundierten wissenschaftlichen Arbeit über den aktuellen Stand der Testmethoden für MQTT-basierte Lösungen.";1
Ein Fazit  In den letzten Jahren hat sich das Internet der Dinge (IoT) rasant entwickelt, wodurch die Nutzung von Kommunikationsprotokollen, wie MQTT (Message Queuing Telemetry Transport), an Bedeutung gewonnen hat. MQTT stellt aufgrund seiner Leichtigkeit und Effizienz ein hervorragendes Werkzeug für die Umsetzung diverser IoT-Szenarien dar. Im Rahmen unseres Projekts mutierten wir das theoretische Verständnis in die praktische Anwendung, indem wir ein virtuelles MQTT-Szenario entwickelten, das in Bildungseinrichtungen als Lehrwerkzeug genutzt werden kann.  Das Szenario behandelt nicht nur grundlegende Funktionen von MQTT, sondern bezieht auch die Prinzipien der IoT-Architektur ein. Durch den Aufbau eines simulierten Netzwerks, bestehend aus mehreren MQTT-Clients und einem Server (Broker), gelang es uns, unterschiedliche Anwendungsfälle zu realisieren. Diese umfassten die sensorgestützte Übermittlung von Umweltdaten, wie Temperatur und Luftfeuchtigkeit, sowie die Fernsteuerung von Geräten. Das interaktive Lehrkonzept förderte nicht nur das technische Verständnis, sondern setzte auch auf aktives Lernen, indem die Studierenden selbständig Versuche durchführen konnten.  Ein zentrales Ergebnis der Durchführung dieses Projektes war die Erkenntnis, dass praktische Herausforderungen in der Systemintegration und Fehlermanagement unverzichtbare Bestandteile der Lernsituation sind. Die Methode des „learning by doing“ erwies sich als besonders effektiv, da die Studierenden durch unmittelbare Erfahrungen nicht nur Wissen akquirieren, sondern auch Problemlösungskompetenzen entwickeln konnten. Weiterhin förderte der kollaborative Aspekt des Projektes, bei dem Teams unterschiedliche Aufgaben bearbeiteten und voneinander lernten, den sozialen Austausch unter den Teilnehmenden und stärkte den Teamgeist.   Das Feedback der Teilnehmer war durchweg positiv. Die Möglichkeit, mit realistischen Szenarien zu arbeiten, führt zu einem deutlichen Anstieg des Interesses und der Motivation, auch komplexe Themen der Informatik zu ergründen. Zukünftige Implementierungen des Projektes sollten zusätzlich multidisziplinäre Ansätze integrieren, indem beispielsweise die Konzepte von MQTT mit Bereichen wie Maschinenlernen oder Künstlicher Intelligenz in Verbindung gebracht werden.   Zusammenfassend lässt sich sagen, dass die Entwicklung eines virtuellen MQTT-Szenarios nicht nur das technische Verständnis der Teilnehmenden entscheidend verbessert hat, sondern auch als Modell für innovative Lehrmethoden dienen kann. Die positiven Rückmeldungen und Lernerfahrungen ermutigen uns, das watt Pool des theoretischen Verständnisses um praktische MPI-Elemente zu erweitern sie mit syntaktisch vielfältigen und realitätsnahen Anwendungsfeldern perspektivisch zu kombinieren, um das Lehren und Lernen in der Gebiet der IoT-Technologien weiter voranzutreiben.;1
Auf diesem Fragment gibt es noch zwei Knöpfe: Ein Knopf für die Ein- und Ausschaltung des Alarms und ein für die Bedienungsanleitung (rechts in der Abbildung 4.4). Der Luftreiniger wird jeden Tag um 22:00 Uhr automatisch auf die höchste Betriebsstufe (FAR-UVC) geschaltet, wenn der Alarm eingeschaltet ist. In der Abbildung 4.5 ist das Fragment für die Visualisierung der verschiedene Sensordaten abgebildet. Die Benutzer*innen können entweder die letzten vierundzwanzig Stunden, sie- ben oder einunddreißig Tage auswählen. Nachdem Drücken des ausgewählten Zeitintervalls werden die Diagramme aktualisiert. Auf den Diagrammen werden die Informationen über die Temperatur, die Helligkeit, das Geräusch, den Luftdruck und die Humidität darge- stellt. Auf der x-Achse ist das jeweilige Zeitintervall zu sehen und auf der y-Achse sind die automatisch skalierte Werte dargestellt. Wurde ein Zeitintervall ausgewählt, müssen die Nutzer*innen durch Betätigung des Aktualisierungsknopfs die Diagramme manuell neuladen.;0
Ausblick  Die vorliegende Arbeit hat sich intensiv mit der Entwicklung einer Fahrzeugfernsteuerung beschäftigt, die auf dem IEEE 802.15 Standard basiert und innovative Ansätze zur Kollisionsvermeidung integriert. Die Ergebnisse zeigen vielversprechende Ansätze zur Verbesserung der Sicherheit und Effizienz im Bereich der Fernsteuerung von Fahrzeugen. Die Implementierung von drahtlosen Kommunikationsprotokollen ermöglicht nicht nur eine flexible Steuerung, sondern auch die Echtzeit-Datenübertragung zwischen dem Fahrzeug und der Steuerungseinheit.  Ein zentraler Aspekt der zukünftigen Entwicklungen in diesem Bereich wird die Weiterentwicklung der Algorithmen zur Kollisionsvermeidung sein. Während die aktuellen Ansätze bereits grundlegende Sicherheitsmechanismen bieten, besteht ein erhebliches Potenzial für die Integration von maschinellem Lernen und KI-Technologien. Diese könnten dazu beitragen, die Reaktionszeiten zu optimieren und die Vorhersagegenauigkeit in komplexen Verkehrssituationen zu erhöhen.   Darüber hinaus könnte die Erweiterung des Systems um zusätzliche Sensoren, wie z.B. LiDAR oder Kameras, die Situationswahrnehmung und Entscheidungsfindung des Fahrzeugs weiter verbessern. Die Kombination dieser Technologien könnte nicht nur die Sicherheit erhöhen, sondern auch die Akzeptanz autonomer und fernsteuerbarer Fahrzeuge in der Gesellschaft fördern.  Ein weiterer wichtiger Aspekt ist die Interoperabilität mit bestehenden Verkehrssystemen und die Einhaltung von Sicherheitsstandards. Zukünftige Forschungsarbeiten sollten sich darauf konzentrieren, wie die entwickelte Technologie nahtlos in bestehende Infrastrukturen integriert werden kann, um die Akzeptanz und den praktischen Nutzen zu maximieren.   Schließlich wird die gesellschaftliche Dimension der Fahrzeugfernsteuerung nicht zu vernachlässigen sein. Die ethischen Implikationen und die Nutzerakzeptanz müssen ebenso in zukünftige Studien einfließen, um ein umfassendes Verständnis für die Auswirkungen dieser Technologien auf die Mobilität der Zukunft zu gewinnen.  Insgesamt eröffnet die Entwicklung einer Fahrzeugfernsteuerung mit Kollisionsvermeidung auf Basis von IEEE 802.15 nicht nur neue technische Möglichkeiten, sondern wirft auch eine Vielzahl von Fragen auf, die in den kommenden Jahren weiter erforscht werden müssen. Die vorliegende Arbeit legt somit den Grundstein für zukünftige Innovationen in der Fahrzeugtechnologie und der intelligenten Verkehrssysteme.;1
Eine vergleichende Analyse der theoretischen Grundlagen  Die Programmiersprachen Java und Kotlin haben sich in den letzten Jahren als zentrale Akteure in der Entwicklung von Software, insbesondere im Bereich der Android-Entwicklung, etabliert. Beide Sprachen weisen fundamentale Unterschiede in ihrer Syntax, Typensystematik und in der Art und Weise auf, wie sie mit Programmierparadigmen umgehen. Diese Unterschiede können auf die theoretischen Grundlagen zurückgeführt werden, die den jeweiligen Sprachen zugrunde liegen.  1. Historischer Kontext und Evolution  Java wurde 1995 von Sun Microsystems eingeführt und hat sich schnell zu einer der am weitesten verbreiteten Programmiersprachen entwickelt. Die Sprache wurde mit dem Ziel konzipiert, plattformunabhängig zu sein, was durch das Konzept der Java Virtual Machine (JVM) ermöglicht wird. Java verfolgt ein objektorientiertes Paradigma und legt großen Wert auf die Prinzipien der Kapselung, Vererbung und Polymorphie.  Kotlin hingegen wurde 2011 von JetBrains vorgestellt und zielt darauf ab, die Einschränkungen von Java zu überwinden. Kotlin ist eine statisch typisierte Sprache, die auf der JVM läuft und vollständig interoperabel mit Java ist. Die Sprache wurde mit dem Ziel entwickelt, eine prägnantere und sicherere Syntax bereitzustellen, die moderne Programmierpraktiken unterstützt.  2. Typensystem und Null-Sicherheit  Ein grundlegendes theoretisches Konzept, das Java und Kotlin unterscheidet, ist das Typensystem. Java verwendet ein reines objektorientiertes Paradigma, in dem primitive Datentypen (wie int, char, etc.) von Objekten (wie Integer, Character, etc.) getrennt sind. Dies kann zu Problemen führen, insbesondere im Hinblick auf Nullwerte, die in Java zu NullPointerExceptions führen können.  Kotlin hingegen integriert primitive Typen und Objekte in ein einheitliches Typensystem. Darüber hinaus implementiert Kotlin eine Null-Sicherheit auf Sprachebene, die es Entwicklern ermöglicht, NullPointerExceptions zu vermeiden. In Kotlin müssen Variablen, die null sein können, explizit als nullable deklariert werden. Dies fördert eine sicherere Programmierung und zwingt Entwickler, sich aktiv mit dem Umgang von Nullwerten auseinanderzusetzen.  3. Syntax und Ausdruckskraft  Die Syntax von Java ist bekannt für ihre Verbosität. Dies kann in großen Codebasen zu einer erhöhten Komplexität führen. Kotlin hingegen bietet eine kompakte und ausdrucksstarke Syntax, die es Entwicklern ermöglicht, weniger Code zu schreiben, um die gleiche Funktionalität zu erreichen. Dies wird durch Funktionen wie Lambda-Ausdrücke, Datenklassen und Erweiterungsfunktionen unterstützt. Diese Merkmale fördern nicht nur die Lesbarkeit des Codes, sondern auch die Wartbarkeit und Testbarkeit.  4. Programmierparadigmen  Java ist stark auf das objektorientierte Programmieren (OOP) ausgerichtet. Während es auch funktionale Elemente unterstützt, bleibt der Fokus auf Klassen und Objekten. Kotlin hingegen ist eine hybride Sprache, die sowohl objektorientierte als auch funktionale Programmierparadigmen unterstützt. Dies ermöglicht es Entwicklern, die Vorteile beider Paradigmen zu nutzen und eine flexibl;1
Auch bei Drupal findet das Prinzip des WYSIWYG -Editors Anwendung. Eine Aufteilung der Inhaltstypen in Blöcke ist hierbei jedoch nicht vorgesehen. Sollen Inhalte in ihrer Anordnung abgeändert werden, so müssen diese direkt innerhalb des Editors verschoben werden, was die Handhabung und Benutzerfreundlichkeit mindert. Da die Grundprinzipien des WYSIWYG -Ansatzes innerhalb von Drupal auch ohne zusätz- liche Installation der Gutenberg-Erweiterung verfolgt werden, wird auch Drupal mit der Kategorie Grün eingestuft.;0
Ausblick für die wissenschaftliche Arbeit: Anforderungsanalyse an ein Aufgabenmanagement-Tool zur Unterstützung des studentischen Software Engineerings  Im Zuge dieser wissenschaftlichen Arbeit wurde eine umfassende Anforderungsanalyse für ein Aufgabenmanagement-Tool entwickelt, das speziell auf die Bedürfnisse von Studierenden im Bereich Software Engineering zugeschnitten ist. Der Fokus lag auf der Identifikation und Priorisierung der funktionalen und nicht-funktionalen Anforderungen, um ein benutzerfreundliches und effizientes Werkzeug zu schaffen.   In der sich stetig wandelnden Landschaft der Softwareentwicklung ist die Notwendigkeit für effektive Werkzeuge zur Unterstützung des Lernprozesses und der Projektarbeit unbestreitbar. Die vorliegende Arbeit hat die Relevanz solcher Tools unterstreicht und gezeigt, wie wichtig es ist, dass Studierende Zugang zu intuitiven und anpassbaren Lösungen haben, die ihre Produktivität und Zusammenarbeit fördern.  Im Ausblick soll verdeutlicht werden, welche weiteren Schritte und Forschungsansätze notwendig sind, um die Implementierung und Evaluierung des entwickelten Tools zu realisieren. Hierbei ist es entscheidend, nicht nur die technischen Anforderungen weiter zu vertiefen, sondern auch die Benutzerfreundlichkeit kontinuierlich durch Nutzerfeedback zu verbessern. Künftige Forschung könnte sich darauf konzentrieren, die Integration des Tools in bestehende Lernmanagementsysteme zu untersuchen, um einen reibungslosen Wissensaustausch und eine nahtlose Zusammenarbeit zwischen Studierenden und Dozenten zu ermöglichen.  Ein weiterer wichtiger Aspekt ist die Betrachtung von adaptiven und intelligenten Funktionen innerhalb des Tools, die auf maschinellem Lernen basieren und das Nutzerverhalten analysieren, um personalisierte Empfehlungen zu geben. Dies könnte die Effizienz zusätzlich steigern und den Lernprozess erheblich unterstützen.  Zusammenfassend lässt sich sagen, dass die entwickelten Anforderungen lediglich den ersten Schritt in einem umfassenden Forschungsprozess darstellen. Die potenzielle Auswirkung eines effektiven Aufgabenmanagement-Tools auf das studentische Software Engineering ist erheblich. Zukünftige Studien sollten auch die Langzeitwirkungen solcher Instrumente auf die Lernergebnisse und die Projektergebnisse im Umfeld des studentischen Software Engineerings untersuchen, um das volle Potenzial der Digitalisierung in der Ausbildung auszuschöpfen.;1
 State of the Art beim Testen von MQTT-basierten LösungenEin Fazit  Das Message Queuing Telemetry Transport (MQTT) Protokoll hat sich in den letzten Jahren als Standard für die Kommunikation in Internet of Things (IoT)-Anwendungen etabliert. Seine Lightweight-Natur, die Unterstützung mobiler Geräte und sykdomübertragene Umgebungen sowie die hohe Effizienz in der Nachrichtenübertragung haben MQTT zu einer bevorzugten Wahl für Entwickler in diversen Anwendungsbereichen gemacht.  Gemäß der aktuellen Literatur und Entwicklungen sind die gängigsten Ansätze zum Testen von MQTT-basierten Lösungen im Wesentlichen dreidimensional strukturiertfunktionales Testen der Anwendung, Last- und Performancetests sowie Sicherheitstests. Der Trend geht dabei in Richtung Automatisierung und die Integration von Testumgebungen in CI/CD-Pipelines, um kontinuierliches Testen in der Softwareentwicklung zu ermöglichen.  Funktionales Testen Ein Ansatz, der sich durchgesetzt hat, ist die Verwendung von speziellen Testing-Tools wie MQTT.fx oder Mosquitto sowie von Frameworks wie Paho oder HiveMQ. Diese Werkzeuge erlauben ein umfassendes Testen der grundlegenden MQTT-Funktionalitäten, wie etwa das Publish-Subscribe-Modell, QoS-Level (Quality of Service), Beziehungsmanagement und Lastdauergetreue Nachrichtenübertragung. Bedeutsam ist hier auch die Abbildung des idealen Anwendungsconstants, um kritische Fälle wie identische Publish-Message Retrievals zu überprüfen.  Last- und Performancetests Die Belastung von MQTT-Botschaften lässt sich durch recht ausgewogene Messaufbauten wie JMeter oder Gatling gut ermitteln. Diese Tools bieten nicht nur Möglichkeiten zur Erstellung von Test-Skripten, sondern enthalten auch Funktionalitäten zur Analyse der Erhebung der statistischen Ergebnisse über die Protokollebene hinweg. Es ist hauptsächlich wichtig, die Performance bei zunehmender Teilnehmerzahlen, latente Antwortzeiten sowie die Stabilität der Broker zu testen. Lasttests gemischt mit unterschiedlicher QoS können Klarheit über das eigentliche Verhaltensspektrum bieten.  Sicherheitstests Der Schutz von IoT-Lösungen wurde im Laufe der Zeit zu einer entscheidenden Frage, weshalb neue Prüfprotokolle momentan fortlaufend implementiert werden. Tests werden häufig durchmodularisierten Werkzeuge durchgeführt, wie etwa OWASP ZAP für Schwachstellenabbau und Penetrationstests im Vorabgangsbereich, das interne Sicherheitsprotokolle der Broker wie die Verschlüsselung durch SSL/TLS analysiert. Der authentische Zugriff- und Berechtigungstest für am Netzwerk teilnehmende interaktive Geräte gehört hierbei zu einem oft übersehenen kritischem Komponenten.  Fazit Zusammenfassend ließ sich feststellen, dass state-of-the-art Techniken im Testing von MQTT-basierten Lösungen maßgeblich durch Automatisierung und Integration in moderne Entwicklungspraktiken geprägt sind. Selbst wenn bedeutende Fortschritte gemacht werden, bleibt eine kontinuierliche Evaluierung und Weiterentwicklung von Testmethoden bei immer exakterer Fragestellung weiterhin erforderlich. Ursachen liegen vor allem in der starken Diversifizierung des IoT-Ökosystems, technischen Next-GenerationProblemen und im adaptiven Bedarf an Sicherheitsmaßnahmen. Rückblickend hat unser Projekt energiespar;1
•Noch kein etablierter Industriestandard Die Begeisterung der Entwickelnden für den neuen deklarativen Ansatz und auch die Größe der mit dem Jetpack Compose Frameworks in Verbindung stehenden Commu- nity steigt zwar kontinuierlich an, allerdings handelt es sich dabei noch um keinen fest etablierten Industriestandard, der regelmäßig im industriellen und produzierenden Umfeld eingesetzt wird . Die Mehrheit der Community und auch die bereits in Kapitel 4.1.3 erwähnten größeren Unternehmen, die Compose bereits in ihrem laufenden Betrieb testen, schreiben dem Ansatz jedoch eine große Bedeutung zu und gehen von einem Trend aus, der sich immer mehr von der Verwendung des klassischen imperativen Ansatzes abwendet. Viele sehen in diesem Bereich einen bevorstehenden Wandel, der sich ähnlich äußern könnt wie damals beim Wechsel von Java zu Kotlin als bevorzugte Sprache für die Android-Programmierung .;0
Ausblick  In der vorliegenden Arbeit wurde eine umfassende Gegenüberstellung von Content-Management-Systemen (CMS) durchgeführt, die sowohl die technischen als auch die praktischen Aspekte dieser Systeme beleuchtet. Die Analyse hat gezeigt, dass die Wahl des richtigen CMS entscheidend für den Erfolg digitaler Projekte ist, da sie nicht nur die Benutzerfreundlichkeit und Effizienz der Content-Erstellung beeinflusst, sondern auch die langfristige Wartbarkeit und Skalierbarkeit der digitalen Plattformen.  Im Zuge der fortschreitenden Digitalisierung und der stetig wachsenden Anforderungen an Unternehmen, sich flexibel an Marktveränderungen anzupassen, wird die Relevanz von CMS in den kommenden Jahren weiter zunehmen. Zukünftige Forschung könnte sich darauf konzentrieren, wie neue Technologien, wie Künstliche Intelligenz und maschinelles Lernen, die Entwicklung und Funktionalität von CMS beeinflussen. Insbesondere die Automatisierung von Inhalten, personalisierte Nutzererfahrungen und die Integration von Datenanalyse-Tools werden voraussichtlich zentrale Themen sein, die die CMS-Landschaft prägen werden.  Darüber hinaus könnte eine vertiefte Untersuchung der Benutzerakzeptanz und der spezifischen Anforderungen unterschiedlicher Zielgruppen wertvolle Erkenntnisse liefern. Die Berücksichtigung von Aspekten wie Barrierefreiheit, mehrsprachige Inhalte und die Integration sozialer Medien wird immer wichtiger, um den vielfältigen Bedürfnissen der Nutzer gerecht zu werden.  Abschließend lässt sich festhalten, dass die Gegenüberstellung von Content-Management-Systemen nicht nur eine Momentaufnahme der aktuellen Technologien darstellt, sondern auch einen Ausblick auf zukünftige Entwicklungen bietet. Die dynamische Natur des digitalen Marktes erfordert kontinuierliche Anpassungen und Innovationen, die sowohl Anbieter als auch Anwender von CMS vor neue Herausforderungen stellen werden. Die vorliegende Arbeit legt den Grundstein für weitere Untersuchungen und Diskussionen in diesem spannenden und sich ständig weiterentwickelnden Bereich.;1
    In der heutigen Bildungslandschaft wird die Fähigkeit, Softwareprojekte effizient zu managen, für angehende Software-Ingenieure zunehmend wichtiger. Die Komplexität von Software-Projekten erfordert nicht nur technische Fertigkeiten, sondern auch ein adäquates Management von Aufgaben, Ressourcen und Zeit. Vor diesem Hintergrund ist die Entwicklung eines effektiven Aufgabenmanagement-Tools von entscheidender Bedeutung, um die dringend benötigte Unterstützung für studentisches Software Engineering zu bieten. Die vorliegende Arbeit widmet sich der Anforderungsanalyse für ein solches Tool und entwirft ein Konzept zur Umsetzung.  1. Zielsetzung und Bedeutung der Anforderungsanalyse  Eine präzise Anforderungsanalyse bildet das Fundament für die Entwicklung eines erfolgreichen Softwareprodukts. Im Kontext der Erstellung eines Aufgabenmanagement-Tools für studentisches Software Engineering besteht das Hauptziel darin, die spezifischen Bedürfnisse der Benutzer zu identifizieren und zu strukturieren. Die Bedeutung dieser Analyse liegt in der Sicherstellung, dass das entwickelte Tool nicht nur funktional ist, sondern auch anwenderfreundlich, skalierbar und anpassungsfähig an verschiedene Projektanforderungen.  2. Methodische Vorgehensweise  Die Anforderungsanalyse erfolgt in mehreren Phasen - LiteraturrechercheEine umfassende Analyse bestehender Tools und deren Funktionen wird durchgeführt, um Best Practices zu identifizieren und Lücken im derzeitigen Angebot zu erkennen.  - Befragungen und WorkshopsUm ein tieferes Verständnis für die Bedürfnisse der Studierenden zu gewinnen, werden qualitative Interviews sowie quantitative Umfragen durchgeführt. Workshops mit Studierenden und Lehrenden ermöglichen es, interaktive Rückmeldungen zu den gewünschten Funktionen zu erhalten.  - Kategorisierung der AnforderungenDie gesammelten Informationen werden in funktionale und nicht-funktionale Anforderungen kategorisiert, um eine klare Übersicht über die Erwartungen an das Tool zu schaffen.  3. Funktionale Anforderungen  Funktionale Anforderungen beschreiben spezifische Verhaltensweisen des Systems. Für das Aufgabenmanagement-Tool ergeben sich folgende Kernanforderungen - AufgabenverwaltungBenutzer müssen in der Lage sein, Aufgaben zu erstellen, zu bearbeiten, zu priorisieren und zu löschen. Die Möglichkeit, Aufgaben bestimmten Projekten oder Teammitgliedern zuzuordnen, ist ebenfalls essentiell.  - ZeiterfassungDie Implementierung von Zeitmanagement-Funktionen, einschließlich der Möglichkeit, Aufwand für einzelne Aufgaben zu erfassen und Fortschritte zu visualisieren, ist von hoher Bedeutung.  - KollaborationFunktionen zur Unterstützung der Teamarbeit, wie gemeinsame To-Do-Listen, Kommentarfunktionen und Benachrichtigungen, fördern die Kommunikation unter Studierenden.  - BerichtswesenDie Möglichkeit, Übersichtsberichte und Analysen zu generieren, ermöglicht den Studierenden, ihre Fortschritte zu evaluieren und anzupassen.  4. Nicht-funktionale Anforderungen  Nicht-funktionale Anforderungen sind ebenfalls entscheidend für den Erfolg des Tools - BenutzerfreundlichkeitDas Interface sollte intuitiv gestaltet sein, um eine hohe Akzeptanz bei den Nutzern zu gewährleisten. Eine lernfreundliche Oberfläche, die auch neue Benutzer schnell einführt, ist unerlässlich.  - ZugänglichkeitDas Tool sollte plattformübergreifend sein, um den unterschiedlichen technischen Voraussetzungen der Studierenden gerecht zu werden. Eine webbasierte Lösung könnte hier sinnvoll sein.  - SicherheitDatenschutz und Datensicherheit müssen gewährleistet sein, um die sensiblen Projektinformationen der Nutzer zu schützen.  5. Umsetzungskonzept  Basierend auf den identifizierten Anforderungen wird ein schrittweises Umsetzungskonzept entwickelt 1. PrototypingDie Entwicklung eines Prototyps ermöglicht es, frühzeitig Benutzerfeedback zu sammeln und Anpassungen vorzunehmen.  2. Iterative EntwicklungEin agiler Entwicklungsansatz fördert die kontinuierliche Verbesserung des Tools, basierend auf den Rückmeldungen der Benutzer und sich ändernden Anforderungen.  3. TestphaseNach der Umsetzung muss das Tool umfangreich getestet werden, um sicherzustellen, dass alle Anforderungen erfüllt sind und die Benutzerfreundlichkeit gegeben ist.  4. EinführungsstrategieUm das Tool effektiv bei Studierenden einzuführen, sind Schulungen und Unterstützung erforderlich. Tutorials, FAQs und Community-Support können hier wertvolle Hilfe leisten.  Schlussfolgerung  Die Anforderungsanalyse bildet das Rückgrat für die Entwicklung eines effektiven Aufgabenmanagement-Tools, das die Herausforderungen des studentischen Software Engineerings adressiert. Durch die Kombination aus funktionalen und nicht-funktionalen Anforderungen sowie einem klaren Umsetzungskonzept kann sichergestellt werden, dass das Tool nicht nur eine pragmatische Lösung bietet, sondern auch einen Mehrwert für die Lehr- und Lernprozesse im Bereich Software Engineering schafft. In dieser Hinsicht schafft die vorliegende Analyse einen soliden Rahmen, um die wensen der Studierenden in ein effektives Produkt zu transferieren und somit deren Erfolg bei der Umsetzung komplexer Softwareprojekte zu fördern.;1
"TitelZero – Möglichkeiten und Gefahren der digitalen ÜberwachungEin Konzept zur Umsetzung    Die digitale Überwachung hat in den letzten Jahren an Bedeutung gewonnen, sowohl im öffentlichen als auch im privaten Sektor. Mit der fortschreitenden Digitalisierung und der zunehmenden Vernetzung von Geräten und Systemen ist die Erfassung, Speicherung und Analyse von Daten zu einer Schlüsselkomponente moderner Gesellschaften geworden. Das Konzept ""Zero"" zielt darauf ab, die Möglichkeiten der digitalen Überwachung zu erfassen und gleichzeitig die damit verbundenen Gefahren zu analysieren. In diesem Prosatext wird ein Konzept zur Umsetzung von ""Zero"" vorgestellt, das sowohl technologische als auch ethische Aspekte berücksichtigt.  Möglichkeiten der digitalen Überwachung  Die digitale Überwachung bietet zahlreiche Chancen, die weit über die reine Datenanalyse hinausgehen. In Bereichen wie der öffentlichen Sicherheit, dem Gesundheitswesen und der Wirtschaft können durch intelligente Überwachungssysteme wertvolle Erkenntnisse gewonnen werden. Beispielsweise ermöglicht die Analyse von Verkehrsströmen in Echtzeit die Optimierung von Verkehrsmanagementsystemen, was zu einer Reduktion von Staus und Emissionen führen kann. Im Gesundheitswesen können Wearables und mobile Apps zur Überwachung von Vitaldaten dazu beitragen, frühzeitig gesundheitliche Risiken zu identifizieren und präventive Maßnahmen zu ergreifen.  Darüber hinaus können datenbasierte Entscheidungsprozesse Unternehmen dabei unterstützen, ihre Effizienz zu steigern und personalisierte Dienstleistungen anzubieten. Die Verwendung von Algorithmen zur Analyse von Kundenverhalten kann beispielsweise die Kundenbindung erhöhen und neue Geschäftsmöglichkeiten erschließen.  Gefahren der digitalen Überwachung  Trotz der vielversprechenden Möglichkeiten birgt die digitale Überwachung auch erhebliche Gefahren. Eine der größten Herausforderungen ist der Verlust der Privatsphäre. Die ständige Erfassung persönlicher Daten kann zu einem Gefühl der ständigen Beobachtung führen, was das individuelle Verhalten und die Entscheidungsfreiheit einschränkt. Zudem besteht das Risiko von Datenmissbrauch, sei es durch Cyberkriminelle oder durch Unternehmen, die ohne Einwilligung auf sensible Informationen zugreifen.  Ein weiteres Problem ist die potenzielle Verzerrung von Daten und Algorithmen, die zu Diskriminierung führen kann. Wenn Überwachungssysteme auf fehlerhaften oder voreingenommenen Daten basieren, können sie unfaire Entscheidungen treffen, die bestimmte Gruppen benachteiligen. Diese ethischen Implikationen müssen bei der Entwicklung von Überwachungstechnologien unbedingt berücksichtigt werden.  Konzept zur Umsetzung von ""Zero""  Um die Möglichkeiten der digitalen Überwachung verantwortungsvoll zu nutzen und die damit verbundenen Gefahren zu minimieren, wird ein mehrstufiges Konzept vorgeschlagen 1. Transparenz und AufklärungEs ist entscheidend, die Öffentlichkeit über die Funktionsweise und die Ziele von Überwachungssystemen aufzuklären. Informationskampagnen können helfen, das Bewusstsein für Datenschutzrechte zu schärfen und das Vertrauen in digitale Technologien zu stärken.  2. Datenschutz durch DesignBei der Entwicklung neuer Überwachungstechnologien sollten Datenschutzaspekte von Anfang an integriert werden. Dies umfasst die Anonymisierung von Daten, die Minimierung der Datenspeicherung sowie die Implementierung von Sicherheitsmaßnahmen, um unbefugten Zugriff zu verhindern.  3. Regulierung und";1
Sobald innerhalb der App eine Navigation stattﬁnden soll, muss dem NavController mitgeteilt werden, zu welcher Dimension gewechselt werden soll. Diese wird dabei im NavGraph über die Route deﬁniert. Der NavController nimmt diese Informationen an und sorgt dafür, dass der entsprechende Inhalt auf dem Bildschirm angezeigt wird . Hierbei bedient er sich der Re- komposition, die jedes Mal automatisch stattﬁndet, wenn zwischen Composables navigiert wird. Diese Rekomposition ist einer der Gründe dafür, warum empfohlen wird, die Funktion navigate(), welche vom NavController zur Ausführung der Navigation angeboten wird, immer nur in Callbacks aufzurufen. Ansonsten würde sie mit jeder Rekomposition ausge- führt, was zu einem merkwürdigen Verhalten der Anwendung führen könnte . Die Funktion navigate() bekommt hierbei immer die Route übergeben. Optional besteht auch die Möglichkeit, über die Route Parameter zu übergeben. Diese müssen allerdings als Placeholder in der Route vermerkt werden. Zusätzlich ist bei mehreren Parametern die Reihenfolge zu beachten, in der diese übergeben werden. In der Zieldimension müssen diese Werte dann explizit ausgelesen werden, um sie verwenden zu können. Ebenfalls unterstützt werden neben optionalen Parametern auch Deeplinks , . Zudem bietet die Bibliothek die Möglichkeit, Nested Navigation zu implementieren () und verspricht neben ﬂüssigen und animierten Übergängen auch eine einfache Implementierung der Backnavigation sowie die allgemeine Unterstützung der bereits erwähnten Navigation Patterns.;0
Die Installation von Drupal erfolgt nach dem gleichen Schema wie WordPress. Nach dem Download der ZIP-Datei des Systems und dem Entpacken und Setzen der Berechtigungen, kann das System direkt im Browser über den Installationsassistenten eingerichtet werden. Abbildung 5.5: Drupal Installationsassistent im Browser1 Anforderungen an der Server stellt es folgende2: •Eine aktuelle SQL-Server Version (MariaDB 10.3, MySQL 5.7, PostgreSQL oder wahlweise SQLite.) Für den Betrieb auf einem Microsoft SQL-Server muss der sogenannte „Drupal Driver for SQL Server“ zusätzlich installiert werden. •Gemäß Drupal Website kann jeder Webserver genutzt werden, der die Anforderungen an PHP erfüllt •PHP: Version 7.3 oder höher, Composer Erweiterung wird benötigt Durch die ebenfalls einfache Installation gegeben durch den Upload des komprimierten Drupal-Verzeichnisses auf einen Webserver wird die Installation von Drupal mit der Farbe Grün kategorisiert.;0
 Evaluation von ElixirNerves als Plattform für IoT-Anwendungen  In der heutigen Welt spielt das Internet der Dinge (IoT) eine entscheidende Rolle in der Vernetzung von Geräten und der automatisierten Datenerfassung. Diese Entwicklung stellt eine Vielzahl an Herausforderungen und Anforderungen an die zugrunde liegenden Softwarearchitekturen. ElixirNerves hat sich als vielversprechende Plattform etabliert, die speziell für die Entwicklung von IoT-Anwendungen konzipiert wurde. Um die Eignung dieser Plattform zu evaluieren, ist es zunächst wichtig, die theoretischen Grundlagen des IoT, die Architektur von ElixirNerves sowie deren bedeutende Merkmale zu betrachten.   1.  des IoT  Das Internet der Dinge beschreibt ein Netzwerk von physischen Objekten, die mit Sensoren, Software und weiteren Technologien ausgestattet sind, um Daten auszutauschen und zu kommunizieren. Die theoretischen Aspekte des IoT stützen sich auf verschiedene Disziplinen, einschließlich Netzwerkprotokolle, Datenmanagement, Sicherheitsaspekte und Interoperabilität. Insbesondere die Konzepte der dezentralen Verarbeitung und der Datenaggregation spielen hierbei eine zentrale Rolle.   Des Weiteren erfordert das IoT eine robuste und skalierbare Softwarearchitektur, die dazu in der Lage ist, Daten in Echtzeit zu verarbeiten. Diese Entwicklungsgrundlagen umfassen unter anderem das Design von Embedded Systems, Cloud-Computing-Architekturen sowie die Implementierung von Datenbanklösungen, die den Anforderungen an Geschwindigkeit und Effizienz gerecht werden.   2. Architektur von ElixirNerves  ElixirNerves ist ein Framework, das auf der Programmiersprache Elixir basiert und sich speziell auf die Entwicklung von Embedded Software für IoT-Geräte fokussiert. Die zugrunde liegende Architektur von ElixirNerves folgt dem Erlang-VM (BEAM), welche für ihre hervorragende Unterstützung von Nebenläufigkeit, Fehlertoleranz und Verteilung bekannt ist. Diese Eigenschaften sind von zentraler Bedeutung für die anspruchsvollen Bedingungen, unter denen IoT-Geräte häufig operieren.  Die Struktur von ElixirNerves ermöglicht es Entwicklern, Anwendungen modulare und wiederverwendbare Komponenten zu schaffen. Ein zentrales Prinzip hierbei ist die Nutzung von Prozessen, die unabhängig voneinander agieren können und durch Nachrichtenübermittlung miteinander kommunizieren. Diese Herangehensweise maximiert die Effizienz der Datenverarbeitung und ermöglicht eine hohe Verfügbarkeit der Dienste, was im Kontext von IoT-Anwendungen von essenzieller Bedeutung ist.   3. Schlüsselmerkmale von ElixirNerves  Die Evaluation von ElixirNerves als Plattform für IoT-Anwendungen sollte auch die spezifischen Merkmale dieser Architektur berücksichtigen - EchtzeitfähigkeitDank der Erlang-VM gestaltet sich die Handhabung von Echtzeitdaten besonders effizient. ElixirNerves ermöglicht die Verarbeitung von Datenströmen in Echtzeit, was in vielen IoT-Anwendungen, wie beispielsweise in der industriellen Automatisierung, unerlässlich ist.  - Verteilte SystemeDas Framework fördert die Entwicklung verteilter Systeme durch eingebaute Mechanismen für die Fehlertoleranz und Lastenverteilung. Das bedeutet, dass IoT-Systeme, die auf ElixirNerves basieren, gegen Ausfälle abgesichert sind und auch unter hoher Last effizient arbeiten können.  - Integration von Hardware und SoftwareElixirNerves erleichtert die Interaktion zwischen Software und Hardware durch eine Vielzahl von Treibern und unterstützten Protokollen. Dies ermöglicht Entwicklern, sowohl die Software- als auch die Hardwarekomponenten einer IoT-Lösung nahtlos zusammenzuführen.  - Community und ÖkosystemDie aktive Community rund um Elixir und Nerves bietet einen wertvollen Pool an Ressourcen, Bibliotheken und Tools, die den Entwicklungsprozess erheblich beschleunigen und vereinfachen.   Fazit  Zusammenfassend lässt sich feststellen, dass ElixirNerves aufgrund seiner einzigartigen Eigenschaften und seiner soliden theoretischen Grundlage eine vielversprechende Plattform für die Entwicklung von IoT-Anwendungen darstellt. Die Kombination aus Echtzeitverarbeitung, Verteilung von Prozessen und der hohen Fehlertoleranz machen es zu einer geeigneten Wahl für Entwickler, die komplexe und zuverlässige IoT-Lösungen implementieren möchten. Zukünftige Forschungsarbeiten sollten sich darauf konzentrieren, spezifische Anwendungsfälle durch empirische Studien zu evaluieren, um die theoretischen Grundlagen weiter zu untermauern und praktische Erfahrungen in die Plattformentwicklung zu integrieren.;1
Um die kontinuierliche Bereitstellung des MQTT Brokers zu gewährleisten und um unge- plante Ausfälle und hohe Antwortzeiten zu vermeiden lohnt es sich Skalierungstests am MQTTBroker durchzuführen. Diese sollen aufzeigen wie gut der MQTTBroker mit einer wachsenden Anzahl von verbunden Clients und zu verschickenden und zu empfangenen Nachrichten umgehen kann.   Für den Broker sind die dauerhaft aufrechtzuerhaltenden TCPVerbindungen zu allen MQTT Clients eine sehr ressourcenintensive Aufgabe. Auch verschiedene Arten von Spikes in Verbindungen oder Nachrichten müssen getestet werden. Dazu zunächst getestet werden wie viele simultane Client Verbindungen der MQTTBroker aufrechterhalten kann, ohne dass der angebotene Service darunter leidet. Daraus lässt sich die maximale Kapazität des MQTTBrokers ableiten. Zusätzlich sollte auch getestet werden, wie viele Verbindungsan- forderungen der MQTTBroker akzeptieren kann ohne auszufallen oder den Service zu gefährden. Dies kann aufzeigen, wie der MQTTBroker bei gewollten oder unabsichtlichen Denial of Service ( DoS)-Szenarien reagiert. So kann ein Ausfall der Internetverbindung, an welche ein große Anzahl von MQTTClients angeschlossen ist, zu einem solchen Szenario führen. Sollten die sich daraus ergebenen Kapazitäten, mit einem eingerechneten Puffer nicht für den geplanten Einsatzzweck reichen, sollte die Verwendung eines MQTTBroker Clusters in Erwägung gezogen werden.    Der nächste Schritt ist das Testen der Kombination aus bestehenden Verbindungen und den Verbindungsanforderungen, dies ist im Besonderen bei MQTT Broker Clustern wichtig, um dort das Load Balancing zu testen. Ein weiterer wichtiger Punkt ist, dass alle Test auch mit den gewünschten Einstellungen und Umgebung der Endanwendung getestet werden sollten. Dazu gehören: •Anzahl der genutzten Topics •Anzahl der MQTT Subscriber pro Topic •Anzahl der “Wildcard”-Subscriptions •Anzahl der aktiven MQTT Publisher •Das gewünschte QoS-Level •Die durchschnittliche Nachrichtengröße •Einstellungen bezüglich der zwischengespeicherten Nachrichten •Nutzung von Transport Layer Security (TLS)       Oft werden von MQTT Subscriber nur wenige Topics des Broker abonniert und später ausgewertet, deshalb muss bezüglich der Topics getestet werden wie hoch die maximale Anzahl an MQTTSubscriber für einen einzelnen Topic ist. Im Umkehrschluss muss auch getestet werden, wie hoch die maximale Anzahl an abonnierten Topics eines einzelnen MQTTSubscriber ist. Anschließend sollte der allgemeine und zuverlässige Durchsatz von MQTTNachrichten getestet werden, die drei dazu genutzten Kategorien sind in Tabelle 4.2 zu sehen. Fan-in In diesem Szenario werden von einer großen Anzahl von MQTTPublis- her und einer kleineren Anzahl von MQTTSubscriber die entsprechen- den Topics abonniert und empfangen. Dies entspricht zum Beispiel einer Datenarchivierung/-speicherung durch einen Backend-Service. Fan-out Hier werden nur durch eine kleine Anzahl von MQTTPublisher Nach- richten an eine größere Anzahl von MQTTSubscriber versendet. Diese Szenario kann einem Broadcast an IoT Geräte entsprechen. One to One Dieses Szenario soll aufzeigen wie hoch die Kapazität des MQTT Brokers bezüglich vieler gleichzeitiger Verbindungen und vieler Nach- richten und damit verbundenem hohem Durchsatz ist. Dazu wird ein Eins-zu-Eins-Szenario simuliert, indem jeder MQTTPublisher an einen Topic mit einem GUIDsendet, welcher von jeweils einem MQTT Subscriber abonniert ist. Tabelle 4.2: MQTT Broker Durchsatz Dabei sollte darauf geachtet werden dass die Nachrichtengröße dem erwartbaren Durch- schnitt entspricht.   Ein weiteres Problem das beim Betrieb eines MQTTBrokers auftauchen kann ist ein Ausfall der zugrundeliegenden Infrastruktur des Brokers. Dies kann zum Beispiel ein Inter- netausfall oder, bei einem Broker Cluster, der Ausfall des Load Balancers sein. Eine solche Eventualität kann getestet werden, indem alle, oder große Teile, der MQTTClients die Verbindung schließen. Womöglich werden diese Clients auch schon kurz darauf versuchen, sich wieder zu verbinden. Zusätzlich werden sogenannte “Last Will” Nachrichten, welche bei Verbindungsverlust eines Clients gesendet werden, eine hohe Last erzeugen. Auch Auswirkungen auf den MQTTBroker sollten vor Inbetriebnahme gründlich untersucht werden.;0
•Verbesserungen im Entwicklungsprozess Durch die Kompatibilität und die Interoperabilität zu bestehendem Code trägt das Framework massiv zu einem verbesserten Entwicklungsprozess bei. Composables und Views können miteinander arbeiten und sich gegenseitig aufrufen. Zudem funk- tionieren viele Bibliotheken, die ursprünglich für das Android View System erstellt wurden, wie Room, ViewModel oder Kotlin Coroutinen in Compose auch weiterhin. Viele Anwendenden beginnen dabei nicht gleich mit reinem Compose, sondern setzen wie Cuvva auf eine Kombination aus Compose und klassischen View System Kompo- nenten. So wurde auch bei der Erstellung der CoﬀeeCompose App für diese Arbeit mit beiden Komponenten gearbeitet. AuchderAndroidStudioSupportmitderPreviewFunktionﬁndetlobendenZuspruch. Square lobt vor allem die Möglichkeit, mehrere Previews parallel und in mehrfacher AusführungmitunterschiedlichenAppstatesoderKonﬁgurationenerstellenzukönnen. Dies führe bei Ihnen zu einer großen Zeitersparnis und somit dazu, dass sie ihren Code schneller an ihre Kunden ausliefern können.;0
 Kapitel 2: Technische Grundlagen  Die Qualität von Software ist ein facettenreiches Thema, das sowohl technische als auch ökonomische Aspekte umfasst. Im Zentrum dieser Diskussion stehen die produktorientierten Metriken, die genutzt werden, um die Eigenschaften und Leistungen von Software zu bewerten. Dieses Kapitel zielt darauf ab, die technischen Grundlagen dieses Themas zu erläutern und die verschiedenen produktorientierten Metriken im Kontext der Softwarequalität und -entwicklung darzustellen.   2.1 Softwarequalität: Ein Begriff mit vielen Dimensionen  Softwarequalität bezieht sich auf die Gesamtheit der Eigenschaften und Merkmale einer Software, die deren Fähigkeit bestimmen, die festgelegten Anforderungen zu erfüllen. Diese Eigenschaften können in verschiedene Dimensionen klassifiziert werden: Funktionalität, Zuverlässigkeit, Benutzerfreundlichkeit, Effizienz, Wartbarkeit und Übertragbarkeit. Insbesondere produktorientierte Metriken bieten eine quantitative Grundlage zur Bewertung dieser qualitativen Eigenschaften, indem sie spezifische Indikatoren für die Beschaffenheit des Softwareprodukts bereitstellen.   2.2 Produktorientierte Metriken: Typen und Funktionen  Produktorientierte Metriken klassifizieren sich in verschiedene Typen, die sowohl deskriptive als auch inhärente Metriken einschließen. Deskriptive Metriken quantifizieren offensichtlich erkennbare Merkmale der Software wie Codezeilen, Anzahl der Fehlerberichte oder Modulkopplung. Im Gegensatz dazu prüfen inhärente Metriken, die die strukturellen Aspekte der Software bewerten, Attribute wie Folgendes:  - Kohäsion: Misst die Selbstgenügsamkeit eines Moduls in Bezug auf die Funktionalität und stellt fest, wie eng verwandte Funktionen innerhalb des Moduls zusammengeschlossen sind.    - Kopplung: Bewertet die Abhängigkeiten zwischen Modulen und ist entscheidend für die Wartbarkeit und Änderungsfreundlichkeit des Codes.    - Cyclatische Komplexität: Eine Metrik zur Beurteilung der Komplexität eines Programmiersatzes, die auch Hinweise auf Testbarkeit und potenzielle Risiken gibt.   2.3 Technische Implementierung der Metriken  Die technische Umsetzung der produktorientierten Metriken erfolgt meist automatisiert, häufig durch den Einsatz spezialisierter Software- und Analysetools. Diese Werkzeuge können einen direkten Zugang zu den Quellcodes ermöglichen und die relevanten Daten in Echtzeit analysieren. Dazu zählen gängige Programmierumgebungen und Integrationsstacks, die bei der Vereinheitlichung von Software-Reportings und der Visualisierung der Ergebnisse helfen.  Um ein fortlaufendes Qualitätsmanagement zu garantieren, ist es fundamental, die Metriken in den Softwareentwicklungsprozess zu integrieren. Dazu bietet es sich an, sogenannte Code Reviews, Continuous Integration (CI) Prozesse und testgetriebene Entwicklungsmethoden einzusetzen, um die Qualität (inimetente Lösung ongoing bearbeiten Prozess) von der ersten Codezeile an zu fördern.   2.4 Nutzen und Herausforderung der produktorientierten Metriken  Der Hauptvorteil produktorientierter Metriken liegt in ihrer Fähigkeit, objektive und messbare Bezugsgrößen zu bieten, die ein tieferes Verständnis für die zugrunde liegenden Dys;1
 In-room Ortung zur Sturzerkennung mittels BluetoothEine      Die Demografisierung der Gesellschaft führt zu einer zunehmend älter werdenden Bevölkerung. In diesem Kontext gewinnt die Sturzprävention einen erheblichen Stellenwert, da Stürze zu den häufigsten Verletzungen bei Senioren zählen und oft dramatische Folgen haben. Die technischen Möglichkeiten zur Sturzerkennung haben in den letzten Jahren erheblich zugenommen. Insbesondere die Verwendung von In-room Ortungstechnologien bietet eine vielversprechende Perspektive im Bereich der Sturzüberwachung. Dieser wissenschaftliche Prosatext evaluiert ein Projekt, das auf der Anwendung von Bluetooth-Technologie zur Sturzerkennung innerhalb von geschlossenen Räumen basiert.   Projekthintergrund Das Projekt wurde mit der Absicht ins Leben gerufen, eine kosteneffiziente und präzise Lösung zur Sturzerkennung zu bieten. Die Nutzung von Bluetooth-Technologie ermöglicht die präzise Lokalisierung von Individuen innerhalb fest definierter Räume, beispielsweise Altenheimen oder barrierefreien Wohnanlagen. Mittels kostengünstiger Bluetooth Low Energy (BLE) Beacons wird der Aufenthaltsort der Benutzer in Echtzeit erfasst und analysiert. Dabei ist der Gedanke, dass sowohl physische Aktivitäten als auch Stürze durch spezifische Bewegungsmuster identifiziert werden können.   Methodik Zur  wurde ein hybrides Forschungsdesign gewählt, das qualitative und quantitative Ansätze integriert. In einer Pilotstudie wurden Teilnehmer über einen Zeitraum von sechs Monaten beobachtet, während sie in einer kontrollierten Umgebung mit platzieren Beacon-Tracks interagierten. Die Sturzerkennungsalgorithmen wurden mittels maschinellen Lernens entwickelt und trainiert, um falsche Entwarnungen zu minimieren und präzise Erkennungsgrößen bereitzustellen.   Ergebnisse Die Auswertung der gesammelten Daten zeigte, dass die Implementierung des Bluetooth-basierten Systems die Erkennung von Stürzen signifikant verbesserten könnte. Von den beobachteten Stürzen wurden über 85% erfolgreich erkannt und an das Pflegepersonal weitergeleitet. Des Weiteren ergaben statistische Analysen, dass benutzergenerierte Daten - wie Standortunregelmäßigkeiten und Bewegungsgeschwindigkeit - korrelierende Merkmale mit einstehenden Gefahren darstellen.   Herausforderungen Jedoch stellte sich im Verlauf des Projekts auch eine Vielzahl von Herausforderungen dar. Dazu gehörten technische Aspekte, wie beispielsweise die Reichweitenbegrenzung und Signalabschirmung innerhalb von Gebäuden, die拒ankenituation während mathematical Fehler in den Algorithmen. Auch die Benutzerakzeptanz war ein zentral Punkt, der durch Rückmeldungen der Teilnehmer speziell in Hinblick auf ihre Wahrnehmung der Privatsphäre und der heimkomplexen Berechnungen eine substanzielle Rolle spielte.   Diskussion Trotz dieser Herausforderungen hat sich die Zielsetzung des Projekts als durchwegs positiv herausgestellt. Die MQTT-basierte Erhebung von Bewegungsdaten und deren Analyse eröffnete nicht nur neue Perspektiven in der Sturzerkennung, sondern auch in der Entwicklung proaktiver Interventionen zur Sturzvermeidung. Indepandierende Monetarisierungsansätze und Lizenzalternativen könnten umgesetzt werden, um die erkannten Hindernisse;1
"Eine Implementierung eigener Lösungen  Die App-Entwicklung hat sich in den letzten Jahren rasant weiterentwickelt, wobei Frameworks eine zentrale Rolle in der Effizienz und Benutzerfreundlichkeit spielen. Jetpack Compose, ein modernes Toolkit zur Erstellung von Benutzeroberflächen für Android-Anwendungen, hat sich als ein bahnbrechendes Werkzeug etabliert. Es bietet Entwicklern die Möglichkeit, deklarative UI-Komponenten zu erstellen, die sich nahtlos in die bestehende Android-Architektur integrieren lassen. Im Folgenden wird die  unter Verwendung des Jetpack Compose Frameworks untersucht, wobei der Fokus auf der praktischen Anwendung und den Vorteilen dieser Technologie liegt.   Grundlagen von Jetpack Compose  Jetpack Compose basiert auf einem deklarativen Ansatz, der es Entwicklern ermöglicht, Benutzeroberflächen durch die Beschreibung ihrer Komponenten zu erstellen, anstatt sie imperativ zu konstruieren. Dies führt zu einem klareren und wartbareren Code. Die Grundbausteine von Jetpack Compose sind sogenannte Composables, die in Kotlin geschrieben sind. Diese Composables können in einer hierarchischen Struktur organisiert werden, wodurch komplexe Benutzeroberflächen einfach zu erstellen und zu verwalten sind.     Um die Möglichkeiten von Jetpack Compose zu demonstrieren, betrachten wir die Entwicklung einer einfachen To-Do-Liste. Diese Anwendung soll es den Benutzern ermöglichen, Aufgaben hinzuzufügen, abzuhaken und zu löschen. Der folgende Abschnitt beschreibt die Schritte zur Implementierung dieser Lösung.   1. Einrichtung des Projekts  Zunächst muss ein neues Android-Projekt mit Jetpack Compose konfiguriert werden. Dies geschieht in Android Studio, indem das entsprechende Template ausgewählt und die notwendigen Abhängigkeiten in der `build.gradle`-Datei hinzugefügt werden ```groovy dependencies {     implementation ""androidx.compose.ui:ui:1.0.0""     implementation ""androidx.compose.material:material:1.0.0""     implementation ""androidx.compose.ui:ui-tooling-preview:1.0.0""     implementation ""androidx.lifecycle:lifecycle-runtime-ktx:2.3.1""     implementation ""androidx.activity:activity-compose:1.3.1"" } ```   2. Erstellung der Benutzeroberfläche  Die Benutzeroberfläche wird durch die Definition von Composables realisiert. Für unsere To-Do-Liste benötigen wir Composables für die Eingabe von Aufgaben, die Anzeige der Aufgabenliste sowie die Interaktion mit den Aufgaben. Die folgende Implementierung zeigt, wie diese Composables strukturiert werden können ```kotlin @Composable fun TodoApp() {     var task by remember { mutableStateOf("""") }     val tasks = remember { mutableStateListOf<String>() }      Column(modifier = Modifier.padding(16.dp)) {         TextField(             value = task,             onValueChange = { task = it },             label = { Text(""Neue Aufgabe"") }         )         Button(onClick = {             if (task.isNotBlank()) {                 tasks.add(task)                 task = """"             }         }) {             Text(""Hinzufügen"")         }         LazyColumn {             items(tasks) { task ->                 Text";1
Realisierung eines IoT-Systems zur Steuerung einer Katzenklappe mittels KI-basierter Katzenerkennung    Die Integration von Internet of Things (IoT)-Technologien in den Alltag eröffnet neue Möglichkeiten zur Automatisierung und Optimierung von Haushaltsgeräten. Im Kontext der Haustierhaltung gewinnt die Entwicklung intelligenter Lösungen zur Tierüberwachung und -steuerung zunehmend an Bedeutung. Dieser Text beschreibt ein Konzept zur Realisierung eines IoT-Systems, das eine Katzenklappe mittels KI-basierter Katzenerkennung steuert. Ziel ist es, eine benutzerfreundliche, sichere und effiziente Lösung zu schaffen, die Haustierbesitzern eine einfache Kontrolle über den Zugang ihrer Katzen ermöglicht.  Konzeptentwicklung  Die Realisierung des IoT-Systems gliedert sich in mehrere SchlüsselkomponentenHardware-Design, Software-Architektur, KI-Trainingsprozess und Benutzeroberfläche. Jede dieser Komponenten spielt eine entscheidende Rolle bei der Funktionalität und Benutzererfahrung des Systems.  1. Hardware-Design  Das Hardware-Design umfasst die Auswahl geeigneter Sensoren und Aktoren zur Steuerung der Katzenklappe. Die Katzenklappe selbst wird mit einem motorisierten Mechanismus ausgestattet, der durch ein Mikrocontroller-Modul (z.B. Raspberry Pi oder Arduino) angesteuert wird. Zur Katzenerkennung wird eine Kamera installiert, die in der Lage ist, Bilder in Echtzeit zu erfassen. Zusätzlich sind Umgebungslichtsensoren und gegebenenfalls RFID-Technologie vorgesehen, um die Klappe nur für autorisierte Katzen zu öffnen.   2. Software-Architektur  Die Software-Architektur des Systems besteht aus mehreren Schichten  - DatenakquiseDie Kamera erfasst kontinuierlich Bilder, die an ein KI-Modell zur Katzenerkennung übermittelt werden. - DatenverarbeitungEin vorab trainiertes KI-Modell, basierend auf Convolutional Neural Networks (CNNs), wird zur Identifikation der Katze eingesetzt. Das Modell wird mit einer Vielzahl von Bildern unterschiedlicher Katzenrassen und -größen trainiert, um eine hohe Erkennungsgenauigkeit zu gewährleisten. - SteuerungslogikBei erfolgreicher Erkennung sendet das System ein Signal an den Motor der Katzenklappe, um diese zu öffnen oder zu schließen. Hierbei wird auch eine Zeitverzögerung implementiert, um sicherzustellen, dass die Klappe nicht unnötig lange offen bleibt.  3. KI-Trainingsprozess  Der Trainingsprozess des KI-Modells ist entscheidend für die Genauigkeit der Katzenerkennung. Eine große Datenbasis ist erforderlich, um das Modell robust zu machen. Hierzu werden Bilder von Katzen in unterschiedlichen Posen, Lichtverhältnissen und Hintergründen gesammelt. Der Trainingsprozess erfolgt in mehreren Phasen - DatensammlungErstellung eines Datensatzes mit annotierten Bildern. - ModelltrainingVerwendung von Transfer Learning, um ein vortrainiertes Modell (z.B. MobileNet oder ResNet) an die spezifischen Anforderungen der Katzenerkennung anzupassen. - EvaluierungTesten des Modells mit einem separaten Validierungsdatensatz, um die Erkennungsgenauigkeit zu bestimmen und gegebenenfalls Anpassungen vorzunehmen.  4. Benutzeroberfläche;1
 Kapitel 2: Technische Grundlagen der Evaluation von ElixirNerves als Plattform für IoT-Anwendungen   2.1 Einführung in IoT-Anwendungen  Das Internet der Dinge (IoT) bezeichnet ein Netzwerk von physischen Objekten, die durch Sensoren, Software und andere Technologien miteinander verbunden sind und Daten austauschen können. Diese Objekte – von Smart-Home-Geräten bis hin zu industriellen Maschinen – bringen zahlreiche Herausforderungen mit sich, sowohl hinsichtlich der Hardware als auch der Software. Eine effektive Plattform für die Entwicklung und Implementierung von IoT-Anwendungen muss skalierbar, leicht anpassbar und robust sein.   2.2 Überblick über Elixir und Nerves  Elixir ist eine funktionale Programmiersprache, die auf der Erlang virtuellen Maschine (BEAM) basiert. Sie bietet leistungsstarke Eigenschaften wie Nebenläufigkeit, Verteilung und Fehlertoleranz. Diese Merkmale machen Elixir besonders geeignet für die Entwicklung von Anwendungen, die hohe Anforderungen an die Verfügbarkeit und Leistung stellen.  Nerves ist ein Framework für die Entwicklung von IoT-Anwendungen mit Elixir. Es ermöglicht Entwicklern, robustes Embedded-Software zu schreiben, das auf einer Vielzahl von Hardwareplattformen ausgeführt werden kann. Nerves kombiniert die Vorteile von Elixir mit speziellen Funktionen für das IoT, wie z. B. die Unterstützung für verschiedene Hardware-Abstraktionen, ein umfassendes Build-System und die Möglichkeit zur Remote-Updates.   2.3 Technische Architektur von ElixirNerves  Die Architektur von ElixirNerves kann in mehrere Schichten unterteilt werden:   2.3.1 Hardware-Abstraktion  Nerves bietet eine abstrahierte Schicht, die die Interaktion mit verschiedenen Hardwarekomponenten erleichtert. Diese Schicht ermöglicht es Entwicklern, direkt mit GPIO-Pins, I2C, SPI und anderen Protokollen zu kommunizieren, ohne sich um die spezifischen Details der zugrunde liegenden Hardware kümmern zu müssen. Dies fördert die Wiederverwendbarkeit von Code und vereinfacht die Entwicklung von plattformübergreifenden IoT-Anwendungen.   2.3.2 System-Building  Das Nerves-System verwendet einen Toolchain-Ansatz, bei dem das gesamte System in einem Docker-Container gebaut wird. Dies stellt sicher, dass die Anwendung in einer konsistenten Umgebung erstellt und getestet wird. Nerves verwendet den Nerves-Distributor und Nerves-Firmware, um die Firmware-Updates und das Bereitstellen von Anwendungen zu verwalten. Dies erleichtert das Warten und Aktualisieren der Geräte nach der Bereitstellung.   2.3.3 Anwendungsentwicklung  Die Entwicklung von Anwendungen in Nerves erfolgt in der Elixir-Sprache, die es den Entwicklern ermöglicht, von den funktionalen Programmierkonzepten und den leistungsstarken Funktionen von Elixir zu profitieren. Durch den Einsatz von Supervisoren, Protokollen und GenServern können komplexe Logik und Zustandsmanagement effektiv umgesetzt werden.   2.4 Sicherheitsaspekte  Die Sicherheit von IoT-Anwendungen ist von entscheidender Bedeutung, da diese Systeme häufig sensible Daten verarbeiten und anfällig für Angriffe sind. ElixirNerves bietet verschiedene Sicherheitsmechanismen, darunter:  - Kryptographie: Nerves unterstützt gängige Kryptographie-Bibliotheken, die sicherstellen, dass Daten sowohl im Ruhezustand als auch während der Übertragung geschützt sind.    - Authentifizierung und Autorisierung: Entwickler können Authentifizierungs- und Autorisierungsmechanismen implementieren, um sicherzustellen, dass nur berechtigte Benutzer und Geräte auf die IoT-Anwendungen zugreifen können.  - Regelmäßige Firmware-Updates: Durch die Möglichkeit zur Bereitstellung von Updates können Sicherheitsanfälligkeiten schnell behoben werden.   2.5 Evaluation und Testverfahren  Die Evaluation von ElixirNerves als Plattform für IoT-Anwendungen erfordert eine sorgfältige Betrachtung verschiedener Faktoren, darunter:  - Zuverlässigkeit: Tests zur Bestimmung der Fehlertoleranz und Systemstabilität unter verschiedenen Bedingungen. - Leistung: Messungen der Latenz und der Reaktionszeiten von Anwendungen unter realistischen Arbeitslasten. - Benutzererfahrung: Evaluierung der Benutzeroberfläche und Interaktionen, insbesondere für Anwendungsfälle wie Smart-Home oder industrielle Automatisierung.   2.6 Fazit  Die technischen Grundlagen von ElixirNerves bieten eine solide Basis für die Entwicklung und Implementierung von IoT-Anwendungen. Die Kombination aus Elixirs leistungsfähigen Funktionen und Nerves‘ spezifischen Zielsetzungen macht es zu einer vielversprechenden Plattform für Entwickler, die innovative IoT-Lösungen realisieren möchten. Die folgende Kapitel werden sich mit spezifischen Anwendungsszenarien und den praktischen Erfahrungen bei der Nutzung von ElixirNerves in der Entwicklung von IoT-Anwendungen beschäftigen.;1
Die Wahl einer Programmiersprache ist entscheidend für den Erfolg eines Softwareprojekts. In den letzten Jahren haben sich Java und Kotlin als prominente Optionen für die Entwicklung von Anwendungen auf der Java Virtual Machine (JVM) etabliert. Die vorliegende Analyse fokussiert sich auf die Evaluierung dieser beiden Sprachen unter Berücksichtigung relevanter Faktoren, die Einfluss auf die Projektentwicklung haben.  Hintergrund  Java, eine der ältesten und am weitesten verbreiteten Programmiersprachen, wurde 1995 eingeführt und ist für ihre Plattformunabhängigkeit sowie ihre weitreichende Community und umfangreiche Bibliotheken bekannt. Kotlin, eine 2011 eingeführte Sprache, die 2017 von Google als offizielle Sprache für Android-Entwicklung anerkannt wurde, bietet eine moderne Syntax und zahlreiche Funktionen, die die Programmierung effizienter und weniger fehleranfällig gestalten.  Evaluationskriterien  Bei der Evaluierung von Java und Kotlin im Kontext eines Softwareprojekts sind mehrere Kriterien maßgeblich, darunter Lesbarkeit, Wartbarkeit, Performance, Interoperabilität, Community-Support, Lernkurve und API-Integration.  1. Lesbarkeit und WartbarkeitKotlin bietet eine klarere und prägnantere Syntax im Vergleich zu Java. Sprachfeatures wie Null-Sicherheit, Lambda-Ausdrücke und Extension Functions fördern eine reduzierte Komplexität und steigern die Lesbarkeit des Codes. In einem praktischen Projektkontext bedeutet dies, dass Entwickler schneller lesen und verstehen können, was zu einer verbesserten Wartbarkeit führt.  2. PerformanceBeide Sprachen bieten ähnliche Leistungseigenschaften, da sie auf der JVM laufen. Allerdings kann die syntaktische Effizienz von Kotlin zu einer geringeren Codezeilenanzahl führen, was potenziell die Compile-Zeit verkürzt. Dennoch bedarf es einer spezifischen Analyse, um performante Aspekte bei komplexen Anwendungen zu bewerten.  3. InteroperabilitätEin entscheidender Vorteil von Kotlin ist die nahtlose Interoperabilität mit Java. Dies ermöglicht es, bestehende Java-Bibliotheken und -Frameworks ohne nennenswerte Modifikationen zu nutzen, was besonders in migrationsorientierten Projekten von Bedeutung ist. Die Fähigkeit, gemischte Projekte zu erstellen, ermöglicht es Teams, schrittweise von Java zu Kotlin überzugehen.  4. Community-Support und LernkurveJava hat eine seit Jahrzehnten etablierte Community, umfangreiche Dokumentation und eine Vielzahl von Ressourcen für Entwickler. Kotlin ist relativ neu, hat aber schnell an Popularität gewonnen und bietet ebenfalls umfangreiche Ressourcen. Für Entwickler, die bereits mit Java vertraut sind, kann die Lernkurve zu Kotlin jedoch als moderat eingeschätzt werden, da viele Konzepte in beiden Sprachen ähnlich sind.  5. API-IntegrationDie Kompatibilität mit modernen API-Designs und -Strukturen spielt eine wesentliche Rolle. Kotlin unterstützt beispielsweise die funktionale Programmierung und ermöglicht die einfache Implementierung von APIs, was viele moderne Entwicklungsparadigmen unterstützt. Dies ist ein wesentlicher Vorteil für Projekte, die Schnittstellen zur Integration mit anderen Systemen benötigen.  Fazit  Die Evaluierung von Java und Kotlin zeigt, dass beide Sprachen ihre spezifischen Stärken und Schwächen aufweisen. Die Entscheidung für eine der beiden Sprachen sollte kontextabhängig erfolgen, wobei Faktoren wie Projektanforderungen, Teamkompetenz und langfristige Wartbarkeit berücksichtigt werden sollten. Während Java nach wie vor eine solide Grundlage für viele Projekte darstellt, bietet Kotlin durch moderne Sprachfeatures und eine bessere Lesbarkeit klare Vorteile, insbesondere in der Entwicklung von neuen Anwendungen.  In der Praxis könnte die synergistische Nutzung beider Sprachen in einem Projekt als optimaler Kompromiss angesehen werden, um die Vorzüge beider Welten zu vereinen.;1
"   Das studentische Software Engineering ist ein komplexer Prozess, der nicht nur technisches Wissen, sondern auch effektives Projektmanagement erfordert. Häufig stehen Studierende vor der Herausforderung, verschiedene Aufgaben zu koordinieren, Fristen einzuhalten und die Zusammenarbeit im Team effizient zu gestalten. In diesem Kontext ist die Entwicklung eines geeigneten Aufgabenmanagement-Tools von zentraler Bedeutung. Die vorliegende Anforderungsanalyse zielt darauf ab, die wesentlichen Funktionen und Eigenschaften eines solchen Tools zu identifizieren, um die studentische Projektarbeit zu optimieren.   Methodik der Anforderungsanalyse  Die Anforderungsanalyse wurde durch qualitative Interviews mit Studierenden verschiedener Studiengänge, die Software Engineering in ihrem Curriculum integriert haben, sowie durch die Auswertung bestehender Literatur und Best Practices im Bereich Projektmanagement durchgeführt. Hierbei wurden spezifische Bedürfnisse und Herausforderungen der Studierenden erfasst. Zudem wurde eine Analyse bereits vorhandener Tools durchgeführt, um Stärken und Schwächen im bestehenden Angebot zu identifizieren.   Identifizierte Anforderungen  Die Ergebnisse der Anforderungsanalyse führten zu einer Kategorisierung der Anforderungen in funktionale und nicht-funktionale Aspekte 1. Funktionale Anforderungen   - AufgabenverwaltungBenutzer sollen in der Lage sein, Aufgaben zu erstellen, zu kategorisieren, zu priorisieren und Deadlines festzulegen.    - Team-ZusammenarbeitDas Tool sollte Funktionen zur Unterstützung der Teamkommunikation bieten, wie z.B. Kommentare zu Aufgaben, Dateianhänge und Benachrichtigungen.    - FortschrittsverfolgungEine Ansicht, die den aktuellen Fortschritt der einzelnen Aufgaben und des gesamten Projektes visualisiert (z.B. durch Kanban-Boards oder Gantt-Diagramme), wird als essenziell erachtet.    - IntegrationsmöglichkeitenDas Tool sollte sich in bestehende Entwicklungsumgebungen oder Plattformen wie GitHub oder Jira integrieren lassen, um einen reibungslosen Workflow zu gewährleisten.  2. Nicht-funktionale Anforderungen   - BenutzerfreundlichkeitDie Oberfläche sollte intuitiv und einfach zu bedienen sein, um Lernkurven zu minimieren und die Akzeptanz zu fördern.    - ZugänglichkeitDas Tool sollte plattformübergreifend und auf mobilen Geräten nutzbar sein, um die Flexibilität zu erhöhen.    - Sicherheit und DatenschutzBei der Kommunikation und Speicherung von Daten müssen Sicherheitsstandards eingehalten werden, um die Nutzerdaten zu schützen.     Die durchgeführte Anforderungsanalyse hat deutlich gemacht, dass ein maßgeschneidertes Aufgabenmanagement-Tool für das studentische Software Engineering zwingend notwendig ist, um den spezifischen Bedürfnissen dieser Zielgruppe gerecht zu werden. Die identifizierten funktionalen und nicht-funktionalen Anforderungen bilden die Grundlage für die Entwicklung eines effektiven Tools, das nicht nur die Effizienz der Projektarbeit steigert, sondern auch die Teamdynamik verbessert und die Lernkurve der Studierenden positiv beeinflusst.  Das Projekt verdeutlicht, dass technische Lösungen im Bereich des Aufgabenmanagements nicht isoliert betrachtet werden dürfen; sie müssen stets in den Kontext der Benutzerbedürfnisse und der spezifischen Herausforderungen im studentischen Umfeld integriert werden. Die erfolgreiche Implementierung eines solchen Tools könnte langfristig zu einer Steigerung der Qualität studentischer Projekte und einem verbesserten Lernerlebnis führen. Die nächsten Schritte sollten die prototypische Entwicklung und eine umfangreiche Testphase beinhalten, um die Praxistauglichkeit des Tools zu evaluieren und gegebenenfalls anzupassen.";1
